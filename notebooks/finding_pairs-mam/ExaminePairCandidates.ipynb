{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter single node differences\n",
    "We have tons of single node differences - how can we filter them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "from IPython.core.display import display, HTML\n",
    "from diff_match_patch import diff_match_patch\n",
    "\n",
    "from colorama import Fore\n",
    "import ipysheet\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_NODE_DIFF_PATH = \"/projects/bdata/datasets/kaggle-competitions/processed/matches.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"slug\": \"allen21huang\", \"version_id\": \"23387521\", \"source\": \"# check and change our directory\\nos.chdir('/kaggle/input/natural-images/data/natural_images')\\nprint(os.listdir())\"}, {\"slug\": \"allen21huang\", \"version_id\": \"23383256\", \"source\": \"os.chdir('/kaggle/input')\\nprint(os.listdir())\"}]\n",
      "[{\"slug\": \"allen21huang\", \"version_id\": \"23387521\", \"source\": \"# check and change our directory\\nos.chdir('/kaggle/input/natural-images/data/natural_images')\\nprint(os.listdir())\"}, {\"slug\": \"allen21huang\", \"version_id\": \"23383256\", \"source\": \"os.chdir('/kaggle/input')\\nprint(os.listdir())\"}]\n",
      "[{\"slug\": \"allen21huang\", \"version_id\": \"23387521\", \"source\": \"# check and change our directory\\nos.chdir('/kaggle/input/natural-images/data/natural_images')\\nprint(os.listdir())\"}, {\"slug\": \"allen21huang\", \"version_id\": \"23383639\", \"source\": \"os.chdir('/kaggle/input')\\nprint(os.listdir())\"}]\n",
      "[{\"slug\": \"allen21huang\", \"version_id\": \"23387521\", \"source\": \"# check and change our directory\\nos.chdir('/kaggle/input/natural-images/data/natural_images')\\nprint(os.listdir())\"}, {\"slug\": \"allen21huang\", \"version_id\": \"23383639\", \"source\": \"os.chdir('/kaggle/input')\\nprint(os.listdir())\"}]\n",
      "[{\"slug\": \"allen21huang\", \"version_id\": \"23387521\", \"source\": \"os.chdir('/kaggle/input')\\nprint(os.listdir())\"}, {\"slug\": \"allen21huang\", \"version_id\": \"23383256\", \"source\": \"# check and change our directory\\nos.chdir('/kaggle/input/natural-images/data/natural_images')\\nprint(os.listdir())\"}]\n"
     ]
    }
   ],
   "source": [
    "!head $SINGLE_NODE_DIFF_PATH -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23610029 /projects/bdata/datasets/kaggle-competitions/processed/matches.jsonl\n"
     ]
    }
   ],
   "source": [
    "!wc -l $SINGLE_NODE_DIFF_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_node_samples = !shuf -n 10000 $SINGLE_NODE_DIFF_PATH\n",
    "single_node_samples = [json.loads(x) for x in single_node_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import tokenize\n",
    "import token\n",
    "import sys\n",
    "\n",
    "def remove_comments(src):\n",
    "    \"\"\"\n",
    "    This reads tokens using tokenize.generate_tokens and recombines them\n",
    "    using tokenize.untokenize, and skipping comment/docstring tokens in between\n",
    "    \"\"\"\n",
    "    f = StringIO(src)\n",
    "    class SkipException(Exception): pass\n",
    "    processed_tokens = []\n",
    "    last_token = None\n",
    "    # go thru all the tokens and try to skip comments and docstrings\n",
    "    for tok in tokenize.generate_tokens(f.readline):\n",
    "        t_type, t_string, t_srow_scol, t_erow_ecol, t_line = tok\n",
    "\n",
    "        try:\n",
    "            if t_type == tokenize.COMMENT:\n",
    "                raise SkipException()\n",
    "\n",
    "            elif t_type == tokenize.STRING:\n",
    "\n",
    "                if last_token is None or last_token[0] in [tokenize.INDENT]:\n",
    "                    # FIXEME: this may remove valid strings too?\n",
    "                    #raise SkipException()\n",
    "                    pass\n",
    "\n",
    "        except SkipException:\n",
    "            pass\n",
    "        else:\n",
    "            processed_tokens.append(tok)\n",
    "\n",
    "        last_token = tok\n",
    "\n",
    "    return tokenize.untokenize(processed_tokens)\n",
    "\n",
    "\n",
    "def remove_comments_from_src(src):\n",
    "    \"\"\"\n",
    "    This reads tokens using tokenize.generate_tokens and recombines them\n",
    "    using tokenize.untokenize, and skipping comment/docstring tokens in between\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = StringIO(src)\n",
    "        class SkipException(Exception): pass\n",
    "        processed_tokens = []\n",
    "        last_token = None\n",
    "        # go thru all the tokens and try to skip comments and docstrings\n",
    "        for tok in tokenize.generate_tokens(f.readline):\n",
    "            t_type, t_string, t_srow_scol, t_erow_ecol, t_line = tok\n",
    "\n",
    "            try:\n",
    "                if t_type == tokenize.COMMENT:\n",
    "                    raise SkipException()\n",
    "\n",
    "                elif t_type == tokenize.STRING:\n",
    "\n",
    "                    if last_token is None or last_token[0] in [tokenize.INDENT]:\n",
    "                        # FIXEME: this may remove valid strings too?\n",
    "                        #raise SkipException()\n",
    "                        pass\n",
    "\n",
    "            except SkipException:\n",
    "                pass\n",
    "            else:\n",
    "                processed_tokens.append(tok)\n",
    "\n",
    "            last_token = tok\n",
    "\n",
    "        return tokenize.untokenize(processed_tokens)\n",
    "    #I belive this happens with invalid python\n",
    "    except (ValueError, tokenize.TokenError, IndentationError) as e:\n",
    "        return src\n",
    "\n",
    "\n",
    "def remove_comments_and_docstrings(src):\n",
    "    \n",
    "    source = StringIO(src)\n",
    "    mod = StringIO()\n",
    "    \n",
    "    prev_toktype = token.INDENT\n",
    "    first_line = None\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "\n",
    "    tokgen = tokenize.generate_tokens(source.readline)\n",
    "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
    "        if 0:   # Change to if 1 to see the tokens fly by.\n",
    "            print(\"%10s %-14s %-20r %r\" % (\n",
    "                tokenize.tok_name.get(toktype, toktype),\n",
    "                \"%d.%d-%d.%d\" % (slineno, scol, elineno, ecol),\n",
    "                ttext, ltext\n",
    "                ))\n",
    "        if slineno > last_lineno:\n",
    "            last_col = 0\n",
    "        if scol > last_col:\n",
    "            mod.write(\" \" * (scol - last_col))\n",
    "#         if toktype == token.STRING and prev_toktype == token.INDENT:\n",
    "#             # Docstring\n",
    "#             continue\n",
    "#         elif toktype == tokenize.COMMENT:\n",
    "#             # Comment\n",
    "#             continue\n",
    "        elif not(toktype == tokenize.COMMENT) and not(toktype == token.STRING and prev_toktype == token.INDENT):\n",
    "            mod.write(ttext)\n",
    "        prev_toktype = toktype\n",
    "        last_col = ecol\n",
    "        last_lineno = elineno\n",
    "    return mod.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "#train_images = train_dogs[:2000] + train_cats[:2000]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(remove_comments(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(remove_comments_and_docstrings(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diffs use were generated using:\n",
    "`python tree_matches --length_threshold 3 --ignore_function_args --remove_exact_duplicates --ignore_strings --sequential_matches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = !cat /homes/gws/mikeam/RobustDataScience/matches.jsonl\n",
    "examples = [json.loads(x) for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def html_diff(a,b):\n",
    "#     differ = difflib.HtmlDiff(wrapcolumn = 80)\n",
    "#     return differ.make_table(a.split(\"\\n\"),b.split(\"\\n\"), context = True)\n",
    "\n",
    "\n",
    "def pprint_code_diff(a,b):\n",
    "    dmp = diff_match_patch()\n",
    "    diffs = dmp.diff_main(a,b)\n",
    "    dmp.diff_cleanupSemantic(diffs)\n",
    "    display(HTML(dmp.diff_prettyHtml(diffs)))\n",
    "    \n",
    "def compare_diff(examples,index):\n",
    "    orig = examples[index][0]\n",
    "    nn = examples[index][1]\n",
    "    pprint_code_diff(orig[\"source\"],nn[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span>&para;<br>RF &lt;- h2o.randomForest(&para;<br>    y = \"open_channels\",&para;<br>    training_frame = h2oTr,&para;<br></span><del style=\"background:#ffe6e6;\">    balance_classes = TRUE,&para;<br>    class_sampling_factors = c(3, 1.1 , 1, 1, 1, 1, 1, 1, 1, 1, 1),&para;<br></del><span>    ntrees = 90,&para;<br>    max_depth = 18,&para;<br>    min_rows = 1,&para;<br>    nbins = 20,&para;<br>    sample_rate = .6,&para;<br>    col_sample_rate_per_tree = .9&para;<br>)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>&para;<br>RF &lt;- h2o.randomForest(&para;<br>    y = \"open_channels\",&para;<br>    training_frame = h2oTr,&para;<br>    ntrees = 100,&para;<br>    max_depth = 18,&para;<br>    min_rows = 1,&para;<br>    nbins = 20,&para;<br>    sample_rate = .6,&para;<br>    col_sample_rate_per_tree = .9</span><ins style=\"background:#e6ffe6;\">,&para;<br>    balance_classes = TRUE,&para;<br>    class_sampling_factors = c(3,1.1,1,1,1,1,1,1,1,1,1)</ins><span>&para;<br>)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>&para;<br>RF &lt;- h2o.randomForest(&para;<br>    y = \"open_channels\",&para;<br>    training_frame = h2oTr,&para;<br>    ntrees = 100,&para;<br>    max_depth = 18,&para;<br>    min_rows = 1,&para;<br>    nbins = 20,&para;<br>    sample_rate = .6,&para;<br>    col_sample_rate_per_tree = .9</span><del style=\"background:#ffe6e6;\">,&para;<br>    balance_classes = TRUE,&para;<br>    class_sampling_factors = c(3,1.1,1,1,1,1,1,1,1,1,1)</del><span>&para;<br>)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>&para;<br>RF &lt;- h2o.randomForest(&para;<br>    y = \"open_channels\",&para;<br>    training_frame = h2oTr,&para;<br>    ntrees = 100,&para;<br>    max_depth = 18,&para;<br>    min_rows = 1,&para;<br>    nbins = 20,&para;<br>    sample_rate = .6,&para;<br>    col_sample_rate_per_tree = .75</span><ins style=\"background:#e6ffe6;\">,&para;<br>    balance_classes = TRUE,&para;<br>    class_sampling_factors = c(3,1.1,1,1,1,1,1,1,1,1,1)</ins><span>&para;<br>)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>&para;<br>RF &lt;- h2o.randomForest(&para;<br>    y = \"open_channels\",&para;<br>    training_frame = h2oTr,&para;<br>    ntrees = 100,&para;<br>    max_depth = 18,&para;<br>    min_rows = 1,&para;<br>    nbins = 20,&para;<br>    sample_rate = .6,&para;<br>    col_sample_rate_per_tree = .75</span><del style=\"background:#ffe6e6;\">,&para;<br>    balance_classes = TRUE,&para;<br>    class_sampling_factors = c(3,1.1,1,1,1,1,1,1,1,1,1)</del><span>&para;<br>)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>&para;<br>RF &lt;- h2o.randomForest(&para;<br>    y = \"open_channels\",&para;<br>    training_frame = DF_tr,&para;<br>    ntrees = 1</span><del style=\"background:#ffe6e6;\">2</del><ins style=\"background:#e6ffe6;\">3</ins><span>0,&para;<br>    max_depth = 18,&para;<br>    min_rows = 1,&para;<br>    nbins = 20,&para;<br>    sample_rate = .6,&para;<br>    col_sample_rate_per_tree = .7</span><ins style=\"background:#e6ffe6;\">,&para;<br>    min_split_improvement = .000015</ins><span>&para;<br>)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>library(data.table)&para;<br>&para;<br>train &lt;- fread(\"../input/kalman-filter-on-clean-data/train.csv\")&para;<br>test &lt;- fread(\"../input/kalman-filter-on-clean-data//test.csv\")&para;<br>&para;<br>batchSlice = </span><del style=\"background:#ffe6e6;\">F</del><ins style=\"background:#e6ffe6;\">T</ins><span>&para;<br>&para;<br>gc()</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>library(data.table)&para;<br>&para;<br>train &lt;- fread(\"../input/kalman-filter-on-clean-data/train.csv\")&para;<br>test &lt;- fread(\"../input/kalman-filter-on-clean-data//test.csv\")&para;<br>&para;<br>batchSlice = </span><del style=\"background:#ffe6e6;\">T</del><ins style=\"background:#e6ffe6;\">F</ins><span>&para;<br>&para;<br>gc()</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>library(data.table)&para;<br>&para;<br>train &lt;- fread(\"../input/kalman-filter-on-clean-data/train.csv\")&para;<br>test &lt;- fread(\"../input/kalman-filter-on-clean-data//test.csv\")&para;<br>bs = </span><del style=\"background:#ffe6e6;\">FALS</del><ins style=\"background:#e6ffe6;\">TRU</ins><span>E&para;<br>invisible(gc())</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>library(data.table)&para;<br>&para;<br>train &lt;- fread(\"../input/kalman-filter-on-clean-data/train.csv\")&para;<br>test &lt;- fread(\"../input/kalman-filter-on-clean-data//test.csv\")&para;<br>bs = </span><del style=\"background:#ffe6e6;\">TRUE</del><ins style=\"background:#e6ffe6;\">F</ins><span>&para;<br>invisible(gc())</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>n_groups = </span><del style=\"background:#ffe6e6;\">4</del><ins style=\"background:#e6ffe6;\">10</ins><span>0&para;<br>test[\"group\"] = 0&para;<br>for i in range(n_groups):&para;<br>    ids = np.arange(i*</span><del style=\"background:#ffe6e6;\">5</del><ins style=\"background:#e6ffe6;\">2</ins><span>0000, (i+1)*</span><del style=\"background:#ffe6e6;\">5</del><ins style=\"background:#e6ffe6;\">2</ins><span>0000)&para;<br>    test.loc[ids,\"group\"] = i&para;<br>    &para;<br>for i in range(n_groups):&para;<br>    sub = test[test.group == i]&para;<br>    signals = sub.signal.values&para;<br>    imax, imin = math.floor(np.max(signals)), math.ceil(np.min(signals))&para;<br>    signals = (signals - np.min(signals))/(np.max(signals) - np.min(signals))&para;<br>    signals = signals*(imax-imin)&para;<br>    test.loc[sub.index,\"open_channels\"] = [0,] + list(np.array(signals[:-1],np.int))&para;<br>    &para;<br>submission.open_channels = np.array(test.open_channels, np.int)&para;<br>submission.to_csv(\"submission.csv\",index=False)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def run_lgb(pre_trian, pre_test, usefull_features, params):&para;<br>    &para;<br>    kf = KFold(n_splits = 5, shuffle = True, random_state = 42)&para;<br>    target = 'open_channels'&para;<br>    oof_pred = np.zeros(len(pre_train))&para;<br>    y_pred = np.zeros(len(pre_test))&para;<br>    feature_importance = pd.DataFrame()&para;<br>    &para;<br>    # train a baseline model and record the weighted cohen kappa score &para;<br>    for fold, (tr_ind, val_ind) in enumerate(kf.split(pre_train)):&para;<br>        print('Fold {}'.format(fold + 1))&para;<br>        x_train, x_val = pre_train[usefull_features].iloc[tr_ind], pre_train[usefull_features].iloc[val_ind]&para;<br>        y_train, y_val = pre_train[target][tr_ind], pre_train[target][val_ind]&para;<br>        train_set = lgb.Dataset(x_train, y_train)&para;<br>        val_set = lgb.Dataset(x_val, y_val)&para;<br>        &para;<br>        model = lgb.train(params, train_set, num_boost_round = </span><del style=\"background:#ffe6e6;\">25</del><ins style=\"background:#e6ffe6;\">30</ins><span>00, early_stopping_rounds = 50, &para;<br>                         valid_sets = [train_set, val_set], verbose_eval = 100)&para;<br>        &para;<br>        oof_pred[val_ind] = model.predict(x_val)&para;<br>        &para;<br>        y_pred += model.predict(pre_test[usefull_features]) / kf.n_splits&para;<br>        &para;<br>        # get fold importance df&para;<br>        fold_importa</span><ins style=\"background:#e6ffe6;\">n</ins><span>ce = pd.DataFrame({'features': usefull_features})&para;<br>        fold_importance['fold'] = fold + 1&para;<br>        fold_importance['importance'] = model.feature_importance()&para;<br>        feature_importance = pd.concat([feature_importance, fold_importance])&para;<br>        &para;<br>    # round predictions&para;<br>    rmse_score = np.sqrt(mean_squared_error(pre_trian[target], oof_pred))&para;<br>    print('Our oof rmse score is: ', rmse_score)&para;<br>    oof_pred = np.clip(oof_pred, 0, 10).astype(int)&para;<br>    y_pred = np.clip(y_pred, 0, 10).astype(int)&para;<br>    cohen_score = cohen_kappa_score(pre_train[target], oof_pred, weights = 'quadratic')&para;<br>    print('Our oof cohen kappa score is: ', cohen_score)&para;<br>    &para;<br>    # plot feature importance&para;<br>    fi_mean = feature_importance.groupby(['features'])['importance'].mean().reset_index()&para;<br>    plt.figure(figsize = (12, 14))&para;<br>    sns.barplot(x = fi_mean['importance'], y = fi_mean['features'])&para;<br>    plt.xlabel('Importance', fontsize = 13)&para;<br>    plt.ylabel('Feature', fontsize = 13)&para;<br>    plt.tick_params(axis = 'x', labelsize = 11)&para;<br>    plt.tick_params(axis = 'y', labelsize = 11)&para;<br>    plt.title('Light Gradient Boosting Feature Importance (5 KFold)')&para;<br>    plt.show()&para;<br>    &para;<br>    &para;<br>    return oof_pred, y_pred, feature_importance&para;<br>&para;<br>&para;<br># define hyperparammeter (some random hyperparammeters)&para;<br>params = {'learning_rate': 0.1, &para;<br>          'feature_fraction': 0.75, &para;<br>          'bagging_fraction': 0.75,&para;<br>          'bagging_freq': 1,&para;<br>          'n_jobs': -1, &para;<br>          'seed': 50,&para;<br>          'metric': 'rmse'&para;<br>        }&para;<br>&para;<br># define the features for training&para;<br>features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch', 'group']]&para;<br>&para;<br>oof_pred, y_pred, feature_importance = run_lgb(pre_train, pre_test, features, params)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def run_lgb(pre_trian, pre_test, usefull_features, params):&para;<br>    &para;<br>    kf = KFold(n_splits = 5, shuffle = True, random_state = 42)&para;<br>    target = 'open_channels'&para;<br>    oof_pred = np.zeros(len(pre_train))&para;<br>    y_pred = np.zeros(len(pre_test))&para;<br>    feature_importance = pd.DataFrame()&para;<br>    &para;<br>    # train a baseline model and record the weighted cohen kappa score &para;<br>    for fold, (tr_ind, val_ind) in enumerate(kf.split(pre_train)):&para;<br>        print('Fold {}'.format(fold + 1))&para;<br>        x_train, x_val = pre_train[usefull_features].iloc[tr_ind], pre_train[usefull_features].iloc[val_ind]&para;<br>        y_train, y_val = pre_train[target][tr_ind], pre_train[target][val_ind]&para;<br>        train_set = lgb.Dataset(x_train, y_train)&para;<br>        val_set = lgb.Dataset(x_val, y_val)&para;<br>        &para;<br>        model = lgb.train(params, train_set, num_boost_round = </span><del style=\"background:#ffe6e6;\">30</del><ins style=\"background:#e6ffe6;\">25</ins><span>00, early_stopping_rounds = 50, &para;<br>                         valid_sets = [train_set, val_set], verbose_eval = 100)&para;<br>        &para;<br>        oof_pred[val_ind] = model.predict(x_val)&para;<br>        &para;<br>        y_pred += model.predict(pre_test[usefull_features]) / kf.n_splits&para;<br>        &para;<br>        # get fold importance df&para;<br>        fold_importance = pd.DataFrame({'features': usefull_features})&para;<br>        fold_importance['fold'] = fold + 1&para;<br>        fold_importance['importance'] = model.feature_importance()&para;<br>        feature_importance = pd.concat([feature_importance, fold_importance])&para;<br>        &para;<br>    # round predictions&para;<br>    rmse_score = np.sqrt(mean_squared_error(pre_trian[target], oof_pred))&para;<br>    print('Our oof rmse score is: ', rmse_score)&para;<br>    oof_pred = np.clip(oof_pred, 0, 10).astype(int)&para;<br>    y_pred = np.clip(y_pred, 0, 10).astype(int)&para;<br>    cohen_score = cohen_kappa_score(pre_train[target], oof_pred, weights = 'quadratic')&para;<br>    print('Our oof cohen kappa score is: ', cohen_score)&para;<br>    &para;<br>    # plot feature importance&para;<br>    fi_mean = feature_importance.groupby(['features'])['importance'].mean().reset_index()&para;<br>    fi_mean.sort_values('importance', ascending = False, inplace = True)&para;<br>    plt.figure(figsize = (12, 14))&para;<br>    sns.barplot(x = fi_mean['importance'], y = fi_mean['features'])&para;<br>    plt.xlabel('Importance', fontsize = 13)&para;<br>    plt.ylabel('Feature', fontsize = 13)&para;<br>    plt.tick_params(axis = 'x', labelsize = 11)&para;<br>    plt.tick_params(axis = 'y', labelsize = 11)&para;<br>    plt.title('Light Gradient Boosting Feature Importance (5 KFold)')&para;<br>    plt.show()&para;<br>    &para;<br>    &para;<br>    return oof_pred, y_pred, feature_importance&para;<br>&para;<br>&para;<br># define hyperparammeter (some random hyperparammeters)&para;<br>params = {'num_leaves': 128,&para;<br>          'min_data_in_leaf': 64,&para;<br>          'objective': 'huber',&para;<br>          'max_depth': -1,&para;<br>          'learning_rate': 0.</span><del style=\"background:#ffe6e6;\">0</del><span>1,&para;<br>          \"boosting\": \"gbdt\",&para;<br>          \"bagging_freq\": 5,&para;<br>          \"bagging_fraction\": 0.8,&para;<br>          \"bagging_seed\": 11,&para;<br>          \"metric\": 'mae',&para;<br>          \"verbosity\": -1,&para;<br>          'reg_alpha': 0.1,&para;<br>          'reg_lambda': 0.3,&para;<br>          'n_jobs': -1&para;<br>         }&para;<br>&para;<br># define the features for training&para;<br>features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]&para;<br>&para;<br>oof_pred, y_pred, feature_importance = run_lgb(pre_train, pre_test, features, params)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def preprocess(train, test):&para;<br>    &para;<br>    pre_train = train.copy()&para;<br>    pre_test = test.copy()&para;<br>    &para;<br>    batch1 = pre_train[pre_train[\"batch\"] == 1]&para;<br>    batch2 = pre_train[pre_train[\"batch\"] == 2]&para;<br>    batch3 = pre_train[pre_train[\"batch\"] == 3]&para;<br>    batch4 = pre_train[pre_train[\"batch\"] == 4]&para;<br>    batch5 = pre_train[pre_train[\"batch\"] == 5]&para;<br>    batch6 = pre_train[pre_train[\"batch\"] == 6]&para;<br>    batch7 = pre_train[pre_train[\"batch\"] == 7]&para;<br>    batch8 = pre_train[pre_train[\"batch\"] == 8]&para;<br>    batch9 = pre_train[pre_train[\"batch\"] == 9]&para;<br>    batch10 = pre_train[pre_train[\"batch\"] == 10]&para;<br>    batch11 = pre_test[pre_test['batch'] == 11]&para;<br>    batch12 = pre_test[pre_test['batch'] == 12]&para;<br>    batch13 = pre_test[pre_test['batch'] == 13]&para;<br>    batch14 = pre_test[pre_test['batch'] == 14]&para;<br>    batches = [batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10, batch11, batch12, batch13, batch14]&para;<br>    &para;<br>    for batch in batches:&para;<br>        for feature in ['signal']:&para;<br>            # some random rolling features&para;<br>            for window in [50, 100, 1000, 5000, 10000, 25000]:&para;<br>                # roll backwards&para;<br>                batch[feature + 'mean_t' + str(window)] = batch[feature].shift(1).rolling(window).mean()&para;<br>                batch[feature + 'std_t' + str(window)] = batch[feature].shift(1).rolling(window).std()&para;<br>                batch[feature + 'min_t' + str(window)] = batch[feature].shift(1).rolling(window).min()&para;<br>                batch[feature + 'max_t' + str(window)] = batch[feature].shift(1).rolling(window).max()&para;<br>                min_max = (batch[feature] - batch[feature + 'min_t' + str(window)]) / (batch[feature + 'max_t' + str(window)] - batch[feature + 'min_t' + str(window)])&para;<br>                batch['norm_t' + str(window)] = min_max * (np.floor(batch[feature + 'max_t' + str(window)]) - np.ceil(batch[feature + 'min_t' + str(window)]))&para;<br>                &para;<br>                # roll forward&para;<br>                batch[feature + 'mean_t' + str(window) + '_lead'] = batch[feature].shift(- window - 1).rolling(window).mean()&para;<br>                batch[feature + 'std_t' + str(window) +'_lead'] = batch[feature].shift(- window - 1).rolling(window).std()&para;<br>                batch[feature + 'min_t' + str(window) + '_lead'] = batch[feature].shift(- window - 1).rolling(window).min()&para;<br>                batch[feature + 'max_t' + str(window) + '_lead'] = batch[feature].shift(- window - 1).rolling(window).max()&para;<br>                min_max = (batch[feature] - batch[feature + 'min_t' + str(window) + '_lead']) / (batch[feature + 'max_t' + str(window) + '_lead'] - batch[feature + 'min_t' + str(window) + '_lead'])&para;<br>                batch['norm_t' + str(window) + '_lead'] = min_max * (np.floor(batch[feature + 'max_t' + str(window) + '_lead']) - np.ceil(batch[feature + 'min_t' + str(window) + '_lead']))&para;<br>                &para;<br>    pre_train = pd.concat([batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10])&para;<br>    pre_test = pd.concat([batch11, batch12, batch13, batch14])&para;<br>    &para;<br>    del batches, batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10, batch11, batch12, batch13, batch14, train, test, min_max&para;<br>    &para;<br>    return pre_train, pre_test&para;<br>&para;<br>def reduce_mem_usage(df, verbose=True):&para;<br>    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']&para;<br>    start_mem = df.memory_usage().sum() / 1024**2    &para;<br>    for col in df.columns:&para;<br>        if col!='open_channels':&para;<br>            col_type = df[col].dtypes&para;<br>            if col_type in numerics:&para;<br>                c_min = df[col].min()&para;<br>                c_max = df[col].max()&para;<br>                if str(col_type)[:3] == 'int':&para;<br>                    if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:&para;<br>                        df[col] = df[col].astype(np.int8)&para;<br>                    elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:&para;<br>                        df[col] = df[col].astype(np.int16)&para;<br>                    elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:&para;<br>                        df[col] = df[col].astype(np.int32)&para;<br>                    elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:&para;<br>                        df[col] = df[col].astype(np.int64)  &para;<br>                else:&para;<br>                    if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max:&para;<br>                        df[col] = df[col].astype(np.float16)&para;<br>                    elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:&para;<br>                        df[col] = df[col].astype(np.float32)&para;<br>                    else:&para;<br>                        df[col] = df[col].astype(np.float64)    &para;<br>    end_mem = df.memory_usage().sum() / 1024**2&para;<br>    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))&para;<br>    return df&para;<br>&para;<br>&para;<br>def scale_fillna(pre_train, pre_test):&para;<br>    features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]&para;<br>    pre_train = pre_train.replace([np.inf, -np.inf], np.nan)&para;<br>    pre_test = pre_test.replace([np.inf, -np.inf], np.nan)&para;<br>    pre_train.fillna(0, inplace = True)&para;<br>    pre_test.fillna(0, inplace = True)&para;<br></span><ins style=\"background:#e6ffe6;\"># </ins><span>    scaler = StandardScaler()&para;<br></span><ins style=\"background:#e6ffe6;\"># </ins><span>    pre_train[features] = scaler.fit_transform(pre_train[features])&para;<br></span><ins style=\"background:#e6ffe6;\"># </ins><span>    pre_test[features] = scaler.transform(pre_test[features])&para;<br>    return pre_train, pre_test&para;<br>&para;<br># feature engineering&para;<br>pre_train, pre_test = preprocess(train, test)&para;<br># reduce memory usage&para;<br>pre_train = reduce_mem_usage(pre_train, verbose=True)&para;<br>pre_test = reduce_mem_usage(pre_test, verbose=True)&para;<br># scaling and filling missing values (this is not required for boosting algorithms, nevertheless i wanted to try and check)&para;<br>pre_train, pre_test = scale_fillna(pre_train, pre_test)&para;<br>del train, test&para;<br>gc.collect()</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def run_lgb(pre_trian, pre_test, usefull_features, params):&para;<br>    &para;<br>    kf = KFold(n_splits = 5, shuffle = True, random_state = 42)&para;<br>    target = 'open_channels'&para;<br>    oof_pred = np.zeros(len(pre_train))&para;<br>    y_pred = np.zeros(len(pre_test))&para;<br>    feature_importance = pd.DataFrame()&para;<br>    &para;<br>    # train a baseline model and record the weighted cohen kappa score &para;<br>    for fold, (tr_ind, val_ind) in enumerate(kf.split(pre_train)):&para;<br>        print('Fold {}'.format(fold + 1))&para;<br>        x_train, x_val = pre_train[usefull_features].iloc[tr_ind], pre_train[usefull_features].iloc[val_ind]&para;<br>        y_train, y_val = pre_train[target][tr_ind], pre_train[target][val_ind]&para;<br>        train_set = lgb.Dataset(x_train, y_train)&para;<br>        val_set = lgb.Dataset(x_val, y_val)&para;<br>        &para;<br>        model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, &para;<br>                         valid_sets = [train_set, val_set], verbose_eval = 100)&para;<br>        &para;<br>        oof_pred[val_ind] = model.predict(x_val)&para;<br>        &para;<br>        y_pred += model.predict(pre_test[usefull_features]) / kf.n_splits&para;<br>        &para;<br>        # get fold importance df&para;<br>        fold_importance = pd.DataFrame({'features': usefull_features})&para;<br>        fold_importance['fold'] = fold + 1&para;<br>        fold_importance['importance'] = model.feature_importance()&para;<br>        feature_importance = pd.concat([feature_importance, fold_importance])&para;<br>        &para;<br>    # round predictions&para;<br>    rmse_score = np.sqrt(mean_squared_error(pre_trian[target], oof_pred))&para;<br>    print('Our oof rmse score is: ', rmse_score)&para;<br>    # want to clip and then round predictions&para;<br>    oof_pred = np.round(np.clip(oof_pred, 0, 10)).astype(int)&para;<br>    y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)&para;<br>    cohen_score = cohen_kappa_score(pre_train[target], oof_pred, weights = 'quadratic')&para;<br>    print('Our oof cohen kappa score is: ', cohen_score)&para;<br>    &para;<br>    # plot feature importance&para;<br>    fi_mean = feature_importance.groupby(['features'])['importance'].mean().reset_index()&para;<br>    fi_mean.sort_values('importance', ascending = False, inplace = True)&para;<br>    plt.figure(figsize = (12, 14))&para;<br>    sns.barplot(x = fi_mean['importance'], y = fi_mean['features'])&para;<br>    plt.xlabel('Importance', fontsize = 13)&para;<br>    plt.ylabel('Feature', fontsize = 13)&para;<br>    plt.tick_params(axis = 'x', labelsize = 11)&para;<br>    plt.tick_params(axis = 'y', labelsize = 11)&para;<br>    plt.title('Light Gradient Boosting Feature Importance (5 KFold)')&para;<br>    plt.show()&para;<br>    &para;<br>    &para;<br>    return oof_pred, y_pred, feature_importance&para;<br>&para;<br>&para;<br># define hyperparammeter (some random hyperparammeters)&para;<br>params = {'learning_rate': 0.</span><del style=\"background:#ffe6e6;\">2</del><ins style=\"background:#e6ffe6;\">3</ins><span>, &para;<br>          'feature_fraction': 0.75, &para;<br>          'bagging_fraction': 0.75,&para;<br>          'bagging_freq': 1,&para;<br>          'n_jobs': -1, &para;<br>          'seed': 50,&para;<br>          'metric': 'rmse'&para;<br>        }&para;<br>&para;<br>&para;<br>&para;<br># define the features for training&para;<br>features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]&para;<br>&para;<br>oof_pred, y_pred, feature_importance = run_lgb(pre_train, pre_test, features, params)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def run_lgb(pre_trian, pre_test, usefull_features, params):&para;<br>    &para;<br>    kf = KFold(n_splits = 5, shuffle = True, random_state = 42)&para;<br>    target = 'open_channels'&para;<br>    oof_pred = np.zeros(len(pre_train))&para;<br>    y_pred = np.zeros(len(pre_test))&para;<br>    feature_importance = pd.DataFrame()&para;<br>    &para;<br>    # train a baseline model and record the weighted cohen kappa score &para;<br>    for fold, (tr_ind, val_ind) in enumerate(kf.split(pre_train)):&para;<br>        print('Fold {}'.format(fold + 1))&para;<br>        x_train, x_val = pre_train[usefull_features].iloc[tr_ind], pre_train[usefull_features].iloc[val_ind]&para;<br>        y_train, y_val = pre_train[target][tr_ind], pre_train[target][val_ind]&para;<br>        train_set = lgb.Dataset(x_train, y_train)&para;<br>        val_set = lgb.Dataset(x_val, y_val)&para;<br>        &para;<br>        model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, &para;<br>                         valid_sets = [train_set, val_set], verbose_eval = 100)&para;<br>        &para;<br>        oof_pred[val_ind] = model.predict(x_val)&para;<br>        &para;<br>        y_pred += model.predict(pre_test[usefull_features]) / kf.n_splits&para;<br>        &para;<br>        # get fold importance df&para;<br>        fold_importance = pd.DataFrame({'features': usefull_features})&para;<br>        fold_importance['fold'] = fold + 1&para;<br>        fold_importance['importance'] = model.feature_importance()&para;<br>        feature_importance = pd.concat([feature_importance, fold_importance])&para;<br>        &para;<br>    # round predictions&para;<br>    rmse_score = np.sqrt(mean_squared_error(pre_trian[target], oof_pred))&para;<br>    print('Our oof rmse score is: ', rmse_score)&para;<br>    # want to clip and then round predictions&para;<br>    oof_pred = np.round(np.clip(oof_pred, 0, 10)).astype(int)&para;<br>    y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)&para;<br>    cohen_score = cohen_kappa_score(pre_train[target], oof_pred, weights = 'quadratic')&para;<br>    print('Our oof cohen kappa score is: ', cohen_score)&para;<br>    &para;<br>    # plot feature importance&para;<br>    fi_mean = feature_importance.groupby(['features'])['importance'].mean().reset_index()&para;<br>    fi_mean.sort_values('importance', ascending = False, inplace = True)&para;<br>    plt.figure(figsize = (12, 14))&para;<br>    sns.barplot(x = fi_mean['importance'], y = fi_mean['features'])&para;<br>    plt.xlabel('Importance', fontsize = 13)&para;<br>    plt.ylabel('Feature', fontsize = 13)&para;<br>    plt.tick_params(axis = 'x', labelsize = 11)&para;<br>    plt.tick_params(axis = 'y', labelsize = 11)&para;<br>    plt.title('Light Gradient Boosting Feature Importance (5 KFold)')&para;<br>    plt.show()&para;<br>    &para;<br>    &para;<br>    return oof_pred, y_pred, feature_importance&para;<br>&para;<br>&para;<br># define hyperparammeter (some random hyperparammeters)&para;<br>params = {'learning_rate': 0.</span><del style=\"background:#ffe6e6;\">3</del><ins style=\"background:#e6ffe6;\">1</ins><span>, &para;<br>          'feature_fraction': 0.75, &para;<br>          'bagging_fraction': 0.75,&para;<br>          'bagging_freq': 1,&para;<br>          'n_jobs': -1, &para;<br>          'seed': 50,&para;<br>          'metric': 'rmse'&para;<br>        }&para;<br>&para;<br>&para;<br>&para;<br># define the features for training&para;<br>features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]&para;<br>&para;<br>oof_pred, y_pred, feature_importance = run_lgb(pre_train, pre_test, features, params)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def run_lgb(pre_tr</span><del style=\"background:#ffe6e6;\">i</del><span>a</span><ins style=\"background:#e6ffe6;\">i</ins><span>n, pre_test, usefull_features, params):&para;<br>    &para;<br>    kf = KFold(n_splits = 5, shuffle = True, random_state = 42)&para;<br>    target = 'open_channels'&para;<br>    oof_pred = np.zeros(len(pre_train))&para;<br>    y_pred = np.zeros(len(pre_test))&para;<br>    feature_importance = pd.DataFrame()&para;<br>    &para;<br>    # train a baseline model and record the weighted cohen kappa score &para;<br>    for fold, (tr_ind, val_ind) in enumerate(kf.split(pre_train)):&para;<br>        print('Fold {}'.format(fold + 1))&para;<br>        x_train, x_val = pre_train[usefull_features].iloc[tr_ind], pre_train[usefull_features].iloc[val_ind]&para;<br>        y_train, y_val = pre_train[target][tr_ind], pre_train[target][val_ind]&para;<br>        train_set = lgb.Dataset(x_train, y_train)&para;<br>        val_set = lgb.Dataset(x_val, y_val)&para;<br>        &para;<br>        model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, &para;<br>                         valid_sets = [train_set, val_set], verbose_eval = 100)&para;<br>        &para;<br>        oof_pred[val_ind] = model.predict(x_val)&para;<br>        &para;<br>        y_pred += model.predict(pre_test[usefull_features]) / kf.n_splits&para;<br>        &para;<br>        # get fold importance df&para;<br>        fold_importance = pd.DataFrame({'features': usefull_features})&para;<br>        fold_importance['fold'] = fold + 1&para;<br>        fold_importance['importance'] = model.feature_importance()&para;<br>        feature_importance = pd.concat([feature_importance, fold_importance])&para;<br>        &para;<br>    # round predictions&para;<br>    rmse_score = np.sqrt(mean_squared_error(pre_trian[target], oof_pred))&para;<br>    print('Our oof rmse score is: ', rmse_score)&para;<br>    # want to clip and then round predictions&para;<br>    oof_pred = np.round(np.clip(oof_pred, 0, 10)).astype(int)&para;<br>    y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)&para;<br>    cohen_score = cohen_kappa_score(pre_train[target], oof_pred, weights = 'quadratic')&para;<br>    print('Our oof cohen kappa score is: ', cohen_score)&para;<br>    &para;<br>    # plot feature importance&para;<br>    fi_mean = feature_importance.groupby(['features'])['importance'].mean().reset_index()&para;<br>    fi_mean.sort_values('importance', ascending = False, inplace = True)&para;<br>    plt.figure(figsize = (12, 14))&para;<br>    sns.barplot(x = fi_mean['importance'], y = fi_mean['features'])&para;<br>    plt.xlabel('Importance', fontsize = 13)&para;<br>    plt.ylabel('Feature', fontsize = 13)&para;<br>    plt.tick_params(axis = 'x', labelsize = 11)&para;<br>    plt.tick_params(axis = 'y', labelsize = 11)&para;<br>    plt.title('Light Gradient Boosting Feature Importance (5 KFold)')&para;<br>    plt.show()&para;<br>    &para;<br>    &para;<br>    return oof_pred, y_pred, feature_importance&para;<br>&para;<br>&para;<br># define hyperparammeter (some random hyperparammeters)&para;<br>params = {'learning_rate': 0.</span><del style=\"background:#ffe6e6;\">1</del><ins style=\"background:#e6ffe6;\">05</ins><span>, &para;<br>          'feature_fraction': 0.75, &para;<br>          'bagging_fraction': 0.75,&para;<br>          'bagging_freq': 1,&para;<br>          'n_jobs': -1, &para;<br>          'seed': 50,&para;<br>          'metric': 'rmse'&para;<br>        }&para;<br>&para;<br>&para;<br>&para;<br># define the features for training&para;<br>features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]&para;<br>&para;<br>oof_pred, y_pred, feature_importance = run_lgb(pre_train, pre_test, features, params)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>def rolling_features(train, test):&para;<br>    &para;<br>    pre_train = train.copy()&para;<br>    pre_test = test.copy()&para;<br>    &para;<br>        &para;<br>    for df in [pre_train, pre_test]:&para;<br>        for window in [1000, 5000, 10000, 20000]:&para;<br>            &para;<br>            # roll backwards&para;<br>            df['signalstd_t' + str(window)] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(1).rolling(window).std())&para;<br>            df['signalvar_t' + str(window)] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(1).rolling(window).var())&para;<br>            df['signalmin_t' + str(window)] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(1).rolling(window).min())&para;<br>            df['signalmax_t' + str(window)] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(1).rolling(window).max())&para;<br>&para;<br>            min_max = (df['signal'] - df['signalmin_t' + str(window)]) / (df['signalmax_t' + str(window)] - df['signalmin_t' + str(window)])&para;<br>            df['norm_t' + str(window)] = min_max * (np.floor(df['signalmax_t' + str(window)]) - np.ceil(df['signalmin_t' + str(window)]))&para;<br>&para;<br>            # roll forward&para;<br>            df['signalstd_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(- window - 1).rolling(window).std())&para;<br>            df['signalvar_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(- window - 1).rolling(window).var())&para;<br>            df['signalmin_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(- window - 1).rolling(window).min())&para;<br>            df['signalmax_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].transform(lambda x: x.shift(- window - 1).rolling(window).max())&para;<br>            &para;<br>            min_max = (df['signal'] - df['signalmin_t' + str(window) + '_lead']) / (df['signalmax_t' + str(window) + '_lead'] - df['signalmin_t' + str(window) + '_lead'])&para;<br>            df['norm_t' + str(window) + '_lead'] = min_max * (np.floor(df['signalmax_t' + str(window) + '_lead']) - np.ceil(df['signalmin_t' + str(window) + '_lead']))&para;<br>                &para;<br>    del train, test, min_max&para;<br>    &para;<br>    return pre_train, pre_test&para;<br>&para;<br>def static_batch_features(df):&para;<br>    &para;<br></span><ins style=\"background:#e6ffe6;\">    # thanks to https://www.kaggle.com/jazivxt/physically-possible for this feature engineering part&para;<br></ins><span>    df = df.sort_values(by=['time']).reset_index(drop=True)&para;<br>    df.index = ((df.time * 10_000) - 1).values&para;<br>    df['batch'] = df.index // 25_000&para;<br>    df['batch_index'] = df.index  - (df.batch * 25_000)&para;<br>    df['batch_slices'] = df['batch_index']  // 2500&para;<br>    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)&para;<br>&para;<br>    for c in ['batch','batch_slices2']:&para;<br>        d = {}&para;<br>        d['mean'+c] = df.groupby([c])['signal'].mean()&para;<br>        d['median'+c] = df.groupby([c])['signal'].median()&para;<br>        d['max'+c] = df.groupby([c])['signal'].max()&para;<br>        d['min'+c] = df.groupby([c])['signal'].min()&para;<br>        d['std'+c] = df.groupby([c])['signal'].std()&para;<br>        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))&para;<br>        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))&para;<br>        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))&para;<br>        d['range'+c] = d['max'+c] - d['min'+c]&para;<br>        d['maxtomin'+c] = d['max'+c] / d['min'+c]&para;<br>        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2&para;<br>        for v in d:&para;<br>            df[v] = df[c].map(d[v].to_dict())&para;<br>&para;<br></span><del style=\"background:#ffe6e6;\">&para;<br>    #add shifts&para;<br>    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])&para;<br>    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]&para;<br>    for i in df[df['batch_index']==0].index:&para;<br>        df['signal_shift_+1'][i] = np.nan&para;<br>    for i in df[df['batch_index']==49999].index:&para;<br>        df['signal_shift_-1'][i] = np.nan&para;<br>&para;<br></del><span>    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:&para;<br>        df[c+'_msignal'] = df[c] - df['signal']&para;<br>        &para;<br>    df.reset_index(drop = True, inplace = True)&para;<br>        &para;<br>    return df&para;<br>&para;<br># feature engineering&para;<br>pre_train1, pre_test1 = rolling_features(train, test)&para;<br>pre_train2 = static_batch_features(train)&para;<br>pre_test2 = static_batch_features(test)</span><del style=\"background:#ffe6e6;\">&para;<br># fill missing values with 0 and scale each batch&para;<br># pre_train, pre_test = scale_fillna(pre_train, pre_test)</del><span>&para;<br>&para;<br># join features for training&para;<br>feat = [col for col in pre_train2.columns if col not in ['open_channels', 'signal', 'time', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]&para;<br>pre_train = pd.concat([pre_train1, pre_train2[feat]], axis = 1)&para;<br>pre_test = pd.concat([pre_test1, pre_test2[feat]], axis = 1)&para;<br>del pre_train1, pre_train2, pre_test1, pre_test2&para;<br>&para;<br>del train, test&para;<br>gc.collect()</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span># use the model to make predictions with the test data&para;<br>y_pred = model</span><ins style=\"background:#e6ffe6;\">_lr</ins><span>.predict(X_test)&para;<br># how did our model perform?&para;<br>count_misclassified = (y_test != y_pred).sum()&para;<br>print('Misclassified samples: {}'.format(count_misclassified))&para;<br>accuracy = metrics.accuracy_score(y_test, y_pred)&para;<br>print('Accuracy: {:.2f}'.format(accuracy))&para;<br>print(accuracy_score(y_test, y_pred))&para;<br>#print(confusion_matrix(y_test, y_pred))&para;<br>confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])&para;<br>plt.figure(figsize = (16,16))&para;<br>sns.heatmap(confusion_matrix, annot=True, linewidths=.5)&para;<br>plt.title('Confusion Matrix', fontsize=10)&para;<br>plt.show()</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span>from scipy import signal&para;<br>&para;<br>batch = train_detrend[train_detrend['batch_500000']==1]&para;<br>window_len = </span><del style=\"background:#ffe6e6;\">5</del><ins style=\"background:#e6ffe6;\">10</ins><span>1&para;<br>train_detrend['signal'] = signal.medfilt(train_detrend['signal'], window_len)&para;<br></span><ins style=\"background:#e6ffe6;\">#train_detrend['open_channels'] = signal.medfilt(train_detrend['open_channels'], window_len)&para;<br></ins><span>test_detrend['signal'] = signal.medfilt(test_detrend['signal'], window_len)&para;<br>&para;<br>plt.figure()&para;<br>plt.plot(train_detrend['signal'])&para;<br>plt.show()&para;<br>plt.figure()&para;<br>plt.plot(train_detrend['open_channels'])&para;<br>plt.show()&para;<br>plt.figure()&para;<br>plt.plot(test_detrend['signal'])&para;<br>plt.show()&para;<br></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><span style=\"color:red\">------------------------------</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_break = '<br><span style=\"color:red\">------------------------------</span>'\n",
    "for i in range(150,170):\n",
    "    compare_diff(examples,i)\n",
    "    display(HTML(html_break))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about the 'Kaggle Diffs'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_examples = !shuf -n 10000 /homes/gws/mikeam/RobustDataScience/diffs_new.jsonl\n",
    "diff_examples = [json.loads(x) for x in diff_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/imet-2019-fgvc6/mathormad/14614696.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-# train all layers\u001b[39m\n",
      "\u001b[32m+                  \u001b[39m\n",
      " for layer in model.layers:\n",
      "     layer.trainable = True\n",
      " \n",
      " callbacks_list = [checkpoint, csv_logger, reduceLROnPlat]\n",
      " model.compile(loss='binary_crossentropy',\n",
      "\u001b[31m-            # loss=focal_loss,\u001b[39m\n",
      "\u001b[32m+                              \u001b[39m\n",
      "             optimizer=Adam(lr=1e-4))\n",
      "\u001b[31m-            # optimizer=AdamAccumulate(lr=1e-4, accum_iters=2))\u001b[39m\n",
      "\u001b[32m+                                                               \u001b[39m\n",
      " \n",
      " model.fit_generator(\n",
      "     train_mixup,\n",
      "     steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
      "     validation_data=validation_generator,\n",
      "     validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
      "     epochs=epochs,\n",
      "     verbose=1,\n",
      "\u001b[31m-    max_queue_size=16, workers=WORKERS, use_multiprocessing=True)\u001b[39m\n",
      "\u001b[32m+    max_queue_size=16, workers=WORKERS, use_multiprocessing=True,\u001b[39m\n",
      "     callbacks=callbacks_list)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/artgor/13437317.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " sub1 = pd.read_csv('../input/lanl-features/submission_1.csv')\n",
      "\u001b[31m-sub1.to_csv('submission_1.csv')\u001b[39m\n",
      "\u001b[32m+sub1.to_csv('submission_1.csv', index=False)\u001b[39m\n",
      "\n",
      "\n",
      " params = {'num_leaves': 32,\n",
      "           'min_data_in_leaf': 79,\n",
      "           'objective': 'gamma',\n",
      "           'max_depth': -1,\n",
      "           'learning_rate': 0.01,\n",
      "           \"boosting\": \"gbdt\",\n",
      "           \"bagging_freq\": 5,\n",
      "           \"bagging_fraction\": 0.8126672064208567,\n",
      "           \"bagging_seed\": 11,\n",
      "           \"metric\": 'mae',\n",
      "           \"verbosity\": -1,\n",
      "           'reg_alpha': 0.1302650970728192,\n",
      "           'reg_lambda': 0.3603427518866501,\n",
      "           'feature_fraction': 0.1\n",
      "          }\n",
      "\u001b[31m-oof_lgb, prediction_lgb, feature_importance = train_model(X_train_scaled, X_test_scaled, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\u001b[39m\n",
      "\u001b[32m+# oof_lgb, prediction_lgb, feature_importance = train_model(X_train_scaled, X_test_scaled, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\u001b[39m\n",
      "\n",
      "\n",
      " submission['time_to_failure'] = prediction_lgb\n",
      "\u001b[31m-submission.to_csv('submission_nn.csv')\u001b[39m\n",
      "\u001b[32m+# submission.to_csv('submission_nn.csv')\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[31m-%%time\u001b[39m\n",
      "\u001b[31m-X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\u001b[39m\n",
      "\u001b[31m-model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1, verbose=-1)\u001b[39m\n",
      "\u001b[31m-model.fit(X_train, y_train, \u001b[39m\n",
      "\u001b[31m-        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\u001b[39m\n",
      "\u001b[31m-        verbose=10000, early_stopping_rounds=200)\u001b[39m\n",
      "\u001b[32m+# %%time\u001b[39m\n",
      "\u001b[32m+# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\u001b[39m\n",
      "\u001b[32m+# model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1, verbose=-1)\u001b[39m\n",
      "\u001b[32m+# model.fit(X_train, y_train, \u001b[39m\n",
      "\u001b[32m+#         eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\u001b[39m\n",
      "\u001b[32m+#         verbose=10000, early_stopping_rounds=200)\u001b[39m\n",
      " \n",
      "\u001b[31m-perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\u001b[39m\n",
      "\u001b[31m-eli5.show_weights(perm, top=50, feature_names=X.columns.tolist())\u001b[39m\n",
      "\u001b[32m+# perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)\u001b[39m\n",
      "\u001b[32m+# eli5.show_weights(perm, top=50, feature_names=X.columns.tolist())\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/planet-understanding-the-amazon-from-space/sanchit2843/12286966.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " class Amazon_dataset(Dataset):\n",
      "\u001b[31m-    def __init__(self,image_dir,y_train,transform = None):\u001b[39m\n",
      "\u001b[32m+    def __init__(self,image_dir,y_train,name,transform = None):\u001b[39m\n",
      " \n",
      "         self.img_dir = image_dir\n",
      "         self.y_train = y_train\n",
      "         self.transform = transform\n",
      "\u001b[31m-        self.id = os.listdir(self.img_dir)\u001b[39m\n",
      "\u001b[32m+        self.id = name\u001b[39m\n",
      "     def __len__(self):\n",
      "         return len(os.listdir(self.img_dir))\n",
      "     def __getitem__(self,idx):\n",
      "\u001b[31m-        img_name = os.path.join(self.img_dir, self.id[idx])\u001b[39m\n",
      "\u001b[32m+        print(self.id[idx])\u001b[39m\n",
      "\u001b[32m+        im_id =  self.id[idx] + '.jpg'\u001b[39m\n",
      "\u001b[32m+        img_name = os.path.join(self.img_dir, im_id)\u001b[39m\n",
      "         image = cv2.imread(img_name)\n",
      "         if self.transform:\n",
      "             image = self.transform(image)\n",
      "         label = torch.from_numpy(self.y_train[idx])\n",
      "\u001b[32m+        label = label.type(torch.cuda.FloatTensor)\u001b[39m\n",
      "         return image,label\n",
      "\n",
      "\n",
      "                 \n",
      " transform = transforms.Compose([transforms.ToPILImage(),\n",
      "                                 transforms.Resize((im_size,im_size)),\n",
      "\u001b[31m-                                transforms.RandomHorizontalFlip(p=0.5),\u001b[39m\n",
      "                                 transforms.RandomVerticalFlip(p=0.5),\n",
      "                                 transforms.ToTensor(),\n",
      "                                 transforms.Normalize([0.311, 0.340, 0.299], [0.167, 0.144, 0.138])\n",
      "                     ])\n",
      " inv_normalize = transforms.Normalize(\n",
      "     mean=[-0.311/0.167, -0.340/0.144, -0.299/0.138],\n",
      "     std=[1/0.167, 1/0.144, 1/0.138]\n",
      " )\n",
      " \n",
      "             \n",
      "\u001b[31m-amazon_data = Amazon_dataset(img_dir,y_train,transform)\u001b[39m\n",
      "\u001b[32m+amazon_data = Amazon_dataset(img_dir,y_train,name,transform)\u001b[39m\n",
      "\u001b[32m+testloader = DataLoader(amazon_data, batch_size=batch_size)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sberbank-russian-housing-market/gunjaagarwal/1178866.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " y_pred = model.predict(dtest)\n",
      "\u001b[31m-                                                   \u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
      "\u001b[31m-\u001b[39m\n",
      " df_sub.to_csv('sub_modified.csv', index=False)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/bigquery-geotab-intersection-congestion/jpmiller/21097703.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-%%opts Points[width=500 height=450 xaxis=None yaxis=None]\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " tt_bos = tt[tt.City == 'Boston'].drop_duplicates('IntersectionId')\n",
      "\u001b[32m+\u001b[39m\n",
      " points_bos = gv.Points(tt_bos, kdims=['Longitude', 'Latitude'], \n",
      "\u001b[31m-                      vdims=['level_0']).opts(color='level_0', cmap='Paired')\u001b[39m\n",
      "\u001b[32m+                      vdims=['level_0']).opts(color='level_0', cmap='Paired',\u001b[39m\n",
      "\u001b[32m+                        width=500, height=450)\u001b[39m\n",
      " tiles = gv.tile_sources.CartoLight()\n",
      "\u001b[31m-hv.Overlay(tiles*points_bos)\u001b[39m\n",
      "\u001b[32m+display(points_bos*tiles)\u001b[39m\n",
      "\n",
      "\n",
      " %%opts Points[width=500 height=450 xaxis=None yaxis=None]\n",
      " \n",
      " tt_phi = tt[tt.City == 'Philadelphia'].drop_duplicates('IntersectionId')\n",
      " points_phi = gv.Points(tt_phi, kdims=['Longitude', 'Latitude'], \n",
      "                       vdims=['level_0']).opts(color='level_0', cmap='Paired')\n",
      " tiles = gv.tile_sources.CartoLight()\n",
      "\u001b[31m-hv.Overlay(tiles*points_phi)\u001b[39m\n",
      "\u001b[32m+display(tiles*points_phi)\u001b[39m\n",
      "\n",
      "\n",
      " index_cols = ['EntryStreetName', 'ExitStreetName', 'EntryHeading', 'ExitHeading']\n",
      " value_cols = ['TimeFromFirstStop_p' + str(i) for i in [20, 40, 50, 60 , 80]]\n",
      " \n",
      "\u001b[31m-def get_times(city, iid, flowlist):\u001b[39m\n",
      "\u001b[32m+def get_times(city, iid, pathlist):\u001b[39m\n",
      "     intersect = train[(train.City == city) & (train.IntersectionId == iid)]\n",
      "\u001b[31m-    flows = intersect.groupby(index_cols)[value_cols].agg(['mean', 'std']).fillna(0)\u001b[39m\n",
      "\u001b[31m-    flows['Flow'] = flowlist\u001b[39m\n",
      "\u001b[31m-    flows.columns = flows.columns.swaplevel()\u001b[39m\n",
      "\u001b[31m-    return flows.sort_index(axis=1)\u001b[39m\n",
      "\u001b[32m+    targets = target_df.loc[intersect.index, :]\u001b[39m\n",
      "\u001b[32m+    intersect = intersect.join(targets)\u001b[39m\n",
      "\u001b[32m+    paths = intersect.groupby(index_cols)[value_cols].agg(['mean', 'std']).fillna(0)\u001b[39m\n",
      "\u001b[32m+    paths['Path'] = pathlist\u001b[39m\n",
      "\u001b[32m+    paths.columns = paths.columns.swaplevel()\u001b[39m\n",
      "\u001b[32m+    return paths.sort_index(axis=1)\u001b[39m\n",
      " \n",
      "\u001b[31m-flowlist_bos = ['E_left', 'E_right', 'NE_left', 'SW_right', 'NE_thru', 'SW_u', 'SW_thru']\u001b[39m\n",
      "\u001b[31m-bos = get_times('Boston', 2, flowlist_bos)\u001b[39m\n",
      "\u001b[32m+pathlist_bos = ['E_left', 'E_right', 'NE_left', 'SW_right', 'NE_thru', 'SW_u', 'SW_thru']\u001b[39m\n",
      "\u001b[32m+bos = get_times('Boston', 2, pathlist_bos)\u001b[39m\n",
      " \n",
      "\u001b[31m-flowlist_phi = ['N_thru', 'N_right', 'E_left', 'E_thru']\u001b[39m\n",
      "\u001b[31m-phi = get_times('Philadelphia', 1824, flowlist_phi)\u001b[39m\n",
      "\u001b[32m+pathlist_phi = ['N_thru', 'N_right', 'E_left', 'E_thru']\u001b[39m\n",
      "\u001b[32m+phi = get_times('Philadelphia', 1824, pathlist_phi)\u001b[39m\n",
      " \n",
      " display(bos, phi)\n",
      "\n",
      "\n",
      " import hvplot.pandas\n",
      " opts = {'invert_yaxis': False,\n",
      "         'yticks': list(range(0,100,20)),\n",
      "         'padding': 0.1,\n",
      "         'width':450,\n",
      "         'height': 300,\n",
      "            }\n",
      " \n",
      "           \n",
      "                 \n",
      " def make_plot(df, aggfunc):\n",
      "     assert (aggfunc == 'mean') | (aggfunc == 'std')\n",
      "\u001b[31m-    flows = df.set_index(('', 'Flow')).loc[:, aggfunc].reset_index()\u001b[39m\n",
      "\u001b[31m-    flows.columns = [flows.columns[0][1]] + [c[-4:] for c in flows.columns[1:]]\u001b[39m\n",
      "\u001b[31m-    plot = hvplot.parallel_coordinates(flows, 'Flow', **opts)\u001b[39m\n",
      "\u001b[32m+    paths = df.set_index(('', 'Path')).loc[:, aggfunc].reset_index()\u001b[39m\n",
      "\u001b[32m+    paths.columns = [paths.columns[0][1]] + [c[-4:] for c in paths.columns[1:]]\u001b[39m\n",
      "\u001b[32m+    plot = hvplot.parallel_coordinates(paths, 'Path', **opts)\u001b[39m\n",
      "     if aggfunc == 'mean':\n",
      "         return plot.options(ylabel='Mean Wait Time')\n",
      "     else:\n",
      "         return plot.options(ylabel='STD of Wait Times', show_legend=False)\n",
      " \n",
      " land_cambridge = make_plot(bos, 'mean').options(title=\"Land & Cambridgeside\") +\\\n",
      "     make_plot(bos, 'std')\n",
      " fifth_cambria = make_plot(phi, 'mean').options(title=\"5th & Cambria\") +\\\n",
      "     make_plot(phi, 'std')\n",
      " \n",
      " display(land_cambridge, fifth_cambria)\n",
      "\n",
      "@@ -1,20 +1,21 @@\n",
      " opts = {'cmap': 'Paired',\n",
      "         'yticks': list(range(0,300,50)),\n",
      "         'colorbar': False,\n",
      "        'grid': True,\n",
      "          }\n",
      "\u001b[32m+\u001b[39m\n",
      " land_ne = tt[(tt.IntersectionId == 2) &\n",
      "              (tt.EntryStreetName == 'Land Boulevard') &\n",
      "              (tt.ExitHeading == 'NE')\n",
      "\u001b[31m-             ]\u001b[39m\n",
      "\u001b[32m+             ].join(target_df)\u001b[39m\n",
      " landplot = land_ne.hvplot.scatter('Hour', 'TimeFromFirstStop_p80', \n",
      "\u001b[31m-                                  c='Weekend', **opts)\u001b[39m\n",
      "\u001b[32m+                                    c='Weekend', **opts)\u001b[39m\n",
      " \n",
      " cambria_e = tt[(tt.IntersectionId == 1824) &\n",
      "              (tt.EntryStreetName == 'West Cambria Street') &\n",
      "              (tt.ExitHeading == 'E')\n",
      "\u001b[31m-             ]\u001b[39m\n",
      "\u001b[32m+             ].join(target_df)\u001b[39m\n",
      " cambplot = cambria_e.hvplot.scatter('Hour', 'TimeFromFirstStop_p80', \n",
      "                                     c='Weekend', **opts)\n",
      " \n",
      " display(landplot.options(title='Land_NE_thru'), cambplot.options(title='Cambria_E_thru'))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/towsifahamed/31587966.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " results = []\n",
      " for idx in test_df.groupby([\"Province/State\", \"Country/Region\"]).count().index:\n",
      "     test_df_on_idx = test_df[(test_df[\"Province/State\"] == idx[0]) &\n",
      "                              (test_df[\"Country/Region\"] == idx[1])]\n",
      "     train_df_on_idx = train_df[(all_df[\"Country/Region\"] == idx[1]) &\n",
      "                                (all_df[\"Province/State\"] == idx[0])]\n",
      "\u001b[31m-    inputs = np.array(train_df_on_idx[[\"Date\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[-maxlen:]\u001b[39m\n",
      "\u001b[32m+    inputs = np.array(train_df_on_idx[[\"Date_num\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[-maxlen:]\u001b[39m\n",
      "     inputs = inputs.reshape(maxlen, input_number)\n",
      "     for day in range(43):\n",
      "\u001b[31m-        if datetime.timedelta(days=day) + test_df_on_idx[\"Date\"].min() in train_df_on_idx[\"Date\"].values:\u001b[39m\n",
      "\u001b[31m-            print(\"find the day in teacher\")\u001b[39m\n",
      "\u001b[32m+        if int(1000000000*(datetime.timedelta(days=day) + test_df_on_idx[\"Date\"].min()).timestamp()) in train_df_on_idx[\"Date\"].values.tolist():\u001b[39m\n",
      "             result = np.array(train_df_on_idx[train_df_on_idx[\"Date\"] == (datetime.timedelta(days=day) + test_df_on_idx[\"Date\"].min())][[\"Date\", \"Lat\", \"Long\", \"ConfirmedCases\", \"Fatalities\"]])[0, 3:]\n",
      "         else:\n",
      "             result = model.predict(np.array(inputs).reshape(1, maxlen, input_number)).reshape(-1)\n",
      "\u001b[31m-        print(result)\u001b[39m\n",
      "\u001b[31m-        inputs = np.concatenate((inputs[1:], np.append(inputs[-1, :3], result).reshape(1, input_number)), axis=0)\u001b[39m\n",
      "\u001b[32m+            inputs = np.concatenate((inputs[1:], np.append(inputs[-1, :3], result).reshape(1, input_number)), axis=0)\u001b[39m\n",
      "         results.append([10**(result[0]*cases_max), result[1]*fatal_max])\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/prafultickoo/2765965.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                                                                                                                       \n",
      " def non_linear_forward(Z,activation):\n",
      "     if activation == 'sigmoid':\n",
      "\u001b[31m-        A = (np.exp(Z)/(1 + np.exp(-Z)))\u001b[39m\n",
      "\u001b[32m+        A = 1 /(1 + np.exp(-Z))\u001b[39m\n",
      "     elif activation == 'relu':\n",
      "         if Z > 0:\n",
      "             A = Z\n",
      "         else:\n",
      "             A = 0.01 * Z\n",
      "             \n",
      "     return A\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/ds4g-environmental-insights-explorer/rai555/30749367.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                                 \n",
      " def get_map(lat=18.200178, lon=-66.664513, tiles='cartodbdark_matter', zoom=9):                                    \n",
      "     map_pr = folium.Map(location=[lat, lon], tiles=tiles, zoom_start=zoom) \n",
      "     map_pr.fit_bounds([[17.90, -67.11], [18.56, -65.59]])                           \n",
      "     return map_pr\n",
      " \n",
      "                                                                                                      \n",
      " \n",
      "                                                  \n",
      " def plot_points_on_map(df,m, by_capacity=False):\n",
      "     for i in range(0,len(df)):\n",
      "         popup = folium.Popup(str(df.name.iloc[i])) \n",
      "         \n",
      "                                                                         \n",
      "         if by_capacity == True:\n",
      "             radius = df.capacity_mw.iloc[i]/42\n",
      "         else:\n",
      "             radius=10\n",
      "         \n",
      "                                                                      \n",
      "         fossil_fuels = ['Coal', 'Gas', 'Oil']\n",
      "         if df.primary_fuel.iloc[i] in fossil_fuels:\n",
      "             folium.CircleMarker(location=[df['latitude'].iloc[i],df['longitude'].iloc[i]],popup=popup,radius=radius,color='red',fill=True,fill_color='red').add_to(m)   \n",
      "         else:\n",
      "             folium.CircleMarker(location=[df['latitude'].iloc[i],df['longitude'].iloc[i]],popup=popup,radius=radius,color='green',fill=True,fill_color='green').add_to(m)     \n",
      "     return(m)\n",
      " \n",
      "\u001b[31m-colormap = cm.get_cmap('plasma')\u001b[39m\n",
      "\u001b[32m+colormap = cm.get_cmap('magma')\u001b[39m\n",
      " \n",
      "                                             \n",
      " def overlay_image_on_puerto_rico(m,file_name,band_layer,lat_min=18.6, lon_min=-67.3,lat_max=17.9, lon_max=-65.2, origin='lower', name=None):\n",
      "     band = rio.open(file_name).read(band_layer)\n",
      "                                                   \n",
      "     vmin, vmax = np.nanpercentile(band, (5,95))\n",
      "     folium.raster_layers.ImageOverlay(\n",
      "         image=band,\n",
      "         origin=origin,\n",
      "         bounds = [[lat_min,lon_min,],[lat_max,lon_max]],\n",
      "\u001b[31m-        opacity = 0.75,\u001b[39m\n",
      "\u001b[32m+        opacity = 0.70,\u001b[39m\n",
      "         name = name,\n",
      "         colormap=lambda x: colormap((x - vmin) / (vmax - vmin))\n",
      "     ).add_to(m)\n",
      "     return m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/expedia-hotel-recommendations/gautamsihag/227546.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-                                  \u001b[39m\n",
      "\u001b[31m-sns.countplot(y='date_time', data=train)\u001b[39m\n",
      "\u001b[32m+                              \u001b[39m\n",
      "\u001b[32m+sns.countplot(y='srch_adults_cnt', hue='srch_children_cnt', data=trainm)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/santa-workshop-tour-2019/xhlulu/24409677.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,10 +1,9 @@\n",
      "\u001b[31m-choice_matrix = data.loc[:, 'choice_0': 'choice_9'].values\u001b[39m\n",
      " best = stochastic_product_search(\n",
      "\u001b[31m-    data=data, \u001b[39m\n",
      "     choice_matrix=choice_matrix, \n",
      "     top_k=2,\n",
      "\u001b[31m-    fam_size=8, \u001b[39m\n",
      "\u001b[31m-    original=original, \u001b[39m\n",
      "\u001b[31m-    n_iter=200000, \u001b[39m\n",
      "\u001b[31m-    verbose=10000\u001b[39m\n",
      "\u001b[32m+    fam_size=12, \u001b[39m\n",
      "\u001b[32m+    original=best, \u001b[39m\n",
      "\u001b[32m+    n_iter=50, \u001b[39m\n",
      "\u001b[32m+    disable_tqdm=True,\u001b[39m\n",
      "\u001b[32m+    verbose=5000\u001b[39m\n",
      " )\n",
      "\n",
      "\n",
      " submission['assigned_day'] = best\n",
      "\u001b[31m-submission.to_csv('submission.csv')\u001b[39m\n",
      "\u001b[32m+final_score = cost_function(best)\u001b[39m\n",
      "\u001b[32m+submission.to_csv(f'submission_{int(final_score)}.csv')\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/global-wheat-detection/sumitjha19/33619148.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                                        \n",
      "\u001b[31m-X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\u001b[39m\n",
      "\u001b[32m+X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\u001b[39m\n",
      " Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
      " print('Getting and resizing train images and masks... ')\n",
      " sys.stdout.flush()\n",
      " \n",
      " for n, id_ in tqdm(enumerate(train_ids[:]), total=len(train_ids)):\n",
      "     path = TRAIN_PATH + id_\n",
      "\u001b[31m-    img = imread(path)[:,:,:IMG_CHANNELS]\u001b[39m\n",
      "\u001b[32m+    img = imread(path)\u001b[39m\n",
      "     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
      "     X_train[n] = img\n",
      "     mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
      "     \n",
      "     id_clean = id_.split('.')[0]\n",
      "     if id_clean in masks.keys():\n",
      "         Y_train[n] = masks[id_clean][:, :, np.newaxis]\n",
      "\u001b[31m-        \u001b[39m\n",
      " \n",
      "                             \n",
      "\u001b[31m-X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\u001b[39m\n",
      "\u001b[32m+X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\u001b[39m\n",
      " sizes_test = list()\n",
      "\u001b[31m-print('Getting and resizing test images... ')\u001b[39m\n",
      "\u001b[32m+print('Getting and resizing test images...')\u001b[39m\n",
      " sys.stdout.flush()\n",
      "\u001b[32m+\u001b[39m\n",
      " for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
      "     path = TEST_PATH + id_\n",
      "\u001b[31m-    img = imread(path)[:,:,:IMG_CHANNELS]\u001b[39m\n",
      "\u001b[32m+    img = imread(path)\u001b[39m\n",
      "     sizes_test.append([img.shape[0], img.shape[1]])\n",
      "     img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
      "     X_test[n] = img\n",
      " \n",
      " print('Done!')\n",
      "\n",
      "\n",
      "                                                                                          \n",
      " \n",
      " def castF(x):\n",
      "     return K.cast(x, K.floatx())\n",
      " \n",
      " def castB(x):\n",
      "     return K.cast(x, bool)\n",
      " \n",
      " def iou_loss_core(true,pred):                                                     \n",
      "     intersection = true * pred\n",
      "     notTrue = 1 - true\n",
      "     union = true + (notTrue * pred)\n",
      " \n",
      "     return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n",
      " \n",
      "\u001b[31m-def competitionMetric2(true, pred):                                             \u001b[39m\n",
      "\u001b[32m+def competitionMetric2(true, pred):\u001b[39m\n",
      " \n",
      "     tresholds = [0.5 + (i * 0.05)  for i in range(5)]\n",
      " \n",
      "                                      \n",
      "     true = K.batch_flatten(true)\n",
      "     pred = K.batch_flatten(pred)\n",
      "     pred = castF(K.greater(pred, 0.5))\n",
      " \n",
      "                                   \n",
      "     trueSum = K.sum(true, axis=-1)\n",
      "     predSum = K.sum(pred, axis=-1)\n",
      " \n",
      "                                          \n",
      "     true1 = castF(K.greater(trueSum, 1))    \n",
      "     pred1 = castF(K.greater(predSum, 1))\n",
      " \n",
      "                                                        \n",
      "     truePositiveMask = castB(true1 * pred1)\n",
      " \n",
      "                                                              \n",
      "     testTrue = tf.boolean_mask(true, truePositiveMask)\n",
      "     testPred = tf.boolean_mask(pred, truePositiveMask)\n",
      " \n",
      "                                           \n",
      "     iou = iou_loss_core(testTrue,testPred) \n",
      "     truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n",
      " \n",
      "                                                          \n",
      "     truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n",
      "     truePositives = K.sum(truePositives)\n",
      " \n",
      "                                                              \n",
      "     trueNegatives = (1-true1) * (1 - pred1)                                   \n",
      "     trueNegatives = K.sum(trueNegatives) \n",
      " \n",
      "     return (truePositives + trueNegatives) / castF(K.shape(true)[0])\n",
      "\n",
      "\n",
      "                    \n",
      "\u001b[31m-inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\u001b[39m\n",
      "\u001b[31m-s = Lambda(lambda x: x / 255) (inputs)\u001b[39m\n",
      "\u001b[32m+inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\u001b[39m\n",
      "\u001b[32m+s = Lambda(lambda x: x / 255) (inputs)                  \u001b[39m\n",
      " \n",
      " c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
      " c1 = Dropout(0.1) (c1)\n",
      " c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
      " p1 = MaxPooling2D((2, 2)) (c1)\n",
      " \n",
      " c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
      " c2 = Dropout(0.1) (c2)\n",
      " c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
      " p2 = MaxPooling2D((2, 2)) (c2)\n",
      " \n",
      " c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
      " c3 = Dropout(0.2) (c3)\n",
      " c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
      " p3 = MaxPooling2D((2, 2)) (c3)\n",
      " \n",
      " c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
      " c4 = Dropout(0.2) (c4)\n",
      " c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
      " p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
      " \n",
      " c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
      " c5 = Dropout(0.3) (c5)\n",
      " c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
      " \n",
      " u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
      " u6 = concatenate([u6, c4])\n",
      " c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
      " c6 = Dropout(0.2) (c6)\n",
      " c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
      " \n",
      " u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
      " u7 = concatenate([u7, c3])\n",
      " c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
      " c7 = Dropout(0.2) (c7)\n",
      " c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
      " \n",
      " u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
      " u8 = concatenate([u8, c2])\n",
      " c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
      " c8 = Dropout(0.1) (c8)\n",
      " c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
      " \n",
      " u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
      " u9 = concatenate([u9, c1], axis=3)\n",
      " c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
      " c9 = Dropout(0.1) (c9)\n",
      " c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
      " \n",
      " outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
      " \n",
      " model = Model(inputs=[inputs], outputs=[outputs])\n",
      " model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[competitionMetric2])\n",
      " model.summary()\n",
      "\n",
      "@@ -1,10 +1,10 @@\n",
      "            \n",
      "\u001b[31m-earlystop = EarlyStopping(patience=15, verbose=1)\u001b[39m\n",
      "\u001b[32m+earlystop = EarlyStopping(patience=10, verbose=1, restore_best_weights=True)\u001b[39m\n",
      " \n",
      "\u001b[31m-hist = model.fit(X_train, \u001b[39m\n",
      "\u001b[31m-                 Y_train,\u001b[39m\n",
      "\u001b[31m-                 validation_split=0.2,\u001b[39m\n",
      "\u001b[31m-                 batch_size=16, \u001b[39m\n",
      "\u001b[31m-                 epochs=40, \u001b[39m\n",
      "\u001b[31m-                 callbacks=[earlystop],\u001b[39m\n",
      "\u001b[31m-                )\u001b[39m\n",
      "\u001b[32m+model.fit(X_train, \u001b[39m\n",
      "\u001b[32m+         Y_train,\u001b[39m\n",
      "\u001b[32m+         validation_split=0.1,\u001b[39m\n",
      "\u001b[32m+         batch_size=32, \u001b[39m\n",
      "\u001b[32m+         epochs=40, \u001b[39m\n",
      "\u001b[32m+         callbacks=[earlystop],\u001b[39m\n",
      "\u001b[32m+        )\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[31m-THRESH = 0.7\u001b[39m\n",
      "\u001b[32m+THRESH = 0.65\u001b[39m\n",
      " \n",
      " preds = model.predict(X_test)[:, :, :, 0]\n",
      " masked_preds = preds > THRESH\n",
      "\n",
      "\n",
      " n_rows = 3\n",
      "\u001b[31m-\u001b[39m\n",
      " f, ax = plt.subplots(n_rows, 3, figsize=(14, 10))\n",
      " \n",
      " for j, idx in enumerate([4,5,6]):\n",
      "     for k, kind in enumerate(['original', 'pred', 'masked_pred']):\n",
      "         if kind == 'original':\n",
      "             img = X_test[idx]\n",
      "         elif kind == 'pred':\n",
      "             img = preds[idx]\n",
      "         elif kind == 'masked_pred':\n",
      "\u001b[31m-            masked_pred = preds[idx] > .75\u001b[39m\n",
      "\u001b[32m+            masked_pred = preds[idx] > THRESH\u001b[39m\n",
      "             img = masked_pred\n",
      "         ax[j, k].imshow(img)\n",
      " \n",
      " plt.tight_layout()\n",
      "\n",
      "\n",
      "                                                     \n",
      " bboxes = list()\n",
      " \n",
      " for j in range(masked_preds.shape[0]):\n",
      "     label_j = label(masked_preds[j, :, :]) \n",
      "\u001b[31m-    props = regionprops(label_j)\u001b[39m\n",
      "\u001b[32m+    props = regionprops(label_j)                                \u001b[39m\n",
      "     bboxes.append(props)\n",
      "\n",
      "\n",
      "\u001b[32m+                                                    \u001b[39m\n",
      " output = dict()\n",
      "\u001b[32m+\u001b[39m\n",
      " for i in range(masked_preds.shape[0]):\n",
      "     bboxes_processed = [get_params_from_bbox(bb.bbox, scaling_factor=SC_FACTOR) for bb in bboxes[i]]\n",
      "     formated_boxes = ['1.0 ' + ' '.join(map(str, bb_m)) for bb_m in bboxes_processed]\n",
      "\u001b[31m-    #if formated_boxes:\u001b[39m\n",
      "\u001b[31m-    #    formated_boxes = formated_boxes[0] \u001b[39m\n",
      "     \n",
      "\u001b[31m-    output[sample_sub[\"image_id\"][i]] = \" \".join(formated_boxes)\u001b[39m\n",
      "\u001b[31m-    #output[sample_sub[\"image_id\"][i]] = formated_boxes\u001b[39m\n",
      "\u001b[32m+    output[sample_sub[\"image_id\"][i]] = \" \".join(formated_boxes)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/grupo-bimbo-inventory-demand/czw123/1016649.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                                                                                  \n",
      "                                                                                           \n",
      "                                                           \n",
      " \n",
      " import numpy as np                 \n",
      " import pandas as pd                                                   \n",
      " \n",
      "                                                               \n",
      "                                                                                                                 \n",
      " \n",
      " from subprocess import check_output\n",
      " print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
      " \n",
      "                                                                      \n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-train = pd.read_csv()\u001b[39m\n",
      "\u001b[32m+use_col = ['Cliente_ID', 'Producto_ID', 'Demanda_uni_equil']\u001b[39m\n",
      "\u001b[32m+print use_col\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/andrewkagel/31757195.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,97 +1,120 @@\n",
      " for country in province_countries:\n",
      "\u001b[32m+                            \u001b[39m\n",
      "     current_country_provinces = province_country_dfs[country]['Province_State'].unique()\n",
      "     for province in current_country_provinces:\n",
      "         current_considered_country_df = province_country_dfs[country][province_country_dfs[country]['Province_State']==province][['ConfirmedCases','Fatalities','Days']].reset_index()\n",
      "         print(country+\" \"+province + \" \" + str(len(current_considered_country_df)))\n",
      "\u001b[31m-        \u001b[39m\n",
      "\u001b[31m-                                                                                                                                                                                                                             \u001b[39m\n",
      "\u001b[31m-                                                         \u001b[39m\n",
      "         current_considered_country_df_copy=current_considered_country_df\n",
      "         \n",
      "\u001b[31m-        for i in range(8):\u001b[39m\n",
      "\u001b[31m-            test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+72), 'ConfirmedCases'] = current_considered_country_df.loc[current_considered_country_df['Days'] == i+72, 'ConfirmedCases'].values[0]\u001b[39m\n",
      "\u001b[31m-            test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+72), 'Fatalities'] = current_considered_country_df.loc[current_considered_country_df['Days'] == i+72, 'Fatalities'].values[0]\u001b[39m\n",
      "\u001b[32m+        for i in range(train_end_day-test_start_day+1):\u001b[39m\n",
      "\u001b[32m+            test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+test_start_day), 'ConfirmedCases'] = current_considered_country_df.loc[current_considered_country_df['Days'] == i+test_start_day, 'ConfirmedCases'].values[0]\u001b[39m\n",
      "\u001b[32m+            test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+test_start_day), 'Fatalities'] = current_considered_country_df.loc[current_considered_country_df['Days'] == i+test_start_day, 'Fatalities'].values[0]\u001b[39m\n",
      "         \n",
      "         indexNames = current_considered_country_df[ current_considered_country_df['ConfirmedCases'] == 0 ].index\n",
      "         current_considered_country_df.drop(indexNames , inplace=True)\n",
      "         \n",
      "                                                                                                                             \n",
      "         cases_train = np.diff(current_considered_country_df['ConfirmedCases'].to_numpy())\n",
      "         fatalities_train = np.diff(current_considered_country_df['Fatalities'].to_numpy())\n",
      "\u001b[31m-        print(np.diff(cases_train))\u001b[39m\n",
      "\u001b[32m+        \u001b[39m\n",
      "                                             \n",
      "         cases_increase_avg = 0\n",
      "         days=0\n",
      "\u001b[31m-        for i in range(max(0,len(cases_train)-10),len(cases_train)-1):\u001b[39m\n",
      "\u001b[32m+        for i in range(len(cases_train)-1):\u001b[39m\n",
      "             cases_increase_avg +=  (cases_train[i+1] -cases_train[i])\n",
      "             days+=1\n",
      "         if(days>0):\n",
      "             cases_increase_avg = int(cases_increase_avg/days)\n",
      "             \n",
      "         days=0\n",
      "         fatal_increase_avg=0\n",
      "\u001b[31m-        for i in range(max(0,len(fatalities_train)-10),len(fatalities_train)-1):\u001b[39m\n",
      "\u001b[32m+        for i in range(len(fatalities_train)-1):\u001b[39m\n",
      "             fatal_increase_avg +=  (fatalities_train[i+1] -fatalities_train[i])\n",
      "             days+=1\n",
      "         if(days>0):\n",
      "             fatal_increase_avg = int(fatal_increase_avg/days)\n",
      "         del current_considered_country_df\n",
      "\u001b[31m-    \u001b[39m\n",
      "         n_steps = max(int(len(cases_train)*0.1),3)\n",
      "\u001b[31m-        print(len(cases_train))\u001b[39m\n",
      "\u001b[31m-        print(\"avg: \"+str(cases_increase_avg)+str(days))\u001b[39m\n",
      "\u001b[32m+        print(\"case increase avg: \"+str(cases_increase_avg)+\" \"+ str(days))\u001b[39m\n",
      "\u001b[32m+        print(\"cases train len: \"+str(len(cases_train)))\u001b[39m\n",
      "         \n",
      "\u001b[31m-        X_cases_val, y_cases_val = split_sequence(cases_train,n_steps)\u001b[39m\n",
      "\u001b[31m-        X_fatal_val, y_fatal_val = split_sequence(fatalities_train,n_steps)\u001b[39m\n",
      "\u001b[32m+                                         \u001b[39m\n",
      "\u001b[32m+        avg_weekly_per_day_case = []\u001b[39m\n",
      "\u001b[32m+        avg_window = 4\u001b[39m\n",
      "\u001b[32m+        avg_step = 2 \u001b[39m\n",
      "\u001b[32m+        if int(len(cases_train)/avg_window) > avg_step:\u001b[39m\n",
      "\u001b[32m+            for i in range(int(len(cases_train)/avg_window)):\u001b[39m\n",
      "\u001b[32m+                    temp_list = cases_train[i*avg_window:i*avg_window+avg_window]\u001b[39m\n",
      "\u001b[32m+                    avg_weekly_per_day_case.append(np.sum(temp_list)/len(temp_list))\u001b[39m\n",
      "\u001b[32m+            avg_weekly_per_day_case = np.array(avg_weekly_per_day_case)\u001b[39m\n",
      "\u001b[32m+            print(\"window avg: \"+str(avg_weekly_per_day_case))\u001b[39m\n",
      "\u001b[32m+                                \u001b[39m\n",
      "\u001b[32m+            X_weekly_avg_val, y_weekly_avg_val = split_sequence(avg_weekly_per_day_case,avg_step)\u001b[39m\n",
      "\u001b[32m+            X_weekly_avg_val = np.reshape(X_weekly_avg_val, (X_weekly_avg_val.shape[0],X_weekly_avg_val.shape[1],1))       \u001b[39m\n",
      "\u001b[32m+            model_weekly_avg=build_model(avg_step)\u001b[39m\n",
      "\u001b[32m+            model_weekly_avg.fit(X_weekly_avg_val, y_weekly_avg_val, epochs=50, verbose=0)\u001b[39m\n",
      "\u001b[32m+            new_entry_avg=X_weekly_avg_val[len(X_weekly_avg_val)-1]\u001b[39m\n",
      "\u001b[32m+            for i in range(int(30/avg_window)+1):\u001b[39m\n",
      "\u001b[32m+                weekly_avg_predict_next = model_weekly_avg.predict(np.reshape(new_entry_avg,(1,avg_step,1)), verbose=0).astype(int)\u001b[39m\n",
      "\u001b[32m+                avg_weekly_per_day_case = np.append(avg_weekly_per_day_case,weekly_avg_predict_next[0])\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+                last_series = np.reshape(new_entry_avg,(1,avg_step,1))\u001b[39m\n",
      "\u001b[32m+                new_entry_avg=np.delete(last_series,[0])\u001b[39m\n",
      "\u001b[32m+                new_entry_avg = np.insert(new_entry_avg,avg_step-1,weekly_avg_predict_next[0])\u001b[39m\n",
      "\u001b[32m+            print(\"7 weeks avg after predict: \"+str(avg_weekly_per_day_case))\u001b[39m\n",
      "         \n",
      "\u001b[32m+        X_cases_val, y_cases_val = split_sequence(cases_train,n_steps)\u001b[39m\n",
      "         X_cases_val = np.reshape(X_cases_val, (X_cases_val.shape[0],X_cases_val.shape[1],1))\n",
      "\u001b[32m+        \u001b[39m\n",
      "\u001b[32m+        X_fatal_val, y_fatal_val = split_sequence(fatalities_train,n_steps)\u001b[39m\n",
      "         X_fatal_val = np.reshape(X_fatal_val, (X_fatal_val.shape[0],X_fatal_val.shape[1],1))\n",
      "         \n",
      "\u001b[31m-        print(X_cases_val.shape)\u001b[39m\n",
      "\u001b[31m-                                                                            \u001b[39m\n",
      "         assert(len(X_fatal_val)==len(X_cases_val))\n",
      "         assert(len(y_fatal_val)==len(y_cases_val))\n",
      "         \n",
      "         model_cases=build_model(n_steps)\n",
      "         model_cases.fit(X_cases_val, y_cases_val, epochs=50, verbose=0)\n",
      "         cases_predict_next = model_cases.predict(np.reshape(X_cases_val[len(X_cases_val)-1],(1,n_steps,1)), verbose=0).astype(int)\n",
      "         \n",
      "         model_fatalities=build_model(n_steps)\n",
      "         model_fatalities.fit(X_fatal_val, y_fatal_val, epochs=50, verbose=0)\n",
      "         fatality_predict_next = model_fatalities.predict(np.reshape(X_fatal_val[len(X_fatal_val)-1],(1,n_steps,1)), verbose=0).astype(int)\n",
      "         \n",
      "\u001b[31m-        test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==80), 'ConfirmedCases'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==79), 'ConfirmedCases'].values[0] + cases_predict_next[0]\u001b[39m\n",
      "\u001b[31m-        test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==80), 'Fatalities'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==79), 'Fatalities'].values[0] + fatality_predict_next[0]\u001b[39m\n",
      "\u001b[32m+        test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==train_end_day+1), 'ConfirmedCases'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==train_end_day), 'ConfirmedCases'].values[0] + cases_predict_next[0]\u001b[39m\n",
      "\u001b[32m+        test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==train_end_day+1), 'Fatalities'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==train_end_day), 'Fatalities'].values[0] + fatality_predict_next[0]\u001b[39m\n",
      "             \n",
      "         print(cases_predict_next[0])\n",
      "         print(X_cases_val[len(X_cases_val)-1])\n",
      "         new_entry_cases=X_cases_val[len(X_cases_val)-1]\n",
      "         new_entry_fatal=X_fatal_val[len(X_fatal_val)-1]\n",
      "         \n",
      "\u001b[31m-        for i in range(34):\u001b[39m\n",
      "\u001b[32m+        for i in range(test_end_day-train_end_day-1):\u001b[39m\n",
      "             \n",
      "\u001b[31m-                                                                     \u001b[39m\n",
      "\u001b[32m+                 \u001b[39m\n",
      "           last_series = np.reshape(new_entry_cases,(1,n_steps,1))\n",
      "           new_entry_cases=np.delete(last_series,[0])\n",
      "           new_entry_cases = np.insert(new_entry_cases,n_steps-1,cases_predict_next[0])\n",
      "\u001b[31m-                                                                                                  \u001b[39m\n",
      "\u001b[31m-    \u001b[39m\n",
      "\u001b[31m-                                                                            \u001b[39m\n",
      "\u001b[31m-                                                                          \u001b[39m\n",
      "         \n",
      "           cases_predict_next = model_cases.predict(np.reshape(new_entry_cases,(1,n_steps,1)), verbose=0).astype(int)\n",
      "           if(cases_predict_next[0]-new_entry_cases[n_steps-1]>cases_increase_avg):\n",
      "             cases_predict_next =  np.array([max(0,new_entry_cases[n_steps-1]+cases_increase_avg)])\n",
      "\u001b[32m+          if (province in ['Kentucky','New Mexico','Sint Maarten','Cayman Islands','Isle of Man'] or (country=='Denmark' and province == 'None*')):\u001b[39m\n",
      "\u001b[32m+            cases_predict_next[0] = avg_weekly_per_day_case[-int(30/avg_window)-1+int(i/avg_window)]\u001b[39m\n",
      "\u001b[32m+          cases_predict_next[0] =  np.array([max(0,cases_predict_next[0])])\u001b[39m\n",
      "           print(np.array(new_entry_cases))\n",
      "           print(cases_predict_next[0])\n",
      " \n",
      "\u001b[32m+                    \u001b[39m\n",
      "           last_series = np.reshape(new_entry_fatal,(1,n_steps,1))\n",
      "           new_entry_fatal=np.delete(last_series,[0])\n",
      "           new_entry_fatal = np.insert(new_entry_fatal,n_steps-1,fatality_predict_next[0])\n",
      "         \n",
      "           fatality_predict_next = model_fatalities.predict(np.reshape(new_entry_fatal,(1,n_steps,1)), verbose=0).astype(int)\n",
      "           if(fatality_predict_next[0]-new_entry_fatal[n_steps-1]>fatal_increase_avg):\n",
      "\u001b[31m-            fatality_predict_next[0] =  max(0,new_entry_fatal[n_steps-1]+fatal_increase_avg)  \u001b[39m\n",
      "\u001b[32m+            fatality_predict_next[0] =  max(0,new_entry_fatal[n_steps-1]+fatal_increase_avg)\u001b[39m\n",
      "\u001b[32m+          fatality_predict_next[0] =  np.array([max(0,fatality_predict_next[0])])\u001b[39m\n",
      "         \n",
      "\u001b[31m-          test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+81), 'ConfirmedCases'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==80), 'ConfirmedCases'].values[0] + cases_predict_next[0]\u001b[39m\n",
      "\u001b[31m-          test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+81), 'Fatalities'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+80), 'Fatalities'].values[0] + fatality_predict_next[0]\u001b[39m\n",
      "\u001b[32m+          test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+train_end_day+2), 'ConfirmedCases'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+train_end_day+1), 'ConfirmedCases'].values[0] + cases_predict_next[0]\u001b[39m\n",
      "\u001b[32m+          test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+train_end_day+2), 'Fatalities'] = test_df.loc[(test_df['Country']==country) & (test_df['Province_State']==province) & (test_df['Days']==i+train_end_day+1), 'Fatalities'].values[0] + fatality_predict_next[0]\u001b[39m\n",
      "\u001b[32m+        del model_fatalities\u001b[39m\n",
      "\u001b[32m+        del model_cases\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/dog-breed-identification/pratikbedre/33341326.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " import numpy as np\n",
      " import pandas as pd\n",
      " import os\n",
      " import matplotlib.pyplot as plt\n",
      " import torch\n",
      " from torch.utils.data import Dataset, DataLoader\n",
      "\u001b[31m-from torchvision import transforms, utils\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " from torch.nn import Conv2d as conv\n",
      " from torch.nn import MaxPool2d as maxpool\n",
      " from torch.nn import MaxUnpool2d as unpool\n",
      " from torch.nn import ReLU as relu\n",
      " from torch.nn import Softmax as softmax\n",
      " from torch.nn import BatchNorm2d as bn\n",
      " from torch.nn import Dropout as drop\n",
      " from torch.nn import Dropout2d as drop2d\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " from torch.nn import Linear as fc\n",
      " from torch.nn import ConvTranspose2d as convT\n",
      " from torch.nn import Identity\n",
      " from torch.nn import MSELoss  as MSE\n",
      " from torch.nn import Sequential\n",
      "\u001b[31m-from torchvision.transforms import ToTensor, Resize, Grayscale, CenterCrop,  ToPILImage, ColorJitter, RandomCrop, RandomHorizontalFlip\u001b[39m\n",
      "\u001b[32m+import albumentations as albu\u001b[39m\n",
      " import torch.optim as optim\n",
      " from torch.optim import lr_scheduler, Adam\n",
      " from PIL import Image\n",
      " from tqdm import tqdm\n",
      "\u001b[31m-from torchvision import models\u001b[39m\n",
      "\u001b[31m-from sklearn.preprocessing import LabelEncoder\u001b[39m\n",
      "\u001b[32m+from albumentations import (HorizontalFlip,VerticalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise,RandomRotate90,Transpose,RandomBrightnessContrast,RandomCrop)\u001b[39m\n",
      "\u001b[32m+from albumentations.pytorch import ToTensor\u001b[39m\n",
      "\u001b[32m+from sklearn.preprocessing import LabelEncoder\u001b[39m\n",
      "\n",
      "\n",
      " train_dir = '/kaggle/input/dog-breed-identification/train/'\n",
      " test_dir = '/kaggle/input/dog-breed-identification/test/'\n",
      "\u001b[31m-train_df = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv' )\u001b[39m\n",
      "\u001b[31m-LE = LabelEncoder()\u001b[39m\n",
      "\u001b[31m-train_df['breed'] = LE.fit_transform(train_df['breed'])\u001b[39m\n",
      "\u001b[31m-train_df.head()\u001b[39m\n",
      "\u001b[32m+data = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv' )\u001b[39m\n",
      "\u001b[32m+                     \u001b[39m\n",
      "\u001b[32m+                                                         \u001b[39m\n",
      "\u001b[32m+data.head()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/cgallay/676030.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " from keras.utils.np_utils import to_categorical\n",
      " from keras.optimizers import SGD\n",
      "\u001b[31m-model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.00001, momentum=0.95, nesterov=True), metrics=['accuracy'])\u001b[39m\n",
      "\u001b[31m-labels = to_categorical(labels, 2)                                            \u001b[39m\n",
      "\u001b[31m-model.fit(train, labels, nb_epoch=5, batch_size=32)\u001b[39m\n",
      "\u001b[32m+model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\u001b[39m\n",
      "\u001b[32m+labels_ = to_categorical(labels, 2)                                            \u001b[39m\n",
      "\u001b[32m+model.fit(train, labels_, nb_epoch=20, batch_size=32)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/tensorflow2-question-answering/riccardodidio/30964543.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                                   \n",
      " le = LabelEncoder()\n",
      " target_label_encoded = le.fit_transform(target)\n",
      "                                                                \n",
      " temp = data.copy()\n",
      " temp['Species'] = target_label_encoded\n",
      "\u001b[31m-temp.corr()\u001b[39m\n",
      "\u001b[32m+temp.corr()['Species'].sort_values(ascending=False)\u001b[39m\n",
      "\n",
      "\n",
      "                            \n",
      " def build_model():\n",
      "\u001b[32m+                                                                                \u001b[39m\n",
      "\u001b[32m+                                                                   \u001b[39m\n",
      "     x = tf.keras.layers.Input(shape=(4,))\n",
      "     dense1 = tf.keras.layers.Dense(32, activation='relu')(x)\n",
      "     y = tf.keras.layers.Dense(3, activation='softmax')(dense1)\n",
      "     model = tf.keras.models.Model(inputs=x, outputs=y)\n",
      "\u001b[31m-    \u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "     return model\n",
      " \n",
      " model = build_model()\n",
      " model.summary()\n",
      "\n",
      "\n",
      " @tf.function\n",
      " def train_step(features, targets):\n",
      "     with tf.GradientTape() as tape:\n",
      "\u001b[32m+                                 \u001b[39m\n",
      "         predictions = model(features)\n",
      "\u001b[32m+                                                      \u001b[39m\n",
      "         loss = loss_object(targets, predictions)\n",
      "\u001b[32m+                                              \u001b[39m\n",
      "     gradients = tape.gradient(loss, model.trainable_variables)\n",
      "\u001b[32m+                                                              \u001b[39m\n",
      "     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[32m+    \u001b[39m\n",
      "     train_loss(loss)\n",
      "     train_accuracy(targets, predictions)\n",
      "     \n",
      " @tf.function\n",
      " def test_step(images, labels):\n",
      "     predictions = model(images)\n",
      "     t_loss = loss_object(labels, predictions)\n",
      " \n",
      "     test_loss(t_loss)\n",
      "     test_accuracy(labels, predictions)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/talkingdata-mobile-user-demographics/anokas/293180.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                                                \n",
      "                                  \n",
      " def create_china_map():\n",
      "                        \n",
      "     lat_min, lat_max = 15, 55\n",
      "\u001b[31m-    lon_min, lon_max = 70, 140\u001b[39m\n",
      "\u001b[32m+    lon_min, lon_max = 75, 135\u001b[39m\n",
      "     \n",
      "                    \n",
      "     m = Basemap(projection='merc',\n",
      "                  llcrnrlat=lat_min,\n",
      "                  urcrnrlat=lat_max,\n",
      "                  llcrnrlon=lon_min,\n",
      "                  urcrnrlon=lon_max,\n",
      "                  lat_ts=35,\n",
      "                  resolution='c')\n",
      "     \n",
      "                \n",
      "     m.fillcontinents(color='#191919',lake_color='#000000')                              \n",
      "     m.drawmapboundary(fill_color='#000000')                                  \n",
      "     m.drawcountries(linewidth=0.1, color=\"w\")                                                   \n",
      "     \n",
      "             \n",
      "     return m\n",
      "\n",
      "\n",
      " m = create_china_map()\n",
      " \n",
      "                                                           \n",
      " x1, y1 = m(lon_min, lat_min)\n",
      " x2, y2 = m(lon_max, lat_max)\n",
      " w, h = (x2-x1)*1.0, (y2-y1)*1.0\n",
      " max_dim = np.max([w, h])\n",
      " \n",
      "                                         \n",
      " max_canvas = 500\n",
      " scale_factor = max_canvas / max_dim\n",
      " if (w>h): canvas_size = [max_canvas, int(max_canvas * (h/w))]\n",
      " elif (w<h): canvas_size = [int(max_canvas * (w/h)), max_canvas]\n",
      " else: canvas_size = (max_canvas, max_canvas)\n",
      "\u001b[31m-blank_canvas = np.zeros((canvas_size[1], canvas_size[0]))\u001b[39m\n",
      " \n",
      "                                                                                       \n",
      " alpha_total1 = np.zeros((canvas_size[1], canvas_size[0]))\n",
      " alpha_total2 = np.zeros((canvas_size[1], canvas_size[0]))\n",
      " for lon, lat in zip(df_events_sample[\"longitude\"].tolist(), df_events_sample[\"latitude\"].tolist()):\n",
      "     cx, cy = m(lon, lat)\n",
      "     cx, cy = cx*scale_factor, cy*scale_factor\n",
      "     g1 = gaussian_blob(canvas_size, fwhm=10, center=(cx,cy))\n",
      "     g2 = gaussian_blob(canvas_size, fwhm=30, center=(cx,cy))\n",
      "     alpha_total1 += g1\n",
      "     alpha_total2 += g2\n",
      "     \n",
      "                 \n",
      " alpha_norm1 = alpha_total1 / alpha_total1.max()\n",
      " alpha_norm2 = alpha_total2 / alpha_total2.max()\n",
      " \n",
      "                                    \n",
      " plt.close()\n",
      " plt.figure()\n",
      " plt.subplot(121)\n",
      " plt.imshow(alpha_norm1)\n",
      " plt.title(\"alpha_norm1\")\n",
      " \n",
      " plt.subplot(122)\n",
      " plt.imshow(alpha_norm2)\n",
      " plt.title(\"alpha_norm2\")\n",
      " plt.show()\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/aerial-cactus-identification/abhijit96/11877588.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " model = Sequential()\n",
      " model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
      " model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
      "\u001b[31m-model.add(Conv2D(16, kernel_size=3, activation='relu'))\u001b[39m\n",
      "\u001b[32m+model.add(Conv2D(8, kernel_size=3, activation='relu'))\u001b[39m\n",
      " model.add(Flatten())\n",
      " model.add(Dense(1, activation='sigmoid'))\n",
      " \n",
      " model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sberbank-russian-housing-market/alanfeder/1164311.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " set.seed(38474)\n",
      " train_rows <- sample(1:nrow(train), nrow(train) * 0.7)\n",
      " \n",
      " \n",
      " train_df <- train %>% mutate(isTest = 0) %>%\n",
      "     mutate_at(c(\"max_floor\", \"material\", \"build_year\", \"num_room\", \"kitch_sq\", \"state\"), is.numeric)\n",
      " test_df  <- test %>% mutate(isTest = 1)\n",
      " \n",
      " full_df <- bind_rows(train_df, test_df) %>% mutate_if(is.character, as.factor)\n",
      " \n",
      " full_mm <- data.matrix(full_df)\n",
      " \n",
      " train_mm1 <- full_mm[full_mm[,'isTest'] == 0,]\n",
      " test_mm1  <- full_mm[full_mm[,'isTest'] == 1,]\n",
      " \n",
      "\u001b[31m-target = train_mm1[,'price_doc']\u001b[39m\n",
      "\u001b[32m+target1 = train_mm1[,'price_doc']\u001b[39m\n",
      " \n",
      " train_mm1 <- train_mm1[,!(colnames(train_mm1) %in% c('price_doc', 'id', 'isTest'))]\n",
      " \n",
      " test_ids <- test_mm1[, 'id']\n",
      " test_mm1 <- test_mm1[,!(colnames(test_mm1) %in% c('price_doc', 'id', 'isTest'))]\n",
      " \n",
      " train_mm <- train_mm1[ train_rows, ]\n",
      " valid_mm <- train_mm1[-train_rows, ]\n",
      " \n",
      " train_target <- target1[ train_rows]\n",
      " valid_target <- target1[-train_rows]\n",
      " \n",
      " train_dm <- xgb.DMatrix(data = train_mm, label = log(train_target))\n",
      " valid_dm <- xgb.DMatrix(data = valid_mm, label = log(valid_target))\n",
      " watchlist <- list(train = train_dm, valid = valid_dm)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/planet-understanding-the-amazon-from-space/prateekjha/15799550.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-for epoch in range(2):\u001b[39m\n",
      "\u001b[32m+# for epoch in range(2):\u001b[39m\n",
      " #     train(epoch)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/state-farm-distracted-driver-detection/xanjay/26651825.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  Xtrain, Ytrain = preprocess_data(trainx, trainy)\u001b[39m\n",
      "\u001b[32m+  Xtrain, Ytrain = preprocess_data(Xtrain, Ytrain)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/whale-detection-challenge/roysai/4750797.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,90 +1,86 @@\n",
      " class data_generator:\n",
      "     def __init__(self, ID_map, ID_lst, batch_size, num_img):\n",
      "         \n",
      "         self.ID_lst=ID_lst\n",
      "         self.batch_size=batch_size\n",
      "         self.num_img=num_img\n",
      "     \n",
      "     def find_neighbors(self):\n",
      "         self.centroid_array=np.zeros((len(ID_map.keys()), OUTPUT_DIMENSION))\n",
      "         \n",
      "         for n, ID in enumerate(ID_map.keys()):\n",
      "             self.centroid_array[n]=ID_map[ID]['centroid']\n",
      "                     \n",
      "         centroid_neighbors=NearestNeighbors(n_neighbors=len(self.ID_lst), algorithm='ball_tree').fit(self.centroid_array)\n",
      "         _, self.nearest_neighbors=centroid_neighbors.kneighbors(self.centroid_array)\n",
      "         \n",
      "         return self.centroid_array\n",
      "         \n",
      "     def generate_positive(self, img_shortlist):\n",
      "         self.pos_sample_lst=[]\n",
      "         for n, image in enumerate(img_shortlist):\n",
      "             ID=check_id(image)\n",
      "             img_lst=ID_map[ID]['images']\n",
      "             idx=img_lst.index(image)\n",
      "             furthest_pos_idx=ID_map[ID]['indices'][idx, len(img_lst)-1]\n",
      "             furthest_pos_img=ID_map[ID]['images'][furthest_pos_idx]\n",
      "             self.pos_sample_lst.append(furthest_pos_img)\n",
      "         return self.pos_sample_lst\n",
      "     \n",
      "     def generate_negative(self, img_shortlist, diff=1):\n",
      "         self.neg_sample_lst=[]\n",
      "         for n, image in enumerate(img_shortlist):\n",
      "             ID=check_id(image)\n",
      "             idx=self.ID_lst.index(ID)\n",
      "                                                  \n",
      "             nearest_centroid=self.ID_lst[self.nearest_neighbors[idx, diff]]                                                     \n",
      "             nearest_nbrs=random.choice(ID_map[nearest_centroid]['images'])\n",
      "             self.neg_sample_lst.append(nearest_nbrs)\n",
      "                                            \n",
      "         return self.neg_sample_lst\n",
      "     \n",
      "     def generate_training_ID_SL(self):\n",
      "         random.shuffle(self.ID_lst)\n",
      "         self.img_shortlist=[]\n",
      "         for ID in ID_SL:\n",
      "             if len(self.img_shortlist)<self.num_img:\n",
      "                 images=df_train['Image'][df_train['Id']==ID].tolist()\n",
      "                 self.img_shortlist.extend(images)                \n",
      "             else:\n",
      "                 break\n",
      "                                    \n",
      " \n",
      "     def generate_training_triplet(self):\n",
      "         while True:\n",
      "             self.generate_training_ID_SL()\n",
      "             m=len(self.img_shortlist)\n",
      "             batches=int(m/self.batch_size)\n",
      "             indexes=np.arange(batches)\n",
      "             ID_map=generate_ID_map(ID_SL)\n",
      "             self.find_neighbors()\n",
      "             \n",
      "             for i in indexes:\n",
      "                 if i==indexes[-1]:\n",
      "                     anchor_lst=self.img_shortlist[i*self.batch_size:]\n",
      "                        \n",
      "                 else:\n",
      "                     anchor_lst=self.img_shortlist[i*self.batch_size:(i+1)*self.batch_size]\n",
      "                 \n",
      "                 pos_lst=self.generate_positive(anchor_lst)\n",
      "                 neg_lst=self.generate_negative(anchor_lst)\n",
      "\u001b[31m-                                \u001b[39m\n",
      "\u001b[31m-                                                              \u001b[39m\n",
      "\u001b[31m-                                                                                            \u001b[39m\n",
      "\u001b[32m+                \u001b[39m\n",
      "                 anchor_img=crop_image(anchor_lst)\n",
      "                 pos_img=crop_image(pos_lst)\n",
      "                 neg_img=crop_image(neg_lst)\n",
      "                 \n",
      "                 yield([pos_img, neg_img, anchor_img], np.zeros(anchor_img.shape[0]))\n",
      "\u001b[31m-                                                                 \u001b[39m\n",
      "\u001b[32m+                \u001b[39m\n",
      " training_images=[]\n",
      " ID_SL=list(ID_SL)\n",
      " print (f'Total number of IDs to be trained is {len(ID_SL)}')\n",
      " num_img=1500\n",
      "\u001b[31m-\u001b[39m\n",
      " batch_size=16\n",
      " steps_per_epoch=int(num_img/batch_size)\n",
      " training_data=data_generator(ID_map, ID_SL, batch_size, num_img)\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-triplet_model.fit_generator(training_data.generate_training_triplet(), steps_per_epoch=steps_per_epoch, epochs=20, workers=2)\u001b[39m\n",
      "\u001b[32m+triplet_model.fit_generator(training_data.generate_training_triplet(), steps_per_epoch=steps_per_epoch, epochs=10, workers=2)\u001b[39m\n",
      " base_model.save_weights('3rd_run 10 epochs')\n",
      "\n",
      "\n",
      " df_val=df[df['train']==False]\n",
      " def generate_encoding(img_lst, batch_size, train=True):\n",
      "     m=len(img_lst)\n",
      "     batches=int(m/batch_size)\n",
      "     indexes=np.arange(batches)\n",
      "     \n",
      "     for i in indexes:\n",
      "         if i!=indexes[-1]:\n",
      "             img_SL=img_lst[i*batch_size:(i+1)*batch_size]\n",
      "         else:\n",
      "             img_SL=img_lst[i*batch_size:]\n",
      "             \n",
      "         img_array=crop_image(img_SL, train=train)\n",
      "         img_encoding=base_model.predict(img_array)\n",
      "         yield img_encoding\n",
      " \n",
      " train_encoding=[]\n",
      " val_encoding=[]\n",
      " \n",
      " print ('Encoding train')\n",
      " for t_en in generate_encoding(df_train['Image'].tolist(), 64, train=True):\n",
      "     train_encoding.extend(t_en)\n",
      " \n",
      " print ('Encoding validate')\n",
      " for v_en in generate_encoding(df_val['Image'].tolist(), 64, train=True):\n",
      "     val_encoding.extend(v_en)\n",
      "\u001b[31m-    \u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sentiment-analysis-on-movie-reviews/etatbak/9732451.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,15 +1,20 @@\n",
      " # This Python 3 environment comes with many helpful analytics libraries installed\n",
      " # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
      " # For example, here's several helpful packages to load in \n",
      " \n",
      " import numpy as np # linear algebra\n",
      " import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
      " import matplotlib.pyplot as plt\n",
      " \n",
      " # Input data files are available in the \"../input/\" directory.\n",
      " # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
      "\u001b[32m+import sys\u001b[39m\n",
      "\u001b[32m+import warnings\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+if not sys.warnoptions:\u001b[39m\n",
      "\u001b[32m+    warnings.simplefilter(\"ignore\")\u001b[39m\n",
      " \n",
      " import os\n",
      " print(os.listdir(\"../input\"))\n",
      " \n",
      " # Any results you write to the current directory are saved as output.\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/expedia-hotel-recommendations/gautamsihag/232070.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  train = pd.read_csv(\"../input/train.csv\", parse_dates=['date_time'], nrows=10000000)\u001b[39m\n",
      "\u001b[32m+  test1 = pd.read_csv(\"../input/test.csv\", parse_dates=['date_time'], nrows=10)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/moghazy/4787301.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "               \n",
      " train['HF1'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])\n",
      " train['HF2'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\n",
      " train['HR1'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\n",
      " train['HR2'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\n",
      " train['FR1'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\n",
      " train['FR2'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n",
      " \n",
      " train['ele_vert'] = train['Elevation']-train['Vertical_Distance_To_Hydrology']\n",
      " train['ele_vert1'] = train['Elevation']+train['Vertical_Distance_To_Hydrology']\n",
      " \n",
      " train['mean_hillshade'] =  (train['Hillshade_9am']  + train['Hillshade_Noon'] + train['Hillshade_3pm'] ) / 3\n",
      " \n",
      " train['MeanHF1'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])/2\n",
      " train['MeanHR1'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])/2\n",
      " train['MeanFR1'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])/2\n",
      " \n",
      " train['MeanNegHF2'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])/2\n",
      " train['MeanNegHR2'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])/2\n",
      " train['MeanNegFR2'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])/2\n",
      " \n",
      " train['slope_hyd'] = np.sqrt(train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)\n",
      " train['Mean_Amenities']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Roadways']) / 3\n",
      " train['Mean_Fire_Hyd']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology']) / 2 \n",
      " \n",
      " train[\"Vertical_Distance_To_Hydrology\"] = abs(train['Vertical_Distance_To_Hydrology'])\n",
      " \n",
      "\u001b[31m-train[\"Slope_slope_hyd\"] = train[\"slope_hyd\"] + train['Slope']\u001b[39m\n",
      " \n",
      " test['HF1'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])\n",
      " test['HF2'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\n",
      " test['HR1'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\n",
      " test['HR2'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\n",
      " test['FR1'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n",
      " test['FR2'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n",
      " \n",
      " test['ele_vert'] = test['Elevation']-test['Vertical_Distance_To_Hydrology']\n",
      " test['ele_vert1'] = test['Elevation'] + test['Vertical_Distance_To_Hydrology']\n",
      " \n",
      " test['mean_hillshade'] = (test['Hillshade_9am']  + test['Hillshade_Noon']  + test['Hillshade_3pm'] ) / 3\n",
      " \n",
      " test['MeanHF1'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])/2\n",
      " test['MeanHR1'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])/2\n",
      " test['MeanFR1'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])/2\n",
      " \n",
      " test['MeanNegHF2'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])/2\n",
      " test['MeanNegHR2'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])/2\n",
      " test['MeanNegFR2'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])/2\n",
      " \n",
      " test['slope_hyd'] = np.sqrt(test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)\n",
      " test['Mean_Amenities']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology'] + test['Horizontal_Distance_To_Roadways']) / 3 \n",
      " test['Mean_Fire_Hyd']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology']) / 2\n",
      " \n",
      " \n",
      "\u001b[31m-test['Vertical_Distance_To_Hydrology'] = abs(test[\"Vertical_Distance_To_Hydrology\"])\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-test[\"Slope_slope_hyd\"] = test[\"slope_hyd\"] + test['Slope']\u001b[39m\n",
      "\u001b[32m+test['Vertical_Distance_To_Hydrology'] = abs(test[\"Vertical_Distance_To_Hydrology\"])\u001b[39m\n",
      "\n",
      "\n",
      " from sklearn.svm import SVC\n",
      " from sklearn.ensemble import ExtraTreesClassifier\n",
      " from sklearn.neural_network import MLPClassifier\n",
      " from sklearn.model_selection import GridSearchCV\n",
      " from sklearn.metrics import classification_report\n",
      " from sklearn.ensemble import AdaBoostClassifier\n",
      " \n",
      " \n",
      "\u001b[31m-clf = ExtraTreesClassifier(n_estimators=900, random_state=0)\u001b[39m\n",
      "\u001b[32m+clf = ExtraTreesClassifier(n_estimators=970, random_state=0)\u001b[39m\n",
      " clf.fit(x_train, y_train)\n",
      " print('Accuracy of SVC on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\n",
      " print('Accuracy of SVC on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))\n",
      " \n",
      "                                                                              \n",
      "                                                                     \n",
      " \n",
      "                                              \n",
      "                                                                                        \n",
      "                             \n",
      " \n",
      "                                                                                                           \n",
      "                                                                                                                   \n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/overfitting/overload10/11775904.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " def create_Feature(df):\n",
      "     ndf=pd.DataFrame()\n",
      "     df['angular_velocity'] = df['angular_velocity_X'] + df['angular_velocity_Y'] + df['angular_velocity_Z']\n",
      "     df['linear_acceleration'] = df['linear_acceleration_X'] + df['linear_acceleration_Y'] + df['linear_acceleration_Z']\n",
      "     df['velocity_to_acceleration'] = df['angular_velocity'] / df['linear_acceleration']\n",
      "     for c in df.columns[3:]:\n",
      "                   \n",
      "         \n",
      "         ndf[c+'_min']=df.groupby(['series_id'])[c].min()\n",
      "         ndf[c+'_max']=df.groupby(['series_id'])[c].max()\n",
      "         ndf[c+'_mean']=df.groupby(['series_id'])[c].mean()\n",
      "         ndf[c+'_std']=df.groupby(['series_id'])[c].std()\n",
      "\u001b[31m-    return ndf\u001b[39m\n",
      "\u001b[32m+    return ndf.reset_index()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/instant-gratification/luffyluffyluffy/14939296.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " %%time\n",
      "\u001b[32m+\u001b[39m\n",
      " oof = np.zeros(len(train))\n",
      " preds = np.zeros(len(test))\n",
      " from tqdm import tqdm_notebook\n",
      " \n",
      " for i in tqdm_notebook(range(512)):\n",
      " \n",
      "     train2 = train[train['wheezy-copper-turtle-magic']==i]\n",
      "     test2 = test[test['wheezy-copper-turtle-magic']==i]\n",
      "     \n",
      "     idx1 = train2.index; idx2 = test2.index\n",
      "     train2.reset_index(drop=True,inplace=True)\n",
      " \n",
      "     data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n",
      "     data2 = VarianceThreshold(threshold=2).fit_transform(data[cols])\n",
      " \n",
      "     train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n",
      "                            \n",
      " \n",
      "\u001b[31m-    skf = StratifiedKFold(n_splits=11, random_state=42)\u001b[39m\n",
      "\u001b[32m+    skf = StratifiedKFold(n_splits=2, random_state=42)\u001b[39m\n",
      "     for train_index, test_index in skf.split(train2, train2['target']):\n",
      "         \n",
      "         polynomial_features= PolynomialFeatures(degree=4)\n",
      "         x_poly = polynomial_features.fit_transform(train3[train_index,:])\n",
      "         x_test = polynomial_features.fit_transform(train3[test_index,:])\n",
      "         te3 = polynomial_features.fit_transform(test3)\n",
      "         clf = LogisticRegression()\n",
      "         \n",
      "         clf.fit(x_poly,train2.loc[train_index]['target'])\n",
      "         oof[idx1[test_index]] = clf.predict_proba(x_test)[:,1]\n",
      "         preds[idx2] += clf.predict_proba(te3)[:,1] / skf.n_splits\n",
      " \n",
      " auc = roc_auc_score(train['target'], oof)\n",
      "\u001b[31m-print(f'AUC: {auc:.5}')\u001b[39m\n",
      "\u001b[32m+print(f'AUC: {auc:.5}')\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-kernels-only/moghazy/4556514.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " from sklearn.svm import SVC\n",
      "\u001b[31m-from sklearn.ensemble import RandomForestClassifier\u001b[39m\n",
      "\u001b[32m+from sklearn.ensemble import ExtraTreesClassifier\u001b[39m\n",
      " \n",
      " clf = ExtraTreesClassifier(n_estimators= 30 , max_depth=None, min_samples_split=2, random_state=0)\n",
      " clf.fit(x_train,y_train)\n",
      " \n",
      " print('Accuracy of SVC on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\n",
      " print('Accuracy of SVC on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/avito-demand-prediction/dicksonchin93/3402503.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[32m+import xgboost as xgb\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " params = {'eta': 0.3,\n",
      "           'tree_method': \"hist\",\n",
      "           'grow_policy': \"lossguide\",\n",
      "           'max_leaves': 1400,  \n",
      "           'max_depth': 0, \n",
      "           'subsample': 0.9, \n",
      "           'colsample_bytree': 0.7, \n",
      "           'colsample_bylevel':0.7,\n",
      "           'min_child_weight':0,\n",
      "           'alpha':4,\n",
      "           'objective': 'regression', \n",
      "           'eval_metric': 'rmse', \n",
      "           'random_state': 99, \n",
      "           'silent': True}\n",
      " \n",
      " tr_data = xgb.DMatrix(X_tr, y_tr)\n",
      " va_data = xgb.DMatrix(X_va, y_va)\n",
      " del X_tr\n",
      " del X_va\n",
      " del y_tr\n",
      " del y_va\n",
      " gc.collect()\n",
      " \n",
      " watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n",
      " \n",
      " model = xgb.train(params, tr_data, 1000, watchlist, maximize=False, early_stopping_rounds = 25, verbose_eval=5)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/movie-review-sentiment-analysis-kernels-only/laowingkin/5493105.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " def keras_dl(model, embed_size, batch_size, epochs):   \n",
      "     inp = Input(shape = (max_len,), name = 'lstm')\n",
      "                                                                   \n",
      "     x = Embedding(max_words,embed_size,weights = [embedding_matrix], trainable = False)(inp)\n",
      "                                                                                                \n",
      "\u001b[31m-    x1 = SpatialDropout1D(0.5)(x)\u001b[39m\n",
      "\u001b[32m+    x1 = SpatialDropout1D(0.6)(x)\u001b[39m\n",
      "     \n",
      "     x_lstm = CuDNNLSTM(128, return_sequences = True)(x1)\n",
      "     x_lstm_c1d = Conv1D(64,kernel_size=3,padding='valid',activation='relu')(x_lstm)\n",
      "     x_lstm_c1d_gp = GlobalMaxPooling1D()(x_lstm_c1d)\n",
      "                                           \n",
      "     \n",
      "                                                                            \n",
      "                                   \n",
      "                                       \n",
      "                                     \n",
      "     \n",
      "     x_gru = CuDNNGRU(128, return_sequences = True)(x1)\n",
      "     x_gru_c1d = Conv1D(64,kernel_size=2,padding='valid',activation='relu')(x_gru)\n",
      "     x_gru_c1d_gp = GlobalMaxPooling1D()(x_gru_c1d)\n",
      "                                         \n",
      "     \n",
      "     inp2 = Input(shape = (td,), name = 'tfidf')\n",
      "     x2 = BatchNormalization()(inp2)\n",
      "     x2 = Dense(16, activation='relu')(x2)\n",
      "     \n",
      "     inp3 = Input(shape = (5,), name = 'score')\n",
      "     x3 = BatchNormalization()(inp3)\n",
      "     x3 = Dense(3, activation='tanh')(x3)\n",
      "     \n",
      "     x_f = concatenate([x_lstm_c1d_gp, x_gru_c1d_gp])                          \n",
      "     x_f = BatchNormalization()(x_f)\n",
      "\u001b[31m-    x_f = Dropout(0.5)(Dense(128, activation='tanh') (x_f))    \u001b[39m\n",
      "\u001b[32m+    x_f = Dropout(0.6)(Dense(128, activation='tanh') (x_f))    \u001b[39m\n",
      "     x_f = BatchNormalization()(x_f)\n",
      "     x_f = concatenate([x_f, x2, x3])\n",
      "\u001b[31m-    x_f = Dropout(0.5)(Dense(32, activation='tanh') (x_f))\u001b[39m\n",
      "\u001b[32m+    x_f = Dropout(0.6)(Dense(32, activation='tanh') (x_f))\u001b[39m\n",
      "     x_f = BatchNormalization()(x_f)\n",
      "                                                            \n",
      "                                                 \n",
      "     x_f = Dense(5, activation = \"softmax\")(x_f)\n",
      "     model = Model(inputs = [inp, inp2, inp3], outputs = x_f)\n",
      "     \n",
      "                                                                                     \n",
      "     model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
      "                                                                           \n",
      "     print(model.summary())\n",
      "     return (model)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/otto-group-product-classification-challenge/wakamezake/6884342.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "                \n",
      "\u001b[31m-columns = data.columns[1:-1]\u001b[39m\n",
      "\u001b[31m-                  \u001b[39m\n",
      "\u001b[31m-X = data[columns]\u001b[39m\n",
      "\u001b[32m+columns = train.columns[1:-1]\u001b[39m\n",
      "\u001b[32m+                         \u001b[39m\n",
      "\u001b[32m+X = train[columns]\u001b[39m\n",
      " print(columns)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/chess/jamesmcguigan/35108720.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-! for n in `seq 1 12`; do\\\u001b[39m\n",
      "\u001b[32m+! for n in `seq 1 14`; do\\\u001b[39m\n",
      "     (echo $n queens; time -p swipl -q -f nqueens.prolog -t \"all_nqueens($n)\" | grep 'solutions' | head -n 1) 2>&1\\\n",
      "     | grep 'queens\\|solutions\\|real' | uniq | tr '\\n' ' ' | gawk '{ printf(\"%3d queens = %8d solutions in %8.2fs\\n\", $1, $5, $4)  }';\\\n",
      " done;\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/santander-value-prediction-challenge/kerneler/6645224.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  plotPerColumnDistribution(df3, 10, 5)\u001b[39m\n",
      "\u001b[32m+  plotPerColumnDistribution(df2, 10, 5)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sberbank-russian-housing-market/plarmuseau/1152819.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,30 +1,46 @@\n",
      " \n",
      " y_train = train[\"price_doc\"]\n",
      " x_test = test.drop([\"id\"], axis=1)\n",
      " #x_test = test[[\"id\",\"timestamp\",\"full_sq\",\"life_sq\",\"floor\",\"build_year\",\"max_floor\",\"kitch_sq\",\"num_room\",\"state\"]]\n",
      " x_train = train.drop([\"id\" ,\"price_doc\"], axis=1)\n",
      " #x_train = train[[\"id\",\"timestamp\",\"full_sq\",\"life_sq\",\"floor\",\"build_year\",\"max_floor\",\"kitch_sq\",\"num_room\",\"state\"]]\n",
      "\u001b[32m+\u001b[39m\n",
      " #x_train=x_train.merge(macro[['timestamp','cpi','ppi','usdrub','eurrub','brent']], left_on='timestamp', right_on='timestamp', how='left')\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+#____________  append macro data\u001b[39m\n",
      " x_train=x_train.merge(macro, left_on='timestamp', right_on='timestamp', how='left')\n",
      " #x_test=x_test.merge(macro[['timestamp','cpi','ppi','usdrub','eurrub','brent']], left_on='timestamp', right_on='timestamp', how='left')\n",
      " x_test=x_test.merge(macro, left_on='timestamp', right_on='timestamp', how='left')\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+#_________________  drop empty columns\u001b[39m\n",
      " x_train=x_train.dropna(axis=1, how='all')\n",
      " x_test=x_test.dropna(axis=1, how='all')\n",
      " #can't merge train with test because the kernel run for very long time\n",
      " #child_on_acc_pre_school,modern_education_share,old_education_build_share \n",
      " \n",
      "\u001b[32m+# find mutual columns\u001b[39m\n",
      "\u001b[32m+traincol=list(x_train.columns.values)\u001b[39m\n",
      "\u001b[32m+testcol=list(x_test.columns.values)\u001b[39m\n",
      "\u001b[32m+mutucol=list(set(traincol).intersection(testcol))\u001b[39m\n",
      "\u001b[32m+# reshape dataframes\u001b[39m\n",
      "\u001b[32m+x_train=x_train[mutucol]\u001b[39m\n",
      "\u001b[32m+x_test=x_test[mutucol]\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " print(x_train.head())\n",
      " print(x_test.info(10))\n",
      " for c in x_train.columns:\n",
      "     if x_train[c].dtype == 'object':\n",
      "         lbl = preprocessing.LabelEncoder()\n",
      "         lbl.fit(list(x_train[c].values)) \n",
      "         x_train[c] = lbl.transform(list(x_train[c].values))\n",
      "         #x_train.drop(c,axis=1,inplace=True)\n",
      "         \n",
      " for c in x_test.columns:\n",
      "     if x_test[c].dtype == 'object':\n",
      "         lbl = preprocessing.LabelEncoder()\n",
      "         lbl.fit(list(x_test[c].values)) \n",
      "         x_test[c] = lbl.transform(list(x_test[c].values))\n",
      "         #x_test.drop(c,axis=1,inplace=True)        \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/freesound-audio-tagging-2019/sabamotto/15153255.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " MPLoader.reset()\n",
      "\u001b[31m-MPLoader.full_load(df_submission)\u001b[39m\n",
      "\u001b[32m+MPLoader.full_load(df_test)\u001b[39m\n",
      " USE_MASK_FREQ = USE_MASK_TIME = False\n",
      " \n",
      "\u001b[31m-test = ImageList.from_df(df_submission, WORK, folder='')\u001b[39m\n",
      "\u001b[31m-learn = load_learner(WORK, test=test)\u001b[39m\n",
      "\u001b[32m+test = ImageList.from_df(df_test, WORK, folder='')\u001b[39m\n",
      "\u001b[32m+learn = load_learner(WORK, test=test) if TRAIN_MODE else load_learner('.', LAST_WEIGHTS, test=test)\u001b[39m\n",
      " preds, _ = learn.TTA(ds_type=DatasetType.Test)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/santander-product-recommendation/katerynad/427676.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " DT$seniority_mm <- round(as.numeric(difftime(DT$fecha_dato, DT$fecha_alta,units='secs')/(60 * 60 * 24 * 30)))\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[32m+DT$seniority_mm[is.na(DT$seniority_mm)] <- 1\u001b[39m\n",
      " DT$seniority <- ifelse(DT$seniority_mm  <= 6, 1, ifelse(DT$seniority_mm <= 12, 2, 3))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/imet-2019-fgvc6/dimitreoliveira/12845910.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-                  \u001b[39m\n",
      "\u001b[31m-BATCH_SIZE = 128\u001b[39m\n",
      "\u001b[31m-EPOCHS = 30\u001b[39m\n",
      "\u001b[32m+            \u001b[39m\n",
      "\u001b[32m+BATCH_SIZE = 64\u001b[39m\n",
      "\u001b[32m+EPOCHS = 200\u001b[39m\n",
      " LEARNING_RATE = 0.0001\n",
      "\u001b[31m-HEIGHT = 64\u001b[39m\n",
      "\u001b[31m-WIDTH = 64\u001b[39m\n",
      "\u001b[32m+HEIGHT = 128\u001b[39m\n",
      "\u001b[32m+WIDTH = 128\u001b[39m\n",
      " CANAL = 3\n",
      "\u001b[31m-N_CLASSES = N_unique_att\u001b[39m\n",
      "\u001b[32m+N_CLASSES = labels.shape[0]\u001b[39m\n",
      "\u001b[32m+ES_PATIENCE = 5\u001b[39m\n",
      "\u001b[32m+DECAY_DROP = 0.5\u001b[39m\n",
      "\u001b[32m+DECAY_EPOCHS = 10\u001b[39m\n",
      " classes = list(map(str, range(N_CLASSES)))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-kernels-only/manufacturingai/13667504.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  df_raw.info()\u001b[39m\n",
      "\u001b[32m+  data.info()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/moghazy/4662645.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,10 +1,10 @@\n",
      " from sklearn.svm import SVC\n",
      " from sklearn.ensemble import ExtraTreesClassifier\n",
      " from sklearn.neural_network import MLPClassifier\n",
      " \n",
      "\u001b[31m-clf = ExtraTreesClassifier(n_estimators=100,\u001b[39m\n",
      "\u001b[32m+clf = ExtraTreesClassifier(n_estimators=400,\u001b[39m\n",
      "                               random_state=0)\n",
      " clf.fit(x_train,y_train)\n",
      " \n",
      " print('Accuracy of SVC on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\n",
      " print('Accuracy of SVC on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))\n",
      "\n",
      "\n",
      " test.head()\n",
      " id = test['Id']\n",
      "\u001b[31m-test.drop(['Id'] , inplace = True , axis = 1)\u001b[39m\n",
      "\u001b[32m+# test.drop(['Id'] , inplace = True , axis = 1)\u001b[39m\n",
      " test.drop(drop , inplace = True, axis = 1)\n",
      "\u001b[32m+# test.drop(drop2 , inplace = True, axis = 1)\u001b[39m\n",
      " \n",
      " # test = scaler.fit_transform(test)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/samusram/20883922.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,20 +1,19 @@\n",
      " import os\n",
      " import numpy as np\n",
      " import pydicom\n",
      " import pandas as pd\n",
      " from random import sample\n",
      " from tqdm import tqdm_notebook as tqdm\n",
      " import hdbscan\n",
      "\u001b[31m-                                    \u001b[39m\n",
      " from scipy.spatial.distance import jensenshannon\n",
      " from sklearn.svm import LinearSVC\n",
      " from sklearn.model_selection import train_test_split\n",
      " from sklearn.preprocessing import StandardScaler\n",
      " from sklearn.metrics import f1_score\n",
      " import pickle\n",
      " import random\n",
      " random.seed(1)\n",
      " from numpy.random import seed\n",
      " seed(1)\n",
      " import matplotlib.pyplot as plt\n",
      " %matplotlib inline\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def red(text):\n",
    "    return (Fore.RED + text + Fore.RESET)\n",
    "\n",
    "def green(text):\n",
    "    return (Fore.GREEN + text + Fore.RESET)\n",
    "    \n",
    "def display_kaggle_diff(diff,return_string=False):\n",
    "    diff_disp = get_diff_visual(diff)\n",
    "    if return_string:\n",
    "        return diff_disp\n",
    "    else:\n",
    "        print(diff_disp)\n",
    "        \n",
    "def get_diff_visual(diff):\n",
    "    \n",
    "    string_lines = []\n",
    "    \n",
    "    string_lines.append(Fore.BLUE + diff[\"original_path\"])\n",
    "    string_lines.append(\"-\"*100 + Fore.RESET)\n",
    "    \n",
    "    for line in diff[\"cell_diff\"].split(\"\\n\"):\n",
    "        if len(line) == 0:\n",
    "            string_lines.append(\"\")\n",
    "            continue\n",
    "        if line[0] == \"+\":\n",
    "            string_lines.append(green(line))\n",
    "        elif line[0] == \"-\":\n",
    "            string_lines.append(red(line))\n",
    "        else:\n",
    "            string_lines.append(line)\n",
    "    return \"\\n\".join(string_lines)\n",
    "\n",
    "\n",
    "for diff in diff_examples[:40]:\n",
    "    display_kaggle_diff(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding some new features...\n",
    "The following was done with:\n",
    "`python src/data/tree_diffs.py  data/processed/competitions/ . --ignore_comments --python_only --git_context 3 --ignore_empty_lines --ignore_line_shuffle --n_workers 1 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224532 /homes/gws/mikeam/RobustDataScience/diffs_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "MORE_FEATURES_PATH = \"/homes/gws/mikeam/RobustDataScience/diffs_test.jsonl\"\n",
    "!wc -l $MORE_FEATURES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many more examples we got, I think by decreasing the context between lines such that we have smaller hunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_features_examples = !shuf -n 100000 $MORE_FEATURES_PATH\n",
    "more_features_examples = [json.loads(x) for x in more_features_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/zillow-prize-1/jaccojurg/30688082.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " plt.figure(figsize=(15,10))\n",
      " \n",
      "\u001b[31m-Countries = ['Italy', 'Netherlands','Sweden','Germany', 'China','US','Belgium', 'Japan','France']\u001b[39m\n",
      "\u001b[32m+Countries = ['Italy', 'Netherlands - Netherlands']\u001b[39m\n",
      " #Countries = ['Italy', 'Netherlands', 'France'] #, 'Belgium'] #, 'Belgium','France']\n",
      " for country in Countries:\n",
      " \n",
      "\u001b[31m-#for r in range(len(info)):\u001b[39m\n",
      "\u001b[31m-#    plt.annotate(info.loc[r]['event'],(info.loc[r]['dayssince'],info.loc[r]['CasesPerM']))\u001b[39m\n",
      "\u001b[32m+for r in range(len(info)):\u001b[39m\n",
      "\u001b[32m+    plt.annotate(info.loc[r]['event'],(info.loc[r]['dayssince'],info.loc[r]['CasesPerM']),\u001b[39m\n",
      "\u001b[32m+                 xytext = (-10+info.loc[r]['dayssince'],info.loc[r]['CasesPerM']-1),\u001b[39m\n",
      "\u001b[32m+                 #color=c[info.loc[r]['location']],\u001b[39m\n",
      "\u001b[32m+                 arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle3\"),\u001b[39m\n",
      "\u001b[32m+                 horizontalalignment='left', verticalalignment='center')\u001b[39m\n",
      " \n",
      " plt.ylabel('Cases per million inhabitants')\n",
      " plt.xlim(0,40)\n",
      "\u001b[31m-plt.legend()\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[32m+plt.legend()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/instant-gratification/autuanliuyc/14739130.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " import lightgbm as lgb\n",
      " from sklearn.model_selection import train_test_split\n",
      "\u001b[32m+from sklearn.decomposition import PCA\u001b[39m\n",
      "\u001b[32m+from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\u001b[39m\n",
      " # Any results you write to the current directory are saved as output.\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/titanic/conorm97/1035089.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " prediction = rfprediction + gbprediction + knnprediction\n",
      " prediction[prediction == 1] = 0\n",
      "\u001b[31m-prediction[prediction == 2] = 0\u001b[39m\n",
      "\u001b[32m+prediction[prediction == 2] = 1\u001b[39m\n",
      " prediction[prediction == 3] = 1\n",
      "\n",
      " PassengerId = np.array(test['PassengerId']).astype(int)\n",
      " solution = pd.DataFrame(prediction, PassengerId, columns=['Survived'])\n",
      "\u001b[31m-solution = solution.to_csv('newsolution2', index_label = ['PassengerId'])\u001b[39m\n",
      "\u001b[32m+solution = solution.to_csv('newsolution', index_label = ['PassengerId'])\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/janiobachmann/2220531.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " from plotly import tools\n",
      " import plotly\n",
      "\u001b[31m-plotly.tools.set_credentials_file(username='AlexanderBach', api_key='LOJCJIZyffcW73avGsJ1')\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/iwildcam-2019-fgvc6/s3chwartz/15023249.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy'\n",
      "                               patience=5, min_lr=1e-5)\n",
      "\u001b[31m-model.fit_generator(generator=train_generator,\u001b[39m\n",
      "\u001b[32m+history = model.fit_generator(generator=train_generator,\u001b[39m\n",
      "                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/acbattle/9733717.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " days = [ 'Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
      "\u001b[32m+days_ = [i for i in range(0,7)]\u001b[39m\n",
      "\u001b[32m+# print(np.unique(list(data_['week'].values)))\u001b[39m\n",
      " percentile_day1 = []\n",
      " percentile_day2 = []\n",
      " percentile_day0 = []\n",
      "\u001b[31m-percentile_day = dict().fromkeys([i for i in days])\u001b[39m\n",
      "\u001b[32m+percentile_day = dict().fromkeys([i for i in days_])\u001b[39m\n",
      " pds = []\n",
      "\u001b[31m-for i in days:    \u001b[39m\n",
      "\u001b[31m-    data = data_[data_['week']==i].price\u001b[39m\n",
      "\u001b[32m+for i in days_:\u001b[39m\n",
      "\u001b[32m+    data = data_[data_['week']==str(i)].price\u001b[39m\n",
      "\u001b[32m+#     print(data)\u001b[39m\n",
      "     # 统计输出信息\n",
      " for i in days:\n",
      "         if(j == True):\n",
      "\u001b[31m-            num+=1\u001b[39m\n",
      "\u001b[32m+            num += 1\u001b[39m\n",
      "     pds.append([i,num,percentile_result[0],percentile_result[1],percentile_result[2],num/len(list(data))])\n",
      "\n",
      "\n",
      " d = []\n",
      "\u001b[31m-week = [ 'Mon','Tue','Wed','Thu','Fri','Sat','Sun']\u001b[39m\n",
      "\u001b[31m-days = [i for i in range(1,8)]\u001b[39m\n",
      "\u001b[31m-d.append(days)\u001b[39m\n",
      "\u001b[32m+d.append(days_)\u001b[39m\n",
      " d.append(percentile_day)\n",
      " color_ = ['#587123','#581223','#587199','#327123','#007123','#127453','#918652']\n",
      " mak = ['*','^','v','o','s','<','>']\n",
      "\u001b[32m+# print(percentile_day)\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+plt.xticks(days_,days)#将数字--->字符显示\u001b[39m\n",
      " for i in range(7):\n",
      "\u001b[31m-    day = [i+1,i+1,i+1]\u001b[39m\n",
      "\u001b[31m-    plt.plot(day, percentile_day[week[i]], color=color_[i], marker=mak[i])\u001b[39m\n",
      "\u001b[31m-plt.plot(days, percentile_day0, color='green', label='25%')\u001b[39m\n",
      "\u001b[31m-plt.plot(days, percentile_day1, color='#873018', label='50%')\u001b[39m\n",
      "\u001b[31m-plt.plot(days, percentile_day2,  color='skyblue', label='75%')\u001b[39m\n",
      "\u001b[32m+    day = [i,i,i]\u001b[39m\n",
      "\u001b[32m+    plt.plot(day, percentile_day[i], color=color_[i], marker=mak[i])\u001b[39m\n",
      "\u001b[32m+plt.plot(days_, percentile_day0, color='green', label='25%')\u001b[39m\n",
      "\u001b[32m+plt.plot(days_, percentile_day1, color='#873018', label='50%')\u001b[39m\n",
      "\u001b[32m+plt.plot(days_, percentile_day2,  color='skyblue', label='75%')\u001b[39m\n",
      " plt.legend() # 显示图例\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/souravkgoyal/36044915.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " plt.figure(figsize = (13,5))\n",
      " plt.subplot(121)\n",
      "\u001b[32m+plt.title('Distribution of sentiments in Train Data')\u001b[39m\n",
      " sns.countplot(train['sentiment'])\n",
      " plt.subplot(122)\n",
      "\u001b[31m-sns.countplot(test['sentiment'])\u001b[39m\n",
      "\u001b[32m+sns.countplot(test['sentiment'])\u001b[39m\n",
      "\u001b[32m+plt.title('Distribution of sentiments in Test Data')\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-question-pairs/trydowuk/1247448.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1,8 +1 @@\n",
      "\u001b[31m-# токенизация\u001b[39m\n",
      "\u001b[31m-import nltk\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-trainDfTokenized = train_df.copy()\u001b[39m\n",
      "\u001b[31m-trainDfTokenized['question1'] = [nltk.word_tokenize(i) for i in trainDfTokenized['question1'].get_values()]\u001b[39m\n",
      "\u001b[31m-trainDfTokenized['question2'] = [nltk.word_tokenize(i) for i in trainDfTokenized['question2'].get_values()]\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " # стемминг и удаление стоп-слов\n",
      " def stemmingForListOfWords(lst):\n",
      " trainDfTokenized['question1'] = [stemmingForListOfWords(i) for i in trainDfTokenized['question1'].get_values()]\n",
      "\u001b[31m-trainDfTokenized['question2'] = [stemmingForListOfWords(i) for i in trainDfTokenized['question2'].get_values()]\u001b[39m\n",
      "\u001b[32m+trainDfTokenized['question2'] = [stemmingForListOfWords(i) for i in trainDfTokenized['question2'].get_values()]\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/plant-seedlings-classification/mmiikeke/28563521.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " import random\n",
      " import pandas as pd\n",
      "\u001b[32m+import numpy as np\u001b[39m\n",
      " from sklearn.model_selection import train_test_split\n",
      "\n",
      " def train():\n",
      "         print(f'Train loss: {training_loss:.4f}\\taccuracy: {training_acc:.4f}\\n')\n",
      "\u001b[31m-        \u001b[39m\n",
      "\u001b[32m+        \"\"\"\u001b[39m\n",
      "         for i, (inputs, labels) in enumerate(validation_loader):\n",
      " def train():\n",
      "         validation_loss_list.append(validation_loss)\n",
      "\u001b[31m-        \u001b[39m\n",
      "\u001b[32m+        \"\"\"\u001b[39m\n",
      "         if training_acc > best_acc:\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/anubhav1302/13767216.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " model.add(Bidirectional(CuDNNLSTM(64,kernel_initializer='he_normal',return_seque\n",
      " model.add(Bidirectional(CuDNNLSTM(128,kernel_initializer='he_normal')))\n",
      "\u001b[31m-model.add(Dropout(0.2))\u001b[39m\n",
      "\u001b[32m+model.add(Dropout(0.4))\u001b[39m\n",
      " model.add(Dense(1024,activation=None))\n",
      "\n",
      "\n",
      " #predictions\n",
      "\u001b[31m-test_predictions=model.predict(test_data_prepd)\u001b[39m\n",
      "\u001b[32m+test_predictions=model.predict(test_data_prepd)\u001b[39m\n",
      "\u001b[32m+preds=test_predictions.reshape[-1]\u001b[39m\n",
      "\n",
      "\n",
      " #Submission file\n",
      "\u001b[31m-sub_file=pd.DataFrame({'id':test['id'],'prediction':test_predictions.reshape[-1]})\u001b[39m\n",
      "\u001b[32m+sub_file=pd.DataFrame({'id':test['id'],'prediction':preds})\u001b[39m\n",
      " sub_file.to_csv('submission.csv',index=False)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/keremt/25633432.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # label distribution for training fold to be used in metric\n",
      "\u001b[31m-train_labels_dist = (train_labels_df[train_labels_df.installation_id.isin(trn_ids)]['accuracy_group']\u001b[39m\n",
      "\u001b[31m-    .value_counts(normalize=True))\u001b[39m\n",
      "\u001b[31m-train_labels_dist_quantiles = np.cumsum([train_labels_dist[i] for i in range(3)])\u001b[39m\n",
      "\u001b[31m-train_labels_dist_quantiles\u001b[39m\n",
      "\u001b[32m+train_labels_dist = (train_with_features_part3['accuracy_group'].value_counts(normalize=True))\u001b[39m\n",
      "\u001b[32m+q = np.cumsum([train_labels_dist[i] for i in range(3)]); q\u001b[39m\n",
      "\n",
      " def convert_targs(targs):\n",
      "     return hard_targs\n",
      "\u001b[31m-assert not any(convert_targs(tensor(train_labels_df['accuracy'])) != tensor(train_labels_df['accuracy_group']))\u001b[39m\n",
      "\u001b[32m+# assert not any(convert_targs(tensor(train_labels_df['accuracy'])) != tensor(train_labels_df['accuracy_group']))\u001b[39m\n",
      " \n",
      " class KappaScoreRegression(RegMetrics):\n",
      "         \"convert preds and calc qwk\"\n",
      "\u001b[31m-        preds = convert_preds(self.preds, q=train_labels_dist_quantiles)\u001b[39m\n",
      "\u001b[32m+        preds = convert_preds(self.preds, q=q)\u001b[39m\n",
      "         targs = convert_targs(self.targs)\n",
      "\n",
      "\n",
      " _preds, _targs = learner.get_preds()\n",
      "\u001b[31m-_preds, _targs = convert_preds(_preds, q=train_labels_dist_quantiles), convert_targs(_targs)\u001b[39m\n",
      "\u001b[32m+_preds, _targs = convert_preds(_preds, q=q), convert_targs(_targs)\u001b[39m\n",
      " cohen_kappa_score(_targs, _preds, weights=\"quadratic\")\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/uysimty/12894383.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " if FAST_RUN:\n",
      "     half_train = int(train_df.shape[0] / 4)\n",
      "\u001b[31m-    train_df = train_df.sample(n=half_train).reset_index().drop(columns=['index'])\u001b[39m\n",
      "\u001b[31m-    epochs=1\u001b[39m\n",
      "\u001b[32m+    train_df = train_df.sample(n=half_train).reset_index().drop(columns=['index'])\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[31m-y=train_df[\"target\"]\u001b[39m\n",
      "\u001b[32m+y=(train_df['target'].values > 0.5).astype(int)\u001b[39m\n",
      " x=train_df[\"comment_text\"]\n",
      "\n",
      "\n",
      " tranformer = Tokenizer(num_words=max_words)\n",
      "\u001b[31m-all_texts = list(train_df[\"comment_text\"].values) + list(test_df[\"comment_text\"].values)\u001b[39m\n",
      "\u001b[31m-tranformer.fit_on_texts(all_texts)\u001b[39m\n",
      "\u001b[32m+# all_texts = list(train_df[\"comment_text\"].values) + list(test_df[\"comment_text\"].values)\u001b[39m\n",
      "\u001b[32m+tranformer.fit_on_texts(train_df[\"comment_text\"])\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/santander-value-prediction-challenge/rishgupta34/4474929.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-def PCA(data,opt=1,val=None):\u001b[39m\n",
      "\u001b[31m-    '''opt==1 if you want to put value of m\u001b[39m\n",
      "\u001b[31m-        else opt==2 if you want to put value of explained variances'''\u001b[39m\n",
      "\u001b[31m-    covariance=np.cov(data,rowvar=False)\u001b[39m\n",
      "\u001b[31m-    eig_val,eig_vec=np.linalg.eig(covariance)\u001b[39m\n",
      "\u001b[31m-    info=[(eig_val[i],eig_vec[:,i]) for i in range(eig_val.shape[0])]\u001b[39m\n",
      "\u001b[31m-    info=sorted(info,key=lambda x:x[0],reverse=True)\u001b[39m\n",
      "\u001b[31m-    sorted_eig_val=[info[i][0] for i in range(len(info))]\u001b[39m\n",
      "\u001b[31m-    sorted_eig_vec=np.array([info[i][1] for i in range(len(info))])\u001b[39m\n",
      "\u001b[31m-    DATA=np.array(data.dot(sorted_eig_vec),dtype=np.float32)\u001b[39m\n",
      "\u001b[31m-#     DATA_test=data_test.dot(sorted_eig_vec)\u001b[39m\n",
      "\u001b[31m-    if(opt==1):\u001b[39m\n",
      "\u001b[31m-        if(val==None):\u001b[39m\n",
      "\u001b[31m-            val=data.shape[1]\u001b[39m\n",
      "\u001b[31m-    elif(opt==2):\u001b[39m\n",
      "\u001b[31m-        if val==None:\u001b[39m\n",
      "\u001b[31m-            print(\"Error put some value of ev\")\u001b[39m\n",
      "\u001b[31m-            return\u001b[39m\n",
      "\u001b[31m-        ev=0\u001b[39m\n",
      "\u001b[31m-        count=0\u001b[39m\n",
      "\u001b[31m-        variance_ratios=sorted_eig_val/np.sum(sorted_eig_val)\u001b[39m\n",
      "\u001b[31m-#         print(variance_ratios.shape)\u001b[39m\n",
      "\u001b[31m-        while (ev<=val) & (count<variance_ratios.shape[0]):\u001b[39m\n",
      "\u001b[31m-            ev+=variance_ratios[count]\u001b[39m\n",
      "\u001b[31m-            count+=1\u001b[39m\n",
      "\u001b[31m-            if count%500==0:\u001b[39m\n",
      "\u001b[31m-                print(count)\u001b[39m\n",
      "\u001b[31m-        val=count\u001b[39m\n",
      "\u001b[31m-#         print(\"Variance Ratios: \",variance_ratios)\u001b[39m\n",
      "\u001b[31m-    reduced_data=DATA[:,:val]\u001b[39m\n",
      "\u001b[31m-#     reduced_data_test=DATA_test[:,:val]\u001b[39m\n",
      "\u001b[31m-    return reduced_data,sorted_eig_vec,val\u001b[39m\n",
      "\u001b[32m+# def PCA(data,opt=1,val=None):\u001b[39m\n",
      "\u001b[32m+#     '''opt==1 if you want to put value of m\u001b[39m\n",
      "\u001b[32m+#         else opt==2 if you want to put value of explained variances'''\u001b[39m\n",
      "\u001b[32m+#     covariance=np.cov(data,rowvar=False)\u001b[39m\n",
      "\u001b[32m+#     eig_val,eig_vec=np.linalg.eig(covariance)\u001b[39m\n",
      "\u001b[32m+#     info=[(eig_val[i],eig_vec[:,i]) for i in range(eig_val.shape[0])]\u001b[39m\n",
      "\u001b[32m+#     info=sorted(info,key=lambda x:x[0],reverse=True)\u001b[39m\n",
      "\u001b[32m+#     sorted_eig_val=[info[i][0] for i in range(len(info))]\u001b[39m\n",
      "\u001b[32m+#     sorted_eig_vec=np.array([info[i][1] for i in range(len(info))])\u001b[39m\n",
      "\u001b[32m+#     DATA=np.array(data.dot(sorted_eig_vec),dtype=np.float32)\u001b[39m\n",
      "\u001b[32m+# #     DATA_test=data_test.dot(sorted_eig_vec)\u001b[39m\n",
      "\u001b[32m+#     if(opt==1):\u001b[39m\n",
      "\u001b[32m+#         if(val==None):\u001b[39m\n",
      "\u001b[32m+#             val=data.shape[1]\u001b[39m\n",
      "\u001b[32m+#     elif(opt==2):\u001b[39m\n",
      "\u001b[32m+#         if val==None:\u001b[39m\n",
      "\u001b[32m+#             print(\"Error put some value of ev\")\u001b[39m\n",
      "\u001b[32m+#             return\u001b[39m\n",
      "\u001b[32m+#         ev=0\u001b[39m\n",
      "\u001b[32m+#         count=0\u001b[39m\n",
      "\u001b[32m+#         variance_ratios=sorted_eig_val/np.sum(sorted_eig_val)\u001b[39m\n",
      "\u001b[32m+# #         print(variance_ratios.shape)\u001b[39m\n",
      "\u001b[32m+#         while (ev<=val) & (count<variance_ratios.shape[0]):\u001b[39m\n",
      "\u001b[32m+#             ev+=variance_ratios[count]\u001b[39m\n",
      "\u001b[32m+#             count+=1\u001b[39m\n",
      "\u001b[32m+#             if count%500==0:\u001b[39m\n",
      "\u001b[32m+#                 print(count)\u001b[39m\n",
      "\u001b[32m+#         val=count\u001b[39m\n",
      "\u001b[32m+# #         print(\"Variance Ratios: \",variance_ratios)\u001b[39m\n",
      "\u001b[32m+#     reduced_data=DATA[:,:val]\u001b[39m\n",
      "\u001b[32m+# #     reduced_data_test=DATA_test[:,:val]\u001b[39m\n",
      "\u001b[32m+#     return reduced_data,sorted_eig_vec,val\u001b[39m\n",
      "\n",
      "\u001b[31m-  reduced_data,sorted_eig_vec,val=PCA(X_train,2,0.95)\u001b[39m\n",
      "\u001b[32m+  # reduced_data,sorted_eig_vec,val=PCA(X_sca_train,2,0.95)\u001b[39m\n",
      "\n",
      "\u001b[31m-  reduced_data.shape\u001b[39m\n",
      "\u001b[32m+  # reduced_data.shape\u001b[39m\n",
      "\n",
      "\u001b[31m-  X_train.shape\u001b[39m\n",
      "\u001b[32m+  # X_sca_train.shape\u001b[39m\n",
      "\n",
      "\u001b[31m-  sorted_eig_vec.shape\u001b[39m\n",
      "\u001b[32m+  # sorted_eig_vec.shape\u001b[39m\n",
      "\n",
      "\u001b[31m-  test=np.array(test.dot(sorted_eig_vec[:,:val]),dtype=np.float32)\u001b[39m\n",
      "\u001b[32m+  # new_test=np.array(sca_test.dot(sorted_eig_vec[:,:val]),dtype=np.float32)\u001b[39m\n",
      "\n",
      "\u001b[31m-  DATA_test.shape\u001b[39m\n",
      "\u001b[32m+  # new_test.shape\u001b[39m\n",
      "\n",
      "\u001b[31m-  lr=LinearRegression(normalize=True)\u001b[39m\n",
      "\u001b[32m+  lr=LinearRegression()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/recruit-restaurant-visitor-forecasting/johannesss/1883216.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " from sklearn.preprocessing import MinMaxScaler\n",
      " from sklearn.metrics import mean_squared_error\n",
      "\u001b[32m+import seaborn as sns\u001b[39m\n",
      "\u001b[32m+import matplotlib.pyplot as plt\u001b[39m\n",
      " \n",
      " from keras.layers import Dense, LSTM\n",
      " from subprocess import check_output\n",
      "\u001b[31m-print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[32m+print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/instant-gratification/mks2192/15289750.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for i in range(8):\n",
      "     plt.subplot(3,3,i+1)\n",
      "\u001b[31m-    sns.distplot(train.iloc[:,i],bins=200)\u001b[39m\n",
      "\u001b[31m-    plt.title(train.columns[i])\u001b[39m\n",
      "\u001b[32m+    sns.distplot(train.iloc[:,i+1],bins=200)\u001b[39m\n",
      "\u001b[32m+    plt.title(train.columns[i+1])\u001b[39m\n",
      "     plt.xlabel('')\n",
      " for i in range(8):\n",
      " plt.subplot(3,3,9)\n",
      "\u001b[31m-std = round(np.std(train.iloc[:,7]),2)\u001b[39m\n",
      "\u001b[32m+std = round(np.std(train.iloc[:,8]),2)\u001b[39m\n",
      " data = np.random.normal(0,std,train.shape[0])\n",
      " sns.distplot(data,bins=200)\n",
      "\u001b[31m-plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"(\"+train.columns[7]+\")\")\u001b[39m\n",
      "\u001b[32m+plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train.columns[8]+\")\")\u001b[39m\n",
      " plt.xlabel('')\n",
      "\n",
      " for i in range(8):\n",
      "     plt.subplot(3,3,i+1)\n",
      "\u001b[31m-    stats.probplot(train.iloc[:,i],plot=plt)\u001b[39m\n",
      "\u001b[31m-    plt.title(train.columns[i])\u001b[39m\n",
      "\u001b[32m+    stats.probplot(train.iloc[:,i+1],plot=plt)\u001b[39m\n",
      "\u001b[32m+    plt.title(train.columns[i+1])\u001b[39m\n",
      "     plt.xlabel('')\n",
      " for i in range(8):\n",
      " plt.subplot(3,3,9)\n",
      "\u001b[31m-std = round(np.std(train.iloc[:,7]),2)\u001b[39m\n",
      "\u001b[32m+std = round(np.std(train.iloc[:,8]),2)\u001b[39m\n",
      " data = np.random.normal(0,std,train.shape[0])\n",
      " stats.probplot(data,plot=plt)\n",
      "\u001b[31m-plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"(\"+train.columns[7]+\")\")\u001b[39m\n",
      "\u001b[32m+plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train.columns[8]+\")\")\u001b[39m\n",
      " plt.xlabel('')\n",
      "\n",
      " for i in range(8):\n",
      "     plt.subplot(3,3,i+1)\n",
      "\u001b[31m-    sns.distplot(train0.iloc[:,i],bins=10)\u001b[39m\n",
      "\u001b[31m-    plt.title(train0.columns[i])\u001b[39m\n",
      "\u001b[32m+    sns.distplot(train0.iloc[:,i+1],bins=10)\u001b[39m\n",
      "\u001b[32m+    plt.title(train0.columns[i+1])\u001b[39m\n",
      "     plt.xlabel('')\n",
      " for i in range(8):\n",
      " plt.subplot(3,3,9)\n",
      "\u001b[31m-std0 = round(np.std(train0.iloc[:,7]),2)\u001b[39m\n",
      "\u001b[32m+std0 = round(np.std(train0.iloc[:,8]),2)\u001b[39m\n",
      " data0 = np.random.normal(0,std0,train0.shape[0])\n",
      " sns.distplot(data0,bins=10)\n",
      "\u001b[31m-plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"(\"+train0.columns[7]+\")\")\u001b[39m\n",
      "\u001b[32m+plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train0.columns[8]+\")\")\u001b[39m\n",
      " plt.xlabel('')\n",
      "\n",
      " for i in range(8):\n",
      "     plt.subplot(3,3,i+1)\n",
      "\u001b[31m-    stats.probplot(train0.iloc[:,i],plot=plt)\u001b[39m\n",
      "\u001b[31m-    plt.title(train0.columns[i])\u001b[39m\n",
      "\u001b[32m+    stats.probplot(train0.iloc[:,i+1],plot=plt)\u001b[39m\n",
      "\u001b[32m+    plt.title(train0.columns[i+1])\u001b[39m\n",
      "     plt.xlabel('')\n",
      " plt.subplot(3,3,9)\n",
      " stats.probplot(data,plot=plt)\n",
      "\u001b[31m-plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"(\"+train0.columns[7]+\")\")\u001b[39m\n",
      "\u001b[32m+plt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train0.columns[8]+\")\")\u001b[39m\n",
      " plt.xlabel('')\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-kernels-only/moghazy/11767196.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " X_train, y_train, X_test, y_test, input_size, val_ids = load_data(train_df)\n",
      " \n",
      "\u001b[31m-def make_nn():\u001b[39m\n",
      "\u001b[32m+def make_nn(size='big'):\u001b[39m\n",
      "     inputs = Input(shape=(input_size,))\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-    x = Dense(256, activation='relu')(inputs)\u001b[39m\n",
      "\u001b[31m-    x = Dense(256, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(256, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(64, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(64, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(64, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(32, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(32, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(32, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(16, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(16, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(16, activation='relu')(x)\u001b[39m\n",
      "\u001b[31m-    x = Dense(7, activation='softmax')(x)\u001b[39m\n",
      "\u001b[32m+    if size=='big':\u001b[39m\n",
      "\u001b[32m+        x = Dense(256, activation='relu')(inputs)\u001b[39m\n",
      "\u001b[32m+        x = Dense(256, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(256, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(64, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(32, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(16, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(7, activation='softmax')(x)\u001b[39m\n",
      "\u001b[32m+        \u001b[39m\n",
      "\u001b[32m+    else:\u001b[39m\n",
      "\u001b[32m+        x = Dense(256, activation='relu')(inputs)\u001b[39m\n",
      "\u001b[32m+        x = Dense(256, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(128, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(64, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(32, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(16, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+        x = Dense(7, activation='softmax')(x)\u001b[39m\n",
      " \n",
      "     model = Model(inputs=inputs, outputs=x)\n",
      "\u001b[31m-\u001b[39m\n",
      "     return model\n",
      " def fit_nn(model, lr, bs, num_epochs):\n",
      " \n",
      "\u001b[31m-fit_nn(make_nn(), 1e-3, 32, 30)\u001b[39m\n",
      "\u001b[32m+fit_nn(make_nn('big'), 1e-3, 32, 30)\u001b[39m\n",
      " model = load_model('checkpoint.h5')\n",
      "\n",
      " print(y_test_classes.value_counts(normalize=True))\n",
      " \n",
      "\u001b[31m-model = fit_nn(make_nn(), 1e-3, 256, 10)\u001b[39m\n",
      "\u001b[32m+fit_nn(make_nn('small'), 1e-3, 256, 10)\u001b[39m\n",
      "\u001b[32m+model = load_model('checkpoint.h5')\u001b[39m\n",
      "\n",
      "\n",
      " y = model.predict(test_df)\n",
      "\u001b[31m-y = [np.argmax(x)+1 for x in y]\u001b[39m\n",
      "\u001b[32m+y = pd.Series([np.argmax(x)+1 for x in y])\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+sns.countplot(y)\u001b[39m\n",
      "\u001b[32m+print(y.value_counts(normalize=True))\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/R/adi996/29621451.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  patient['region'].unique()\u001b[39m\n",
      "\u001b[32m+  patient['country'].unique()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/aerial-cactus-identification/frlemarchand/13418669.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-epochs = 40\u001b[39m\n",
      "\u001b[32m+epochs = 50\u001b[39m\n",
      " model.fit_generator(train_generator,\n",
      " model.fit_generator(train_generator,\n",
      "           shuffle=True,\n",
      "\u001b[31m-          callbacks=[history])\u001b[39m\n",
      "\u001b[32m+          callbacks=callbacks)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sentiment-analysis-on-movie-reviews/hedluund/27115902.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " #!pip install -U scikit-learn\n",
      "\u001b[31m-!pip install smogn\u001b[39m\n",
      "\u001b[32m+#!pip install smogn\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/titanic/axeloh/11402955.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for dataset in datasets:\n",
      "     # Mapping Embarked\n",
      "\u001b[32m+    dataset['Embarked'] = dataset['Embarked'].fillna('S')\u001b[39m\n",
      "     dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
      " for dataset in datasets:\n",
      "     # Mapping Fare\n",
      "\u001b[32m+    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\u001b[39m\n",
      "     dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/gmhost/7772735.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n",
      "     model = capsule()\n",
      "\u001b[31m-    if i == 0;print(model.summary()) \u001b[39m\n",
      "\u001b[32m+    if i == 0:print(model.summary()) \u001b[39m\n",
      "     model.fit(X_train, Y_train, batch_size=512, epochs=6, validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks, \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/josealways123/35070757.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " predictions = []\n",
      " models = []\n",
      "\u001b[31m-for fold in range(skf.n_splits):\u001b[39m\n",
      "\u001b[31m-    model = BERTweetModel(conf=config)\u001b[39m\n",
      "\u001b[31m-    if torch.cuda.is_available():\u001b[39m\n",
      "\u001b[31m-        model.cuda()\u001b[39m\n",
      "\u001b[31m-    model.load_state_dict(torch.load(f'../input/mosh1-data-orig/roberta_fold{fold}.pth',map_location=torch.device('cpu')))\u001b[39m\n",
      "\u001b[31m-    model.eval()\u001b[39m\n",
      "\u001b[31m-    models.append(model)\u001b[39m\n",
      "\u001b[32m+#for fold in range(skf.n_splits):\u001b[39m\n",
      "\u001b[32m+model = BERTweetModel(conf=config)\u001b[39m\n",
      "\u001b[32m+if torch.cuda.is_available():\u001b[39m\n",
      "\u001b[32m+    model.cuda()\u001b[39m\n",
      "\u001b[32m+model.load_state_dict(torch.load(f'../input/mosh1-data-orig/roberta_fold3.pth')\u001b[39m\n",
      "\u001b[32m+model.eval()\u001b[39m\n",
      "\u001b[32m+models.append(model)\u001b[39m\n",
      " count=0\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/jpadilhas/30651255.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " fig = px.choropleth(america_grouped_latest, locations=\"pais\",\n",
      "                     color_continuous_scale='portland', \n",
      "\u001b[31m-                    title='Mapa de calor dos países infectados da América do Norte', scope='north america', height=800)\u001b[39m\n",
      "\u001b[32m+                    title='Mapa de calor dos países infectados da AMÉRICA DO NORTE', scope='north america', height=800)\u001b[39m\n",
      " fig.show()\n",
      "\n",
      " fig = px.choropleth(america_grouped_latest, locations=\"pais\",\n",
      "                     color_continuous_scale='portland', \n",
      "\u001b[31m-                    title='Mapa de calor dos países infectados da América do Sul', scope='south america', height=800)\u001b[39m\n",
      "\u001b[32m+                    title='Mapa de calor dos países infectados da AMÉRICA DO SUL', scope='south america', height=800)\u001b[39m\n",
      " fig.show()\n",
      "\n",
      " countries_data = pd.merge(america_grouped_latest, countries, on='pais', how='inn\n",
      " countries_data['p_confirmado'] = countries_data['confirmado'] / countries_data['populacao'] * 100\n",
      "\u001b[32m+countries_data['p_mortes'] = countries_data['mortes'] / countries_data['confirmado'] * 100\u001b[39m\n",
      " \n",
      "\n",
      " fig = px.bar(countries_data.sort_values('p_confirmado', ascending=False)[:20][::\n",
      " fig.update_layout(\n",
      "\u001b[31m-    title=\"Casos confirmados x População por país\",\u001b[39m\n",
      "\u001b[32m+    title=\"Casos confirmados x População por país (América)\",\u001b[39m\n",
      "     xaxis_title=\"País\",\n",
      "\n",
      "\n",
      "\u001b[31m-fig = px.line(grouped, x=\"data\", y=\"mortes\", color_discrete_sequence = ['cyan'])\u001b[39m\n",
      "\u001b[32m+fig = px.line(grouped, x=\"data\", y=\"mortes\", color_discrete_sequence = ['red'])\u001b[39m\n",
      " fig.update_layout(\n",
      "\n",
      " fig = px.bar(latest_grouped.sort_values('mortes', ascending=False)[:20][::1],\n",
      " fig.update_layout(\n",
      "\u001b[31m-    title=\"Total de Mortes confirmadas no MUNDO\",\u001b[39m\n",
      "\u001b[32m+    title=\"Total de Mortes confirmadas no MUNDO POR PAÍS\",\u001b[39m\n",
      "     xaxis_title=\"País\",\n",
      "\n",
      "\n",
      "\u001b[31m-#eua\u001b[39m\n",
      "\u001b[32m+#brazil\u001b[39m\n",
      " temp = grouped_br_date.melt(id_vars=\"data\", value_vars=['confirmado', 'mortes'], var_name='casos', value_name='count')\n",
      " \n",
      "\u001b[31m-fig = px.line(temp, x=\"data\", y=\"count\", color='casos', color_discrete_sequence = ['cyan', 'red'],log_y=True)\u001b[39m\n",
      "\u001b[32m+fig = px.line(temp, x=\"data\", y=\"count\", color='casos', color_discrete_sequence = ['cyan', 'red'],log_y=False)\u001b[39m\n",
      " fig.update_layout(\n",
      "\n",
      "\n",
      "\u001b[31m-#eua\u001b[39m\n",
      "\u001b[32m+#rest\u001b[39m\n",
      " temp = grouped_rest_date.melt(id_vars=\"data\", value_vars=['confirmado', 'mortes'], var_name='casos', value_name='count')\n",
      " \n",
      "\u001b[31m-fig = px.line(temp, x=\"data\", y=\"count\", color='casos', color_discrete_sequence = ['cyan', 'red'],log_y=True)\u001b[39m\n",
      "\u001b[32m+fig = px.line(temp, x=\"data\", y=\"count\", color='casos', color_discrete_sequence = ['cyan', 'red'],log_y=False)\u001b[39m\n",
      " fig.update_layout(\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/flower-classification-with-tpus/chekoduadarsh/33332696.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " IMAGE_SIZE = [512, 512]\n",
      "\u001b[31m-EPOCHS = 10\u001b[39m\n",
      "\u001b[32m+EPOCHS = 60\u001b[39m\n",
      " BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
      "\n",
      "\n",
      " IMAGE_SIZE = [512, 512]\n",
      "\u001b[31m-EPOCHS = 50\u001b[39m\n",
      "\u001b[32m+EPOCHS = 40\u001b[39m\n",
      " BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-5/sureshmecad/33792533.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # This Python 3 environment comes with many helpful analytics libraries installed\n",
      "\u001b[31m-# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\u001b[39m\n",
      "\u001b[31m-# For example, here's several helpful packages to load\u001b[39m\n",
      "\u001b[32m+# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\u001b[39m\n",
      "\u001b[32m+# For example, here's several helpful packages to load in \u001b[39m\n",
      " \n",
      " import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
      " \n",
      "\u001b[31m-# Input data files are available in the read-only \"../input/\" directory\u001b[39m\n",
      "\u001b[32m+# Input data files are available in the \"../input/\" directory.\u001b[39m\n",
      " # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
      " for dirname, _, filenames in os.walk('/kaggle/input'):\n",
      " \n",
      "\u001b[31m-# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \u001b[39m\n",
      "\u001b[31m-# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\u001b[39m\n",
      "\u001b[32m+# Any results you write to the current directory are saved as output.\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/google-quest-challenge/kaushal2896/23969185.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " model = Sequential([\n",
      "\u001b[31m-        Dense(128, input_shape=(X_train.shape[1],)),\u001b[39m\n",
      "\u001b[32m+        Dense(256, input_shape=(X_train.shape[1],)),\u001b[39m\n",
      "         Activation('relu'),\n",
      "\u001b[31m-        Dense(64),\u001b[39m\n",
      "\u001b[31m-        \u001b[39m\n",
      "\u001b[32m+        Dense(128),\u001b[39m\n",
      "\u001b[32m+        Activation ('relu'),\u001b[39m\n",
      "\u001b[32m+        Dense(128),\u001b[39m\n",
      "         Activation ('relu'),\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/R/wjholst/35547513.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " dict_reopenx = {\n",
      " \n",
      "\u001b[31m-skip_R_calc = ['Swexden','New Zealand','Belgium','Spain','France','US - Alasxxka', 'US - Mississippi','US - Montana']\u001b[39m\n",
      "\u001b[32m+skip_R_calc = ['Swexden','New Zealand','Belgium','Spain','France','US - Alasxxka', 'US - Mississippi','US - Montana','US - Hawaii',]\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/ghouls-goblins-and-ghosts-boo/oysteijo/437094.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " with open('submission-{}-hidden.csv'.format(num_hidden), 'w') as f:\n",
      "\u001b[31m-\tf.write(\"id,type\\n\")\u001b[39m\n",
      "\u001b[31m-\tfor index, monster in X_test.iterrows():\u001b[39m\n",
      "\u001b[31m-\t\tprobs = nn.forward( np.array(monster, dtype=np.float32))[0]\u001b[39m\n",
      "\u001b[31m-\t\tf.write(\"{},{}\\n\".format(index, y_train.columns.values[np.argmax(probs)][1:]))\u001b[39m\n",
      "\u001b[32m+    f.write(\"id,type\\n\")\u001b[39m\n",
      "\u001b[32m+    for index, monster in X_test.iterrows():\u001b[39m\n",
      "\u001b[32m+        probs = nn.forward( np.array(monster, dtype=np.float32))[0]\u001b[39m\n",
      "\u001b[32m+        f.write(\"{},{}\\n\".format(index, y_train.columns.values[np.argmax(probs)][1:]))\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/dimitreoliveira/5584144.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " print('Dataset size: %s' % DATASET_SIZE)\n",
      "\u001b[31m-print('Steps: %s' % STEPS)\u001b[39m\n",
      "\u001b[32m+print('Epochs: %s' % EPOCHS)\u001b[39m\n",
      " print('Learning rate: %s' % LEARNING_RATE)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/deepfake-detection-challenge/ustczhq/30482044.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def predict_on_video_set(video_paths,model):\n",
      "             pred=logits[1].item()\n",
      "\u001b[32m+            if pred>0.99:\u001b[39m\n",
      "\u001b[32m+                pred=0.99\u001b[39m\n",
      "\u001b[32m+            if pred<0.01:\u001b[39m\n",
      "\u001b[32m+                pred=0.01\u001b[39m\n",
      "             predictions.append(pred)\n",
      "\n",
      "\n",
      "\u001b[31m-modelx=WSDAN(num_classes=2, M=32, net='xception', pretrained=False).cuda()\u001b[39m\n",
      "\u001b[32m+modelx=WSDAN(num_classes=2, M=8, net='xception', pretrained=False).cuda()\u001b[39m\n",
      " modelx.load_state_dict(torch.load('/kaggle/input/zzzzzz/mymodel/ckpt.pth')['state_dict'])\n",
      " modelx.eval()\n",
      " predictions = predict_on_video_set(test_videos,modelx)\n",
      "\u001b[31m-predictions=np.array(predictions)\u001b[39m\n",
      "\u001b[31m-fx=np.argsort(predictions)\u001b[39m\n",
      "\u001b[31m-fa=(np.arange(fx.shape[0])-(fx.shape[0]-1)/2)/np.sqrt(fx.shape[0])\u001b[39m\n",
      "\u001b[31m-fa=1/(1+np.exp(-fa))\u001b[39m\n",
      "\u001b[31m-for i in range(fa.shape[0]):\u001b[39m\n",
      "\u001b[31m-    predictions[fx[i]]=fa[i]\u001b[39m\n",
      "\u001b[31m-predictions=predictions.tolist()\u001b[39m\n",
      " submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/aspgvu/26102945.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for (i, row) in tqdm(event4070.iterrows(), total=event4070.shape[0]):\n",
      "     else:\n",
      "\u001b[31m-        event4070.at[i, 'event_count_diff'] = event_count\u001b[39m\n",
      "\u001b[32m+        if event_diff == 0: \u001b[39m\n",
      "\u001b[32m+            event4070.at[i, 'event_count_diff'] = event_count\u001b[39m\n",
      "\u001b[32m+        if event_diff < 0:\u001b[39m\n",
      "\u001b[32m+            event4070.at[i, 'event_count_diff'] = -1\u001b[39m\n",
      "     \n",
      " for (i, row) in tqdm(event4070.iterrows(), total=event4070.shape[0]):\n",
      "     \n",
      "\u001b[31m-    time_diff = game_time - prev_game_time\u001b[39m\n",
      "\u001b[31m-    if time_diff >= 0 and time_diff < 9999:\u001b[39m\n",
      "\u001b[31m-        event4070.at[i, 'time_diff'] = time_diff\u001b[39m\n",
      "\u001b[32m+    if game_time >= prev_game_time:\u001b[39m\n",
      "\u001b[32m+        time_diff = game_time - prev_game_time\u001b[39m\n",
      "     else:\n",
      "\u001b[31m-        event4070.at[i, 'time_diff'] = 9999\u001b[39m\n",
      "\u001b[32m+        time_diff = 0\u001b[39m\n",
      "\u001b[32m+    event4070.at[i, 'time_diff'] = time_diff\u001b[39m\n",
      "     \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2018/bonlime/2707321.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  make_mosaic(x_test,print_connectivity=False,plot_images=True);\u001b[39m\n",
      "\u001b[32m+  make_mosaic(x_test,return_connectivity=True,plot_images=True);\u001b[39m\n",
      "\n",
      "\u001b[31m-  make_mosaic(x_train,print_connectivity=False,plot_images=True);\u001b[39m\n",
      "\u001b[32m+  make_mosaic(x_train,return_connectivity=False,plot_images=True);\u001b[39m\n",
      "\n",
      "\n",
      " ## This is how connectivity graph look like\n",
      "\u001b[31m-make_mosaic(x_test,print_connectivity=True,plot_images=False);\u001b[39m\n",
      "\u001b[32m+make_mosaic(x_test,return_connectivity=True,plot_images=False);\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/ffedericoni/5450941.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def tf_isAirport(latitude,longitude,airport_name='JFK'):\n",
      "         tf.logical_and(\n",
      "\u001b[31m-            tf.greater(longitude, coord[0]), tf.less(longitude, coord[1])\u001b[39m\n",
      "\u001b[32m+            tf.greater(latitude, coord[0]), tf.less(latitude, coord[1])\u001b[39m\n",
      "         ),\n",
      "         tf.logical_and(\n",
      "\u001b[31m-            tf.greater(latitude, coord[2]), tf.less(latitude, coord[3])\u001b[39m\n",
      "\u001b[32m+            tf.greater(longitude, coord[2]), tf.less(longitude, coord[3])\u001b[39m\n",
      "         )\n",
      "\n",
      "\n",
      " def feat_eng_func(features, label=None):\n",
      "\u001b[31m-    if DEBUG:\u001b[39m\n",
      "\u001b[31m-        print(\"Feature Engineered Label:\", label)\u001b[39m\n",
      "\u001b[32m+    print(\"Feature Engineered Label:\", label)\u001b[39m\n",
      "     #New features based on pickup datetime\n",
      " def feat_eng_func(features, label=None):\n",
      "     )\n",
      "\u001b[31m-    features['same_side_EAS'] = tf.equal(river_side(features['pickup_longitude'], features['pickup_latitude'], river_name='EAS'),\u001b[39m\n",
      "\u001b[31m-                                         river_side(features['dropoff_longitude'], features['dropoff_latitude'], river_name='EAS'))\u001b[39m\n",
      "\u001b[31m-    features['same_side_HUD'] = tf.equal(river_side(features['pickup_longitude'], features['pickup_latitude'], river_name='HUD'),\u001b[39m\n",
      "\u001b[31m-                                         river_side(features['dropoff_longitude'], features['dropoff_latitude'], river_name='HUD'))\u001b[39m\n",
      "\u001b[32m+    \u001b[39m\n",
      " #    features['pickup_minute'] = tf.substr(features['pickup_datetime'], 14, 2)\n",
      "\n",
      " with timer('Evaluating'):\n",
      "         features, label = sess.run(train_input_fn())\n",
      "\u001b[31m-    if DEBUG:\u001b[39m\n",
      "         print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)\n",
      "\n",
      " def create_feature_cols():\n",
      " #    tf.feature_column.numeric_column('dropoff_latitude'),\n",
      "\u001b[31m-    tf.feature_column.numeric_column('same_side_EAS'),\u001b[39m\n",
      "\u001b[31m-    tf.feature_column.numeric_column('same_side_HUD'),\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "     tf.feature_column.numeric_column('passenger_count'),\n",
      "\n",
      " eval_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, ba\n",
      " shutil.rmtree(OUTDIR, ignore_errors = True)\n",
      "\u001b[32m+#estimator = tf.estimator.LinearRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols())\u001b[39m\n",
      " runconfig = tf.estimator.RunConfig(model_dir = OUTDIR, keep_checkpoint_max=1, \n",
      " runconfig = tf.estimator.RunConfig(model_dir = OUTDIR, keep_checkpoint_max=1,\n",
      "                                    tf_random_seed=5)\n",
      "\u001b[31m-#estimator = tf.estimator.LinearRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols())\u001b[39m\n",
      "\u001b[31m-#optimizer = tf.train.FtrlOptimizer(learning_rate=0.05, #default is 0.05\u001b[39m\n",
      "\u001b[31m-#                                   learning_rate_power=-0.6,\u001b[39m\n",
      "\u001b[31m-#                                   initial_accumulator_value=0.1,\u001b[39m\n",
      "\u001b[31m-#                                   l1_regularization_strength=0.0,\u001b[39m\n",
      "\u001b[31m-#                                   l2_regularization_strength=0.0\u001b[39m\n",
      "\u001b[31m-#                                  )\u001b[39m\n",
      " estimator = tf.estimator.DNNRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols(),\n",
      "\u001b[31m-                                     hidden_units=[110, 110, 110], # 32 OK (43,37, 29: KO)\u001b[39m\n",
      "\u001b[31m-                                     optimizer= 'Ftrl', \u001b[39m\n",
      "\u001b[32m+                                     hidden_units=[128, 128, 128], # 32 OK (43,37, 29: KO)\u001b[39m\n",
      "\u001b[32m+                                     optimizer='Ftrl', \u001b[39m\n",
      "                                      batch_norm=False, \n",
      "\u001b[31m-                                     config=runconfig,\u001b[39m\n",
      "\u001b[32m+                                     config=runconfig\u001b[39m\n",
      "                                      dropout=0.1) \n",
      " estimator = tf.estimator.DNNRegressor(model_dir = OUTDIR, feature_columns = crea\n",
      " with timer('Training...'):\n",
      "\u001b[31m-    estimator.train(train_input_fn, max_steps=60000)\u001b[39m\n",
      "\u001b[32m+    estimator.train(train_input_fn, max_steps=80000)\u001b[39m\n",
      " with timer('Evaluating'):\n",
      " print(evaluation, file = sys.stderr)\n",
      " print(\"hidden_units=[64, 64, 64], V77\")\n",
      "\u001b[32m+print(\"hidden_units=[64, 64, 64], V77\", file = sys.stderr)\u001b[39m\n",
      " print(\"Added/removed embedding_column(yearday, 2), V78\")\n",
      "\u001b[32m+print(\"Added/removed embedding_column(yearday, 2), V78\", file = sys.stderr)\u001b[39m\n",
      " print(\"Added/removed clip fares at 2.50 instead of 1.0, V79\")\n",
      " print(\"num_buckets 20->25, V96\")\n",
      " print(\"num_buckets 25->27, V97\")\n",
      "\u001b[31m-print(\"added river_side feature, V120\")\u001b[39m\n",
      "\u001b[31m-#{'average_loss': 17.87\u001b[39m\n",
      "\u001b[32m+#{'average_loss': 18.07\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/9744586.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def make_model(emb_weights):\n",
      "     \n",
      "\u001b[31m-    embedding = SpatialDropout1D(0.2)(embedding)\u001b[39m\n",
      "\u001b[32m+    embedding = SpatialDropout1D(0.15)(embedding)\u001b[39m\n",
      "     lstm = Bidirectional(CuDNNGRU(64, return_sequences=True))(embedding)\n",
      "\u001b[31m-    lstm = SpatialDropout1D(0.2)(lstm)\u001b[39m\n",
      "\u001b[32m+    lstm = SpatialDropout1D(0.15)(lstm)\u001b[39m\n",
      "     lstm = Bidirectional(CuDNNGRU(32, return_sequences=True))(lstm)\n",
      "     a = Attention(MAX_SEQUENCE_LENGTH)(lstm)\n",
      "\u001b[31m-    a = BatchNormalization()(a)\u001b[39m\n",
      "     d1 = Dense(32)(a)\n",
      "\n",
      "\n",
      "\u001b[31m-thresh = Ts[np.argmax(f1s)]\u001b[39m\n",
      "\u001b[31m-avg = np.average(kaggle_predictions, axis=0)\u001b[39m\n",
      "\u001b[31m-df_out = pd.DataFrame({\"qid\":df_test[\"qid\"].values})\u001b[39m\n",
      "\u001b[31m-df_out['prediction'] = (avg > thresh).astype(int)\u001b[39m\n",
      "\u001b[31m-df_out.to_csv(\"submission.csv\", index=False)\u001b[39m\n",
      "\u001b[32m+# thresh = Ts[np.argmax(f1s)]\u001b[39m\n",
      "\u001b[32m+# avg = np.average(kaggle_predictions, axis=0)\u001b[39m\n",
      "\u001b[32m+# df_out = pd.DataFrame({\"qid\":df_test[\"qid\"].values})\u001b[39m\n",
      "\u001b[32m+# df_out['prediction'] = (avg > thresh).astype(int)\u001b[39m\n",
      "\u001b[32m+# df_out.to_csv(\"submission.csv\", index=False)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/elo-merchant-category-recommendation/frtgnn/8976177.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  post_processing_coeff = 0.3\u001b[39m\n",
      "\u001b[32m+  post_processing_coeff = 0.03\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/jialinzhang/9471768.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " print(os.listdir(\"../input\"))\n",
      " # Any results you write to the current directory are saved as output.\n",
      "\u001b[31m-# 3000条训练数据,F1_Score:0.235,Threshold:0.5,(句向量由question中各个词的词向量取平均得到)\u001b[39m\n",
      "\u001b[31m-# 300000条训练数据,F1_Score:0.492,Threshold:0.5,(句向量由question中各个词的词向量取平均得到)\u001b[39m\n",
      "\u001b[31m-# 400000条训练数据,F1_Score:0.490,Threshold:0.5,(句向量由question中各个词的词向量取平均得到)\u001b[39m\n",
      "\u001b[31m-# 300000条训练数据,F1_Score:0.326,Threshold:0.5,(句向量由question中各个词的词向量乘以其对应的TFIDF值，再取平均得到)\u001b[39m\n",
      "\u001b[31m-# 400000条训练数据,F1_Score:0.373,Threshold:0.5,(句向量由question中各个词的词向量乘以其对应的TFIDF值，再取平均得到)\u001b[39m\n",
      "\u001b[31m-# 此版本用1300000条训练数据,句向量由question中各个词的词向量取平均得到,threshold:0.26\u001b[39m\n",
      "\u001b[31m-# 由于训练集有130万条数据,数据集过大,故采用增量训练方式,每份训练集包含60万条数据\u001b[39m\n",
      "\u001b[32m+# 此版本用800000条训练数据,句向量由question中各个词的词向量取平均得到,threshold:0.5\u001b[39m\n",
      "\n",
      "\n",
      " # Data_stage_1.py\n",
      "\u001b[31m-# -*- coding: utf-8 -*-\u001b[39m\n",
      "\u001b[31m-# @Time    : 2019/1/7 14:56\u001b[39m\n",
      "\u001b[32m+# -*- coding: utf-8 -*- \u001b[39m\n",
      "\u001b[32m+# @Time    : 2019/1/7 14:56 \u001b[39m\n",
      " # @Author  : Weiyang\n",
      " questions = [] # [[word1,word2,..],..] question文本且分成一个一个词\n",
      " labels = [] # labels\n",
      "\u001b[31m-all_words = [] # [word1,word2,...]\u001b[39m\n",
      "\u001b[32m+all_words = [] # [word1,word2,...] \u001b[39m\n",
      " symbols = ['\\''] # don't, I'm\n",
      " count = 0\n",
      " for question,label in zip(train_data['question_text'],train_data['target']):\n",
      "\u001b[31m-    if count > 1300000:\u001b[39m\n",
      "\u001b[32m+    if count > 800000:\u001b[39m\n",
      "         break\n",
      " for question,label in zip(train_data['question_text'],train_data['target']):\n",
      "     count += 1\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[32m+    \u001b[39m\n",
      " del train_data\n",
      "\n",
      "\n",
      "\u001b[31m-# xgb_model.py\u001b[39m\n",
      "\u001b[32m+# lgb_model.py\u001b[39m\n",
      " # -*- coding: utf-8 -*- \n",
      "\n",
      " '''\n",
      "\u001b[31m-模块目标：用xgboost实现分类\u001b[39m\n",
      "\u001b[32m+模块目标：用LightGBM实现分类\u001b[39m\n",
      " 输入1：questions_vector,test_questions_vector,labels,test_qid\n",
      "\n",
      " \n",
      "\u001b[31m-import xgboost as xgb\u001b[39m\n",
      "\u001b[31m-from xgboost import plot_importance\u001b[39m\n",
      "\u001b[32m+import lightgbm as lgb\u001b[39m\n",
      " from sklearn.metrics import precision_score\n",
      " from sklearn.metrics import recall_score\n",
      " from sklearn.metrics import f1_score\n",
      "\u001b[31m-from matplotlib import pyplot as plt\u001b[39m\n",
      " import numpy as np\n",
      " import time\n",
      " \n",
      "\u001b[31m-class xgb_model:\u001b[39m\n",
      "\u001b[32m+class lgb_model:\u001b[39m\n",
      " \n",
      " class xgb_model:\n",
      "         y_train=self.label_true[:split_num]\n",
      "\u001b[31m-        num_rounds=100 #训练迭代次数\u001b[39m\n",
      "\u001b[31m-        params={\u001b[39m\n",
      "\u001b[31m-            'objective':'binary:logistic',\u001b[39m\n",
      "\u001b[31m-            'max_depth':2,\u001b[39m\n",
      "\u001b[31m-            'silent':1,\u001b[39m\n",
      "\u001b[31m-            'eta':1,\u001b[39m\n",
      "\u001b[31m-            'nthread':4\u001b[39m\n",
      "\u001b[31m-        }\u001b[39m\n",
      "\u001b[32m+        num_boost_round=100 #训练迭代次数\u001b[39m\n",
      "\u001b[32m+        params = {  \u001b[39m\n",
      "\u001b[32m+           'boosting_type': 'gbdt',  \u001b[39m\n",
      "\u001b[32m+           'objective': 'binary',  \u001b[39m\n",
      "\u001b[32m+           'metric': {'binary_logloss', 'auc'},  #二进制对数损失\u001b[39m\n",
      "\u001b[32m+           'num_leaves': 5,  \u001b[39m\n",
      "\u001b[32m+           'max_depth': 6,  \u001b[39m\n",
      "\u001b[32m+           'min_data_in_leaf': 450,  \u001b[39m\n",
      "\u001b[32m+           'learning_rate': 0.1,  \u001b[39m\n",
      "\u001b[32m+           'feature_fraction': 0.9,  \u001b[39m\n",
      "\u001b[32m+           'bagging_fraction': 0.95,  \u001b[39m\n",
      "\u001b[32m+           'bagging_freq': 5,  \u001b[39m\n",
      "\u001b[32m+           'lambda_l1': 1,    \u001b[39m\n",
      "\u001b[32m+           'lambda_l2': 0.001,  # 越小l2正则程度越高  \u001b[39m\n",
      "\u001b[32m+           'min_gain_to_split': 0.2,  \u001b[39m\n",
      "\u001b[32m+           'verbose': 5,  \u001b[39m\n",
      "\u001b[32m+           'is_unbalance': True  \u001b[39m\n",
      "\u001b[32m+        } \u001b[39m\n",
      "\u001b[32m+        \u001b[39m\n",
      "         if is_incremental == True:\n",
      " class xgb_model:\n",
      "             # batch_size : 每份训练集的大小\n",
      "\u001b[32m+            count = 0\u001b[39m\n",
      "             for start in range(0,len(x_train),batch_size):\n",
      "\u001b[31m-                self.model = xgb.train(params,dtrain=xgb.DMatrix(x_train[start:start+batch_size],y_train[start:start+batch_size]),xgb_model=self.model)\u001b[39m\n",
      "\u001b[32m+                train_x = np.array(x_train[start:start+batch_size])\u001b[39m\n",
      "\u001b[32m+                train_y = y_train[start:start+batch_size]\u001b[39m\n",
      "\u001b[32m+                print(count,train_x.shape,start+batch_size)\u001b[39m\n",
      "\u001b[32m+                if len(train_x.shape) != 2:\u001b[39m\n",
      "\u001b[32m+                    print('-'*30)\u001b[39m\n",
      "\u001b[32m+                    count += 1\u001b[39m\n",
      "\u001b[32m+                    continue\u001b[39m\n",
      "\u001b[32m+                count += 1\u001b[39m\n",
      "\u001b[32m+                self.model = lgb.train(params,train_set=lgb.Dataset(train_x,train_y),keep_training_booster=self.model)\u001b[39m\n",
      "         else:\n",
      "\u001b[31m-            dtrain=xgb.DMatrix(x_train,y_train)\u001b[39m\n",
      "\u001b[31m-            self.model=xgb.train(params,dtrain,num_rounds,xgb_model=self.model)\u001b[39m\n",
      "\u001b[31m-        #显示重要特征,值越大说明该特征越重要\u001b[39m\n",
      "\u001b[31m-        #plot_importance(self.model)\u001b[39m\n",
      "\u001b[31m-        #plt.show()\u001b[39m\n",
      "\u001b[32m+            # 不进行增量学习\u001b[39m\n",
      "\u001b[32m+            x_train = np.array(x_train)\u001b[39m\n",
      "\u001b[32m+            train_set=lgb.Dataset(x_train,y_train)\u001b[39m\n",
      "\u001b[32m+            self.model=lgb.train(params,train_set,num_boost_round,keep_training_booster=self.model)\u001b[39m\n",
      "\u001b[32m+        \u001b[39m\n",
      "         # 输出在测试集上的结果\n",
      "\u001b[31m-        x_test = self.data[split_num:]\u001b[39m\n",
      "\u001b[32m+        x_test = np.array(self.data[split_num:])\u001b[39m\n",
      "         y_test = self.label_true[split_num:]\n",
      "\u001b[31m-        dtest = xgb.DMatrix(x_test)\u001b[39m\n",
      "\u001b[31m-        label_pre = self.model.predict(dtest)\u001b[39m\n",
      "\u001b[32m+        label_pre = self.model.predict(x_test)\u001b[39m\n",
      "         label_pre = list(label_pre)\n",
      "\u001b[32m+        #print(label_pre)\u001b[39m\n",
      "         #寻找最佳阈值\n",
      " class xgb_model:\n",
      " \n",
      "\u001b[31m-    def cross_validation(self):\u001b[39m\n",
      "\u001b[31m-        \"\"\"交叉验证训练模型\"\"\"\u001b[39m\n",
      "\u001b[31m-        x_train = self.data\u001b[39m\n",
      "\u001b[31m-        y_train = self.label_true\u001b[39m\n",
      "\u001b[31m-        dtrain = xgb.DMatrix(x_train, y_train)\u001b[39m\n",
      "\u001b[31m-        num_rounds = 500  # 训练迭代次数\u001b[39m\n",
      "\u001b[31m-        params={\u001b[39m\n",
      "\u001b[31m-            'objective':'binary:logistic',\u001b[39m\n",
      "\u001b[31m-            'max_depth':2,\u001b[39m\n",
      "\u001b[31m-            'silent':0,\u001b[39m\n",
      "\u001b[31m-            'eta':1,\u001b[39m\n",
      "\u001b[31m-            'nthread':4\u001b[39m\n",
      "\u001b[31m-        }\u001b[39m\n",
      "\u001b[31m-        result=xgb.cv(params,dtrain,num_rounds,nfold=10)#10折交叉验证,全部数据\u001b[39m\n",
      "\u001b[31m-        for line in result:\u001b[39m\n",
      "\u001b[31m-            print(line)\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "     def predict(self,test_data,test_qid,threshold):\n",
      " class xgb_model:\n",
      "         #读取训练数据,格式为:qid \\t value1 \\t value2 \\t....\n",
      "\u001b[31m-        test_data = test_data # 存储question的vector\u001b[39m\n",
      "\u001b[32m+        test_data = np.array(test_data) # 存储question的vector\u001b[39m\n",
      "         test_qid = test_qid # 存储question的qid\n",
      "\u001b[31m-        dtest=xgb.DMatrix(test_data)\u001b[39m\n",
      "         # load trained model\n",
      "\u001b[31m-        label_pre=self.model.predict(dtest)\u001b[39m\n",
      "\u001b[32m+        label_pre=self.model.predict(test_data)\u001b[39m\n",
      "         label_pre = [int(prob >= threshold) for prob in list(label_pre)]\n",
      " class xgb_model:\n",
      " start = time.time()\n",
      "\u001b[31m-x = xgb_model()\u001b[39m\n",
      "\u001b[32m+x = lgb_model()\u001b[39m\n",
      " # 训练模型\n",
      " print(len(questions_vector),len(labels))\n",
      " x.load_data(questions_vector,labels)\n",
      "\u001b[31m-x.train(is_incremental=True,batch_size=600000) #增量学习,每份训练集包含30万条数据\u001b[39m\n",
      "\u001b[31m-#x.cross_validation()\u001b[39m\n",
      "\u001b[31m-#del questions_vector,labels\u001b[39m\n",
      "\u001b[32m+x.train(is_incremental=True,batch_size=600000) #增量学习,每份训练集包含60万条数据\u001b[39m\n",
      "\u001b[32m+del questions_vector,labels\u001b[39m\n",
      " # 模型预测\n",
      "\u001b[31m-sub = x.predict(test_questions_vector,test_qids,0.26)\u001b[39m\n",
      "\u001b[32m+sub = x.predict(test_questions_vector,test_qids,0.5)\u001b[39m\n",
      " # 将question为空的qid的类别添加进去并且设置为1\n",
      " if len(empty_question_qids) !=0:\n",
      " sub.to_csv('submission.csv',index=False)\n",
      "\u001b[31m-print(empty_question_qids)\u001b[39m\n",
      " \n",
      "\u001b[31m-print('The total time of xgb_model.py program is : %d min' % ((time.time() - start)/60))\u001b[39m\n",
      "\u001b[32m+print('The total time of program is : %d min' % ((time.time() - start)/60))\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/otto-group-product-classification-challenge/wakamezake/31339551.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[32m+sample_submit = pd.read_csv(\"../input/sampleSubmission.csv\")\u001b[39m\n",
      " submit = pd.concat([sample_submit[['id']], pd.DataFrame(y_pred_test)], axis = 1)\n",
      " submit.columns = sample_submit.columns\n",
      "\u001b[31m-submit.to_csv('submit.csv', index=False)\u001b[39m\n",
      "\u001b[32m+submit.to_csv('submit_xgboost.csv', index=False)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/seizure-detection/deepcnn/365056.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " NFFT = 1024\n",
      " N = 16; ch = 16\n",
      "\u001b[32m+m = 80000 # This will give 200 x 200 spectrogram\u001b[39m\n",
      "     \n",
      " def plot_spectrogram(pairs):\n",
      "         plt.subplot(2, 1, 1)\n",
      "\u001b[31m-        Pxx, freqs, bins, im = plt.specgram(X0[:20000,i], NFFT=NFFT, Fs=Fs, noverlap=512, cmap=plt.cm.jet)\u001b[39m\n",
      "\u001b[32m+        Pxx, freqs, bins, im = plt.specgram(X0[:m,i], NFFT=NFFT, Fs=Fs, noverlap=512, cmap=plt.cm.jet)\u001b[39m\n",
      "         plt.title('ch ' + str(i) + ': preictal absent or class 0')      \n",
      " def plot_spectrogram(pairs):\n",
      "         plt.subplot(2, 1, 2)\n",
      "\u001b[31m-        Pxx, freqs, bins, im = plt.specgram(X1[:20000,i], NFFT=NFFT, Fs=Fs, noverlap=512, cmap=plt.cm.jet)\u001b[39m\n",
      "\u001b[32m+        Pxx, freqs, bins, im = plt.specgram(X1[:m,i], NFFT=NFFT, Fs=Fs, noverlap=512, cmap=plt.cm.jet)\u001b[39m\n",
      "         plt.title('ch ' + str(i) + ': preictal present or class 1')\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/titanic/fatmakursun/13694282.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  test_pred = pd.DataFrame(test_prediction, columns= ['Target'])\u001b[39m\n",
      "\u001b[32m+  test_pred = pd.DataFrame(test_prediction, columns= ['Survived'])\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/santander-value-prediction-challenge/wesamelshamy/4165173.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " sns.set()\n",
      "\u001b[31m-mn = train.mean()\u001b[39m\n",
      "\u001b[31m-std = train.std()\u001b[39m\n",
      "\u001b[32m+mn_train = train.mean()\u001b[39m\n",
      "\u001b[32m+std_train = train.std()\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+mn_test = test.mean()\u001b[39m\n",
      "\u001b[32m+std_test = test.std()\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " \n",
      " fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
      " ax = axes[0]\n",
      "\u001b[31m-sns.distplot(mn, norm_hist=False, kde=False, ax=ax)\u001b[39m\n",
      "\u001b[31m-ax.get_yaxis().set_major_formatter(\u001b[39m\n",
      "\u001b[31m-    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\u001b[39m\n",
      "\u001b[32m+sns.distplot(mn_train, kde=False, norm_hist=True, ax=ax)\u001b[39m\n",
      "\u001b[32m+sns.distplot(mn_test, kde=False, norm_hist=True, ax=ax)\u001b[39m\n",
      " ax.get_xaxis().set_major_formatter(\n",
      "     matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
      "\u001b[31m-plt.title('Distribution of the mean value of features.')\u001b[39m\n",
      "\u001b[32m+ax.set_title('Distribution of the mean value of train/test features.')\u001b[39m\n",
      " ax.set_xlabel(r'Mean value ($\\mu$)')\n",
      " ax.set_ylabel('Number of features')\n",
      "\u001b[32m+ax.legend(['train', 'test'])\u001b[39m\n",
      " \n",
      " ax = axes[1]\n",
      "\u001b[31m-sns.distplot(std, norm_hist=False, kde=False, ax=ax)\u001b[39m\n",
      "\u001b[31m-ax.get_yaxis().set_major_formatter(\u001b[39m\n",
      "\u001b[31m-    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\u001b[39m\n",
      "\u001b[32m+sns.distplot(std_train, kde=False, norm_hist=True , ax=ax)\u001b[39m\n",
      "\u001b[32m+sns.distplot(std_test, kde=False, norm_hist=True , ax=ax)\u001b[39m\n",
      " ax.get_xaxis().set_major_formatter(\n",
      "     matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
      "\u001b[31m-ax.set_title('Distribution of std value of features.')\u001b[39m\n",
      "\u001b[32m+ax.set_title('Distribution of std value of train/test features.')\u001b[39m\n",
      " ax.set_xlabel(r'Standard Deviation ($\\sigma^2$)')\n",
      "\u001b[31m-ax.set_ylabel('Number of features');\u001b[39m\n",
      "\u001b[32m+ax.set_ylabel('Number of features')\u001b[39m\n",
      "\u001b[32m+ax.legend(['train', 'test']);\u001b[39m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for diff in more_features_examples[:40]:\n",
    "    display_kaggle_diff(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_diffs_that_match_regex(diffs,regex,limit=100):\n",
    "    matches = 0\n",
    "    for diff in diffs:\n",
    "        if re.search(regex,diff[\"cell_diff\"]):\n",
    "            display_kaggle_diff(diff)\n",
    "            matches += 1\n",
    "        if matches == limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/santander-customer-transaction-prediction/aaronl87/16375570.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " import pandas as pd\n",
      "\u001b[31m-from pandas.plotting import scatter_matrix\u001b[39m\n",
      " import numpy as np\n",
      "\u001b[31m-\u001b[39m\n",
      " from scipy.stats import ttest_ind, levene\n",
      " \n",
      "\u001b[31m-%matplotlib inline\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " import warnings\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/titanic/andyyang/1342951.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-# Statitic Test\u001b[39m\n",
      "\u001b[32m+# Statitic Test, variable is continuous, so we choose T-test\u001b[39m\n",
      " # H0: People survived and not survived have same fare, mean(survive_fare)=mean(non_survive_fare)\n",
      "\u001b[32m+from scipy.stats import ttest_ind\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/m5-forecasting-accuracy/armenabnousi/29611576.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " import numpy as np # linear algebra\n",
      " import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
      "\u001b[32m+import random\u001b[39m\n",
      "\u001b[32m+from scipy.stats import ttest_ind\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/carlineng/8787104.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,3 @@\n",
      "\u001b[32m+# Kern typically gets his punts off in 1.98 seconds. Hekker in 2.12 seconds.\u001b[39m\n",
      "\u001b[32m+# Difference again is statistically significant\u001b[39m\n",
      " print('Brett Kern time from snap to punt: {0}'.format(kern_snap_punt.mean()))\n",
      " print('Difference in time from snap to punt: {0}'.format(hekker_snap_punt.mean()\n",
      " \n",
      "\u001b[31m-print(stats.ttest_ind(kern_snap_punt.dropna(), hekker_snap_punt, equal_var=False))\u001b[39m\n",
      "\u001b[32m+print(stats.ttest_ind(kern_snap_punt.dropna(), hekker_snap_punt, equal_var=False))\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/m5-forecasting-accuracy/armenabnousi/29656654.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " sum_nonevent = sales_one_month_non_event.groupby('item_id').sum()#\n",
      " sum_nonevent['nonevent_sum'] = sum_nonevent.sum(axis = 1)\n",
      "\u001b[31m-sum_event.head()\u001b[39m\n",
      "\u001b[32m+sum_event.head()\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[31m-test_stat, pvals = ttest_ind(sales_comp[one_month_before[event]], sales_comp[nonevents[event]], axis = 1)\u001b[39m\n",
      "\u001b[32m+test_stat, pvals = ttest_ind(sales_comp[one_month_before[event]], sales_comp[nonevents[event]], axis = 1, equal_var = False)\u001b[39m\n",
      " sales_comp['test_stat'] = test_stat\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/gowrithampi/33522254.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-from matplotlib import pyplot as plt\u001b[39m\n",
      "\u001b[31m-from matplotlib import ticker as mtick\u001b[39m\n",
      "\u001b[31m-%matplotlib inline\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " from lifelines import KaplanMeierFitter\n",
      "\u001b[31m-figsize = (8,8)\u001b[39m\n",
      "\u001b[31m- \u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " kmf_data = covid_recovered['time_to_recovery'].value_counts()\n",
      " ax.set(xlabel = 'Days since diagnosis ' , ylabel = 'Percentage still sick', titl\n",
      " kmf.fit(time_to_recovery, event_occured, label = 'Percentage sick')\n",
      "\u001b[31m-kmf.plot(ax = ax)\u001b[39m\n",
      "\u001b[32m+kmf.plot(ax = ax)\u001b[39m\n",
      "\n",
      "\n",
      " \n",
      "\u001b[31m-import seaborn as sns\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      " import matplotlib.pyplot as plt\n",
      " print(tabulate(pd.DataFrame(covid_gender['Gender'].value_counts())))\n",
      " \n",
      "\u001b[32m+med_time_to_recovery_male =  covid_gender.loc[covid_gender['Gender']=='Male','time_to_recovery'].median()\u001b[39m\n",
      "\u001b[32m+print(\"Median time to recovery for males is{0: .0f} days.\".format(med_time_to_recovery_male))\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+med_time_to_recovery_female =  covid_gender.loc[covid_gender['Gender']=='Female','time_to_recovery'].median()\u001b[39m\n",
      "\u001b[32m+print(\"Median time to recovery for females is{0: .0f} days.\".format(med_time_to_recovery_female))\u001b[39m\n",
      " \n",
      " plt.show()\n",
      " ttest,pval = ttest_ind(covid_gender.loc[covid_gender['Gender']=='Female','time_to_recovery'],covid_gender.loc[covid_gender['Gender']=='Male','time_to_recovery'])\n",
      "\u001b[31m-print(\"p-value\",pval)\u001b[39m\n",
      "\u001b[32m+print(\"p-value {0: .2f}\".format(pval))\u001b[39m\n",
      "\n",
      "\n",
      " \n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " fig = plt.figure(figsize = figsize)\n",
      "\n",
      " covid_recovered['recovered'] = 1\n",
      " covid_age_gender = covid_recovered[(pd.notnull(covid_recovered['Age Bracketf'])&pd.notnull(covid_recovered['Gender']))]\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-rossi_dataset = load_rossi()\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      "\u001b[31m-\u001b[39m\n",
      " cph = CoxPHFitter()\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/adam1brownell/8794251.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-within_forty_ccuss_count = ccuss_line_pd[ccuss_line_pd.yard_line <= 30].shape[0]\u001b[39m\n",
      "\u001b[32m+within_forty_ccuss_count = ccuss_line_pd[ccuss_line_pd.yard_line <= 35].shape[0]\u001b[39m\n",
      " outside_fourty_ccuss_count = ccuss_line_pd.shape[0] - within_forty_ccuss_count\n",
      "\u001b[31m-within_fourty_count = punt_loc_pd[punt_loc_pd.yard_line <= 30].shape[0]\u001b[39m\n",
      "\u001b[32m+within_fourty_count = punt_loc_pd[punt_loc_pd.yard_line <= 35].shape[0]\u001b[39m\n",
      " outside_fourty_count = punt_loc_pd.shape[0] - within_fourty_count\n",
      " \n",
      "\u001b[31m-print('Concussions occur on', round(100*within_forty_ccuss_count / within_fourty_count,2), '% of the time on punts within own 30,')\u001b[39m\n",
      "\u001b[31m-print('in comparison to',round(100*outside_fourty_ccuss_count / outside_fourty_count,2), \"% on punts outside your 30\")\u001b[39m\n",
      "\u001b[31m-print('Players are', round((round(100*within_forty_ccuss_count / within_fourty_count,2)/round(100*outside_fourty_ccuss_count / outside_fourty_count,2)),2), 'times more likely to get concussed during a punt within your own 30')\u001b[39m\n",
      "\u001b[32m+print('Concussions occur on', round(100*within_forty_ccuss_count / within_fourty_count,2), '% of the time on punts within own 35,')\u001b[39m\n",
      "\u001b[32m+print('in comparison to',round(100*outside_fourty_ccuss_count / outside_fourty_count,2), \"% on punts outside your 35\")\u001b[39m\n",
      "\u001b[32m+print('Players are', round((round(100*within_forty_ccuss_count / within_fourty_count,2)/round(100*outside_fourty_ccuss_count / outside_fourty_count,2)),2), 'times more likely to get concussed during a punt within your own 35')\u001b[39m\n",
      "\n",
      "@@ -1,2 +1 @@\n",
      "\u001b[31m-from scipy.stats import ttest_ind\u001b[39m\n",
      " stat, pvalue = ttest_ind(ccuss_punt_yards,no_ccuss_punt_yards)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/mwolffe/10405473.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
      " import matplotlib.pyplot as plt # plotting library\n",
      "\u001b[32m+from matplotlib import rcParams # formatting params\u001b[39m\n",
      "\u001b[32m+from scipy.stats import ttest_ind\u001b[39m\n",
      "\u001b[32m+\u001b[39m\n",
      "\u001b[32m+# fix formatter to auto-layout\u001b[39m\n",
      "\u001b[32m+rcParams.update({'figure.autolayout': True})\u001b[39m\n",
      " \n",
      " import matplotlib.pyplot as plt # plotting library\n",
      " import os\n",
      "\u001b[31m-print(os.listdir(\"../input\"))\u001b[39m\n",
      "\u001b[32m+print(os.listdir(\"../input/omscs_hci_survey_coded\"))\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[31m-raw_survey_results = pd.read_csv(\"../input/wolffe_survey_results.csv\")\u001b[39m\n",
      "\u001b[32m+raw_survey_results = pd.read_csv(\"../input/omscs_hci_survey_coded/wolffe_survey_results_coded.csv\")\u001b[39m\n",
      " raw_survey_results # display survey results\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/loan-default-prediction/vishwanathkannan/27969737.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for i in numerical:\n",
      "     df2=df.groupby('loan_default').get_group(1)\n",
      "\u001b[31m-    t,pvalue=ttest_ind(df1[i],df2[i])b\u001b[39m\n",
      "\u001b[32m+    t,pvalue=ttest_ind(df1[i],df2[i])\u001b[39m\n",
      "     p.append(1-pvalue)\n",
      "\n",
      "\n",
      " df.loc[:,'No of Accounts'] = df['PRI.NO.OF.ACCTS'] + df['SEC.NO.OF.ACCTS']\n",
      "\u001b[31m-\u001b[39m\n",
      " df.loc[:,'PRI Inactive accounts'] = df['PRI.NO.OF.ACCTS'] - df['PRI.ACTIVE.ACCTS']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(more_features_examples, \"ttest_ind\", limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Diff Filters\n",
    "The following were created using the consolidated diff filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSOLIDATED_DIFF_FILTERS_PATH = \"/homes/gws/mikeam/RobustDataScience/data/processed/filtered_diffs.jsonl\"\n",
    "consolidated_examples = !shuf $CONSOLIDATED_DIFF_FILTERS_PATH\n",
    "consolidated_examples = [json.loads(x) for x in consolidated_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93032"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(consolidated_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/tmdb-box-office-prediction/takedown/13477503.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-test['revenue'] =  np.expm1(test[\"xgbfinal\"])\u001b[39m\n",
      "\u001b[32m+test['revenue'] =  np.expm1(test[\"lgbfinal\"])\u001b[39m\n",
      " test[['id','revenue']].head()\n",
      "\u001b[34mdata/processed/competitions/severstal-steel-defect-detection/watanabe2362/20644177.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def rle2mask(rle):\n",
      " def mask2rle(mask):\n",
      "\u001b[31m-    if np.sum(mask) == 0: return '1 1'\u001b[39m\n",
      "\u001b[32m+    if np.sum(mask) == 0: return ''\u001b[39m\n",
      "     ar = mask.flatten(order='F')\n",
      "\u001b[34mdata/processed/competitions/aerial-cactus-identification/kshashankrao/15278097.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "train_dataset = Dataset(data = X_train, transform=image_transform)\n",
      "\u001b[31m-test_dataset = Dataset(data = X_test, transform=image_transform)\u001b[39m\n",
      "\u001b[32m+test_dataset = Dataset(data = X_val, transform=image_transform)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/skeller/30635682.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for i in df1:\n",
      "         #model = SARIMAX(data, order=(1,0,0), seasonal_order=(0,1,1,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n",
      "\u001b[31m-        model = SARIMAX(data, order=(2,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\u001b[39m\n",
      "\u001b[32m+        model = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,1,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\u001b[39m\n",
      "         #model = ARIMA(data, order=(3,1,2))\n",
      " for i in df1:\n",
      "         #model = SARIMAX(data, order=(1,0,0), seasonal_order=(0,1,1,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n",
      "\u001b[31m-        model = SARIMAX(data, order=(2,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\u001b[39m\n",
      "\u001b[32m+        model = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,1,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\u001b[39m\n",
      "         #model = ARIMA(data, order=(3,1,2))\n",
      "\u001b[34mdata/processed/competitions/titanic/rblcoder/10140248.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train_scaled = train[['Age','SibSp','Parch', 'Fare']].copy()\u001b[39m\n",
      "\u001b[32m+train_scaled = train[num_features].copy()\u001b[39m\n",
      " train_scaled = scaler.transform(train_scaled)\n",
      "\u001b[31m-train_scaled = pd.concat([train_scaled,train[['Pclass','Sex','Cabin','Embarked']]],axis=1)\u001b[39m\n",
      "\u001b[32m+train_scaled = pd.concat([train_scaled,train[cat_features]],axis=1)\u001b[39m\n",
      " print(train_scaled.info())\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/eugeniematveeva/34106870.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "class SentimentDataset:\n",
      "         orig_sentiment = self.sentiment[idx]\n",
      "\u001b[31m-        x, y = self._prepare_data(text=orig_text,\u001b[39m\n",
      "\u001b[31m-                                  sentiment=orig_sentiment,\u001b[39m\n",
      "\u001b[32m+        sentiment, x, y = self._prepare_data(sentiment=orig_sentiment,\u001b[39m\n",
      "\u001b[32m+                                  text=orig_text,\u001b[39m\n",
      "                                   selected_text=orig_selected_text)\n",
      "\u001b[32m+        sentiment = torch.tensor(sentiment, dtype=torch.long)\u001b[39m\n",
      "         x = torch.tensor(x, dtype=torch.long)\n",
      "         y = torch.tensor(y, dtype=torch.long)\n",
      "\u001b[31m-        return x, y #, orig_text, orig_selected_text, orig_sentiment\u001b[39m\n",
      "\u001b[32m+        return sentiment, x, y #, orig_text, orig_selected_text, orig_sentiment\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/yulinzxc/5834750.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    tmp_household_predict_df = heads_predict_df.set_index(\"idhogar\")\u001b[39m\n",
      "\u001b[31m-    tmp_heads_predict_df = all_predict_df.loc[all_predict_df[\"parentesco1\"] == 1, :].set_index(\"idhogar\")\u001b[39m\n",
      "\u001b[31m-    tmp_heads_predict_df.loc[tmp_household_predict_df.index, \"Target\"] = tmp_household_predict_df.Target\u001b[39m\n",
      "\u001b[31m-    tmp_heads_predict_df = tmp_heads_predict_df.set_index(\"Id\")\u001b[39m\n",
      "\u001b[32m+    tmp_household_predict_df = X_predict.set_index(\"idhogar\")\u001b[39m\n",
      "\u001b[32m+    tmp_X_predict = all_predict_df.loc[all_predict_df[\"parentesco1\"] == 1, :].set_index(\"idhogar\")\u001b[39m\n",
      "\u001b[32m+    tmp_X_predict.loc[tmp_household_predict_df.index, \"Target\"] = tmp_household_predict_df.Target\u001b[39m\n",
      "\u001b[32m+    tmp_X_predict = tmp_X_predict.set_index(\"Id\")\u001b[39m\n",
      "     tmp_all_predict_df = all_predict_df.set_index(\"Id\")\n",
      "\u001b[31m-    tmp_all_predict_df.loc[tmp_heads_predict_df.index, \"Target\"] = tmp_heads_predict_df.Target\u001b[39m\n",
      "\u001b[32m+    tmp_all_predict_df.loc[tmp_X_predict.index, \"Target\"] = tmp_X_predict.Target\u001b[39m\n",
      "     tmp_all_predict_df[\"Target\"] = tmp_all_predict_df[\"Target\"].fillna(1).astype(int)\n",
      " def generate_final_predict_df(input_predict):\n",
      "     return final_predict_df\n",
      "\u001b[32m+final_predict_df = generate_final_predict_df(uniform_prediction)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/vinothkumarsubbiah/18972792.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train_df = data[(data.fare_amount > 0)] \u001b[39m\n",
      "\u001b[31m-train_df = train_df[(train_df.passenger_count > 0)]\u001b[39m\n",
      "\u001b[32m+df = data[(data.fare_amount > 0)] \u001b[39m\n",
      "\u001b[32m+df = df[(df.passenger_count > 0)]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/golion/20962361.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .5)\u001b[39m\n",
      "\u001b[32m+  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/siddharthcha519810/31722905.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sub['toxic'] = model_distilbert.predict(test_dataset, verbose=1)\u001b[39m\n",
      "\u001b[32m+sub['toxic'] = model.predict(test_dataset, verbose=1)\u001b[39m\n",
      " sub.to_csv('submission.csv', index=False)\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/christopherdzuwa/24354352.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5)])\u001b[39m\n",
      "\u001b[32m+layout = go.Layout(\u001b[39m\n",
      "\u001b[32m+    title=go.layout.Title(\u001b[39m\n",
      "\u001b[32m+        text=\"Women vs Men in Kenyan startups\" ,\u001b[39m\n",
      "\u001b[32m+        xref='paper',\u001b[39m\n",
      "\u001b[32m+        x=0\u001b[39m\n",
      "\u001b[32m+    )\u001b[39m\n",
      "\u001b[32m+)\u001b[39m\n",
      "\u001b[32m+fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5)], layout=layout)\u001b[39m\n",
      " fig.show()\n",
      "\u001b[34mdata/processed/competitions/tmdb-box-office-prediction/a4anandr/13002665.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "for fold in np.arange(k):\n",
      "\u001b[31m-        train_splits[fold] = train_df.loc[fold * train_size/fold : (fold+1) * train_size/k -1,:]\u001b[39m\n",
      "\u001b[32m+        train_splits[fold] = train_df.loc[fold * train_size/k : (fold+1) * (train_size/k) -1,:]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/rsna-intracranial-hemorrhage-detection/viswajithkn/22139175.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def get_pixels_hu(ds):\n",
      "     subdural_img = scaleAndconvertImage(ds,80,200)\n",
      "\u001b[31m-    bone_img = scaleAndconvertImage(ds,600,2000)\u001b[39m\n",
      "\u001b[32m+    soft_img = scaleAndconvertImage(ds,40,380)\u001b[39m\n",
      "     image = np.zeros((brain_img.shape[0], brain_img.shape[1], 3))\n",
      " def get_pixels_hu(ds):\n",
      "     image[:, :, 1] = subdural_img\n",
      "\u001b[31m-    image[:, :, 2] = bone_img    \u001b[39m\n",
      "\u001b[32m+    image[:, :, 2] = soft_img    \u001b[39m\n",
      "     return image    \n",
      " def generateImageData(train_files,y_train):\n",
      "             x_batch_data = []\n",
      "\u001b[31m-            y_batch_data = []            \u001b[39m\n",
      "\u001b[32m+            y_batch_data = [] \u001b[39m\n",
      "\u001b[32m+def generateValImageData(val_files,y_val):\u001b[39m\n",
      "\u001b[32m+    numBatches = int(np.ceil(len(val_files)/batch_size))\u001b[39m\n",
      "\u001b[32m+    while True:\u001b[39m\n",
      "\u001b[32m+        x_batch_data = []\u001b[39m\n",
      "\u001b[32m+        y_batch_data = []\u001b[39m\n",
      "\u001b[32m+        for i in range(numBatches):\u001b[39m\n",
      "\u001b[32m+            batchFiles = val_files[i*batch_size : (i+1)*batch_size]\u001b[39m\n",
      "\u001b[32m+            x_batch_data = np.array([readDCMFile(basePath + 'stage_1_train_images/' + i_f +'.dcm') for i_f in batchFiles])\u001b[39m\n",
      "\u001b[32m+            y_batch_data = y_val[i*batch_size : (i+1)*batch_size]\u001b[39m\n",
      "\u001b[32m+            yield x_batch_data,y_batch_data\u001b[39m\n",
      "\u001b[32m+            x_batch_data = []\u001b[39m\n",
      "\u001b[32m+            y_batch_data = []             \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/buchan/12740611.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "hist = model.fit_generator(\n",
      "     callbacks = [\n",
      "\u001b[31m-        EarlyStopping(monitor='val_loss', patience = 5, verbose = 1)\u001b[39m\n",
      "\u001b[32m+        EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)\u001b[39m\n",
      "     ]\n",
      "\u001b[34mdata/processed/competitions/severstal-steel-defect-detection/vh1981/21623973.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "     img = cv2.imread(os.path.join(TRAIN_IMAGE_PATH, filename ))\n",
      "\u001b[31m-    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\u001b[39m\n",
      "\u001b[32m+    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \u001b[39m\n",
      "\u001b[32m+    ax_idx += 1\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/avito-demand-prediction/hugoncosta/3507760.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "def preprocess(df):\n",
      "     df[\"Day of Month\"] = df['activation_date'].dt.day\n",
      "     df[\"txt\"] = df[\"title\"] + \" \" + df[\"description\"] + \" \" + df[\"param_1\"] + \" \" + df[\"param_2\"] + \" \" + df[\"param_3\"]\n",
      "     df[\"txt\"] = df[\"txt\"].str.lower() \n",
      "\u001b[31m-    df.drop([\"activation_date\", \"title\", \"description\", \"param_1\", \"param_2\", \"param_3\"], axis = 1, inplace = True)\u001b[39m\n",
      "\u001b[32m+    df[\"txt\"] = df[\"txt\"].str.replace('[^\\w\\s]',' ')\u001b[39m\n",
      "\u001b[32m+    df[\"txt\"] = df[\"txt\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\u001b[39m\n",
      "\u001b[32m+    df[\"stem_txt\"] =df[\"txt\"][0:len(df)].apply(lambda x: \" \".join([rs.stem(word) for word in x.split()]))\u001b[39m\n",
      "\u001b[32m+    df.drop([\"activation_date\", \"title\", \"description\", \"param_1\", \"param_2\", \"param_3\", \"txt\"], axis = 1, inplace = True)\u001b[39m\n",
      "     return df\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/CVxTz/7485842.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for batch in chunker(test_files, batch_size):\n",
      "     X = np.array(X)\n",
      "\u001b[31m-    preds_batch = model.predict(X).ravel().tolist()\u001b[39m\n",
      "\u001b[32m+    preds_batch = ((model.predict(X).ravel()+model.predict(X[:, ::-1, :, :]).ravel()+model.predict(X[:, ::-1, ::-1, :]).ravel()+model.predict(X[:, :, ::-1, :]).ravel())/4).tolist()\u001b[39m\n",
      "     preds += preds_batch\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/shravankp/14964799.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "xgbr = XGBRegressor()\n",
      "\u001b[31m-xgbr.fit(train_x, train_y)\u001b[39m\n",
      "\u001b[32m+xgb_train = xgbr.fit(train_x, train_y)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/jshubham/30600372.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "fig = px.scatter(clean_data,'hospibed','confirmed_norm',title = \"Confirmed % vs\n",
      "                                 hover_name=\"country\")\n",
      "\u001b[31m-fig.layout.yaxis.tickformat = ',.2%' \u001b[39m\n",
      "\u001b[32m+fig.layout.yaxis.tickformat = ',.2%'\u001b[39m\n",
      "\u001b[32m+fig.update_layout(xaxis_title = 'No. hospital beds per 1k',yaxis_title = 'Confirmed cases (%)')\u001b[39m\n",
      " fig.show()\n",
      "\u001b[34mdata/processed/competitions/quora-question-pairs/plarmuseau/1192169.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    svd = TruncatedSVD(n_components=10, n_iter=20, random_state=42)\u001b[39m\n",
      "\u001b[32m+    svd = TruncatedSVD(n_components=30)\u001b[39m\n",
      "     temp=pd.DataFrame(svd.fit_transform(questionD_tfidf))\n",
      " def get_features(df_features):\n",
      "     df_features=df_features.join(temp,how='inner')  \n",
      "\u001b[34mdata/processed/competitions/liverpool-ion-switching/neomatrix369/32892473.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clean_hist = []\u001b[39m\n",
      "\u001b[32m+train_clean_hist = []\u001b[39m\n",
      " for j,i in enumerate(model_segments):\n",
      "\u001b[31m-    clean_hist.append(np.histogram(train_signal[i], bins=hist_bins)[0])\u001b[39m\n",
      "\u001b[31m-    clean_hist[-1] = clean_hist[-1] / 500_000 # normalize histogram\u001b[39m\n",
      "\u001b[32m+    train_clean_hist.append(np.histogram(train_signal[i], bins=hist_bins)[0])\u001b[39m\n",
      "\u001b[32m+    train_clean_hist[-1] = train_clean_hist[-1] / 500_000 # normalize histogram\u001b[39m\n",
      " axes.legend();\n",
      "\u001b[34mdata/processed/competitions/titanic/ren666/4281406.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "clf.fit(X,y)\n",
      " #对cross validation数据进行预测\n",
      "\u001b[31m-cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')\u001b[39m\n",
      "\u001b[31m-predictions = clf.predict(cv_df.values[:,1:])\u001b[39m\n",
      "\u001b[32m+split_cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')\u001b[39m\n",
      "\u001b[32m+predictions = clf.predict(split_cv_df.values[:,1:])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/floriancpchx/10338124.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,2 @@\n",
      "\u001b[31m-df_train = df_train.drop(columns=['store_and_fwd_flag','id','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','pickup_datetime','dropoff_datetime','date','precipitation','snow fall','snow depth'])\u001b[39m\n",
      "\u001b[32m+df_test = df_test.drop(columns=['id','pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','store_and_fwd_flag','date','precipitation','snow fall','snow depth'])\u001b[39m\n",
      "\u001b[32m+df_test.info()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/whale-detection-challenge/dromosys/8650874.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,4 @@\n",
      "\u001b[31m-learn = create_cnn(data, models.resnet50, metrics=[accuracy, map5], model_dir=MODEL_PATH)\u001b[39m\n",
      "\u001b[32m+%%time\u001b[39m\n",
      "\u001b[32m+learn = create_cnn(data, models.resnet50, lin_ftrs=[2048], model_dir=MODEL_PATH)\u001b[39m\n",
      "\u001b[32m+learn.clip_grad();\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/sharmasanthosh/413613.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "algo = \"XGB\"\n",
      " #Add the n_estimators value to the below list if you want to run the algo\n",
      "\u001b[31m-n_list = numpy.array([1000])\u001b[39m\n",
      "\u001b[32m+n_list = numpy.array([300])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/kingnguyen/24567749.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-mae = 0\u001b[39m\n",
      "\u001b[31m-rmsle = 0\u001b[39m\n",
      "\u001b[31m-splits = 10\u001b[39m\n",
      "\u001b[32m+    kf = KFold(n_splits=splits, shuffle=True, random_state=42)\u001b[39m\n",
      "\u001b[32m+    modelStackCV = StackingCVRegressor(regressors=(model, modelLGBM,modelGBR),\u001b[39m\n",
      "\u001b[32m+                                    meta_regressor=model,\u001b[39m\n",
      "\u001b[32m+                                    use_features_in_secondary=True)\u001b[39m\n",
      "\u001b[32m+    for train_index, test_index in kf.split(dfTrainFinal):\u001b[39m\n",
      "\u001b[32m+        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\u001b[39m\n",
      "\u001b[32m+        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]      \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/freesound-audio-tagging/maxwell110/12823590.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-samp = train[train.n_label == 3].sample(1)\u001b[39m\n",
      "\u001b[32m+samp = train[(train.n_label == 6) & (train.is_curated == True)].sample(1)\u001b[39m\n",
      " print(samp.labels.values[0])\n",
      "\u001b[34mdata/processed/competitions/titanic/joshuajhchoi/27975899.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-frequencies = train1[\"Initial\"].value_counts(normalize = True)\u001b[39m\n",
      "\u001b[32m+frequencies = data[\"SibSp\"].value_counts(normalize = True)\u001b[39m\n",
      " frequencies\n",
      "\u001b[34mdata/processed/competitions/allstate-claims-severity/aaboyles/392850.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "params = {\n",
      " # Grid Search CV optimized settings\n",
      "\u001b[31m-bst = xgb.train(params, xgdmat, num_boost_round = 5000)\u001b[39m\n",
      "\u001b[32m+num_rounds = 1000\u001b[39m\n",
      "\u001b[32m+bst = xgb.train(params, xgdmat, num_boost_round = num_rounds)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/mukul1904/19824781.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "# based on above plot, picking the best params\n",
      "\u001b[31m-clf = RandomForestRegressor(n_estimators=20, max_depth=8, random_state=42)\u001b[39m\n",
      "\u001b[32m+clf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/arretvice/8079055.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def train_model(train_df,val_df,n_epochs=5,batch_size=1024):\n",
      "     inp = Input(shape=(max_word_len, 300))\n",
      "\u001b[31m-    lstm1=Bidirectional(CuDNNLSTM(64, return_sequences=True))(inp)\u001b[39m\n",
      "\u001b[31m-    attention1=Attention(max_word_len)(lstm1)\u001b[39m\n",
      "\u001b[32m+    lstm1=Bidirectional(CuDNNLSTM(32, return_sequences=True))(inp)\u001b[39m\n",
      "\u001b[32m+    lstm2=Bidirectional(CuDNNLSTM(32, return_sequences=True))(lstm1)\u001b[39m\n",
      "\u001b[32m+    attention1=Attention(max_word_len)(lstm2)\u001b[39m\n",
      "     dense1=Dense(32, activation='elu')(attention1)\n",
      "\u001b[31m-    outp = Dense(1, activation=\"sigmoid\")(dense1)\u001b[39m\n",
      "\u001b[32m+    dense2=Dense(32, activation='elu')(dense1)\u001b[39m\n",
      "\u001b[32m+    outp = Dense(1, activation=\"sigmoid\")(dense2)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/nfl-playing-surface-analytics/robikscube/25863997.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-ax = (tracks.query('isRushPass and s > 0.1') \\\u001b[39m\n",
      "\u001b[32m+injury_prone_pos = ['Wide Reciever', 'Linebacker', 'Defensive Back']\u001b[39m\n",
      "\u001b[32m+ax = (tracks.query('Position_inj in @injury_prone_pos and isRushPass and s > 0.1') \\\u001b[39m\n",
      "     .groupby(['Position_inj','isInjuryPlay'])['isLateralMovement'].mean() * 100) \\\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-3/kaimingk/31478085.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  data_test[\"rel_date\"] = data_test.apply(lambda x: (x[\"Date\"] - data_train_by_loc[\"Date\"].min()[x[\"location\"]]), axis=1).dt.days\u001b[39m\n",
      "\u001b[32m+  data_test[\"rel_date\"] = data_test.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/santander-customer-transaction-prediction/deepak525/11254808.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "#y_pred_valid = model.predict(X_valid)\n",
      "\u001b[31m-    prediction += model.predict(X_test, num_iteration=model.best_iteration)/10\u001b[39m\n",
      "\u001b[32m+    prediction += model.predict(X_test, num_iteration=model.best_iteration)/5\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/mercari-price-suggestion-challenge/viveknium/1818040.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "from sklearn import ensemble\n",
      "\u001b[31m-clf =  ensemble.GradientBoostingRegressor(learning_rate=0.5,max_features=0.8, n_estimators=1200, max_depth = 2,warm_start = True, verbose=1, random_state=45)\u001b[39m\n",
      "\u001b[32m+clf =  ensemble.GradientBoostingRegressor(learning_rate=0.5,max_features=0.8, n_estimators=1000, max_depth = 3,warm_start = True, verbose=1, random_state=45)\u001b[39m\n",
      " clf.fit(data, y)\n",
      "\u001b[34mdata/processed/competitions/rsna-intracranial-hemorrhage-detection/fanconic/23281674.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-test_df.iloc[:, :] = np.average(history.test_predictions, axis=0, weights=[1,2]) # let's do a weighted average for epochs (>1)\u001b[39m\n",
      "\u001b[32m+test_df.iloc[:, :] = y_test\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/beja96/10588869.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
      " embedded_sequences = embedding_layer(sequence_input)\n",
      "\u001b[31m-X = Bidirectional(CuDNNGRU(128, return_sequences=True))(embedded_sequences)\u001b[39m\n",
      "\u001b[32m+X = LSTM(128, return_sequences=True)(embedded_sequences)\u001b[39m\n",
      " X = Dropout(0.5)(X)\n",
      "\u001b[31m-X = Bidirectional(CuDNNGRU(128, return_sequences=False))(X)\u001b[39m\n",
      "\u001b[32m+X = LSTM(128)(X)\u001b[39m\n",
      " X = Dropout(0.5)(X)\n",
      "\u001b[31m-X = Dense(1)(X)\u001b[39m\n",
      "\u001b[32m+X = Dense(1, activation = \"sigmoid\")(X)\u001b[39m\n",
      " X = Activation('sigmoid')(X)\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/bapanes/6789192.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\u001b[39m\n",
      "\u001b[31m-model = build_model(mynp_train_0.shape[1]) \u001b[39m\n",
      "\u001b[32m+model_after_gridSearch = build_model(mynp_train_0.shape[1]) \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/36358298.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.13)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
      "\u001b[34mdata/processed/competitions/sentiment-analysis-on-movie-reviews/ruchibahl18/10416767.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " sequences = tokenizer.texts_to_sequences(X)\n",
      "\u001b[31m-X = pad_sequences(sequences, maxlen=max_len)\u001b[39m\n",
      "\u001b[32m+X = pad_sequences(sequences, maxlen=50)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/suniliitb96/6132871.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pred=my_new_model.predict_generator(test_generator, steps = len(test_generator), verbose=1)\u001b[39m\n",
      "\u001b[32m+pred=my_new_model.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/plasticgrammer/4889718.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-skewed = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\u001b[39m\n",
      "\u001b[32m+skewed = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\u001b[39m\n",
      " skewed_feats = skewed[skewed > 1].index\n",
      " print(skewed_feats)\n",
      "\u001b[34mdata/processed/competitions/tmdb-box-office-prediction/ajitesh0202/10618149.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-test['revenue'] =  np.expm1(0.2 * test[\"lgbfinal\"]+ 0.4 * test[\"catfinal\"] + 0.2 * test[\"xgbfinal\"])\u001b[39m\n",
      "\u001b[32m+test['revenue'] =  np.expm1(0.4* test[\"lgbfinal\"]+ 0.4 * test[\"catfinal\"] + 0.2 * test[\"xgbfinal\"])\u001b[39m\n",
      " test[['id','revenue']].to_csv('submission_Dragon1.csv', index=False)\n",
      "\u001b[34mdata/processed/competitions/avito-demand-prediction/jingqliu/3398440.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  count = dict(zip(list(count.keys()),range(1,43547 + 1)))\u001b[39m\n",
      "\u001b[32m+  count = dict(zip(list(count.keys()),range(1, 84411 + 1)))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/statoil-iceberg-classifier-challenge/ayanmaity/2012573.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X = Conv2D(10,kernel_size=(5,5))(X)\u001b[39m\n",
      "\u001b[32m+    X = Conv2D(32,kernel_size=(5,5))(X)\u001b[39m\n",
      "     X = BatchNormalization()(X)\n",
      " def Iceberg_model(input_shape):\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/arnabghose128/8225208.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pred_test_y = (pred_test_y > 0.34).astype(int)\u001b[39m\n",
      "\u001b[32m+pred_test_y = (pred_test_y > best_thresh).astype(int)\u001b[39m\n",
      " test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
      "\u001b[34mdata/processed/competitions/siim-acr-pneumothorax-segmentation/k1gaggle/33253278.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-connect_to_server () {\u001b[39m\n",
      "\u001b[32m+check_exit_status() {\u001b[39m\n",
      "\u001b[32m+  if [ -f $EXIT_FILE_PATH -a x$(cat $EXIT_FILE_PATH) = x0 ]; then\u001b[39m\n",
      "\u001b[32m+    return 0\u001b[39m\n",
      "\u001b[32m+  fi\u001b[39m\n",
      "\u001b[32m+  return 1 # not ok\u001b[39m\n",
      "\u001b[32m+}\u001b[39m\n",
      "\u001b[32m+connect_to_server() {\u001b[39m\n",
      "   cat rpt\n",
      " connect_to_server () {\n",
      " connect_setup() {\n",
      "\u001b[31m-  test -f $EXIT_FILE_PATH && test $(cat $EXIT_FILE_PATH) -eq 0 && rm $EXIT_FILE_PATH && return 0\u001b[39m\n",
      "\u001b[32m+  check_exit_status && return 0\u001b[39m\n",
      "   PID_FILE_PATH=$PID_FILE_PATH.$BASHPID\n",
      " connect_setup() {\n",
      "     COPROC_PID_backup=$COPROC_PID\n",
      "\u001b[32m+    echo $COPROC_PID_backup $PID_FILE_PATH # debug\u001b[39m\n",
      "     echo $COPROC_PID_backup > $PID_FILE_PATH\n",
      " connect_setup() {\n",
      "   RSRET=$?\n",
      "\u001b[31m-  [ x\"$RSRET\" == x\"0\" ] && echo $RSRET > $EXIT_FILE_PATH && return $RSRET\u001b[39m\n",
      "\u001b[32m+  if [ x\"$RSRET\" == x\"0\" ]; then\u001b[39m\n",
      "\u001b[32m+    echo $RSRET > $EXIT_FILE_PATH\u001b[39m\n",
      "\u001b[32m+    return $RSRET\u001b[39m\n",
      "\u001b[32m+  fi\u001b[39m\n",
      "   # else part below\n",
      " connect_setup() {\n",
      "   pgrep $RSPID && kill $RSPID\n",
      "\u001b[32m+  sleep 5 && [ ! $RSRET -eq 120 ] && connect_again # just recursively, sleep in case...\u001b[39m\n",
      "   echo $RSRET > $EXIT_FILE_PATH && return $RSRET   # exit, will cause rvs script exit, beside, RSRET not 0, mean connection loss thing\n",
      " floatToInt() {\n",
      "   echo $parsed\n",
      "\u001b[31m-} 2>/dev/null\u001b[39m\n",
      "\u001b[31m-while true; do\u001b[39m\n",
      "\u001b[31m-  if [ -f $EXIT_FILE_PATH -a ! $(cat $EXIT_FILE_PATH) -eq 0 ]; then\u001b[39m\n",
      "\u001b[31m-    connect_again 15  # just check connection loss and reconnect\u001b[39m\n",
      "\u001b[31m-    rm $EXIT_FILE_PATH  # just use the file as indicator\u001b[39m\n",
      "\u001b[31m-  fi\u001b[39m\n",
      "\u001b[31m-  sleep 3  # check every 3 seconds\u001b[39m\n",
      "\u001b[31m-done &\u001b[39m\n",
      "\u001b[32m+} 2> /dev/null\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/santander-value-prediction-challenge/nanomathias/4300260.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,3 @@\n",
      "\u001b[32m+%%time \u001b[39m\n",
      " # How many samples to take from both train and test\n",
      " print(f\">> Dropped {len(colsToRemove)} duplicate columns\")\n",
      " # Go through the columns one at a time (can't do it all at once for this dataset)\n",
      "\u001b[31m-for col in tqdm(total_df.columns):\u001b[39m\n",
      "\u001b[32m+total_df_all = deepcopy(total_df)              \u001b[39m\n",
      "\u001b[32m+for col in total_df.columns:\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/rpeer333/16527976.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def write_submission_file():\n",
      "     test = HistoPatches(image_dir=os.path.join(DATA_DIR, 'test'), transform=image_trans)\n",
      "\u001b[31m-    test_loader = DataLoader(test, batch_size=64)\u001b[39m\n",
      "\u001b[32m+    test_loader = DataLoader(test, batch_size=64, sample_n=1000)\u001b[39m\n",
      "     prediction_df = pd.DataFrame(columns=['id', 'label'])\n",
      "\u001b[34mdata/processed/competitions/planet-understanding-the-amazon-from-space/statinstilettos/1352531.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def get_features(img):\n",
      "          #'hough_min':hough_min, 'hough_kurtosis':hough_kurtosis, 'hough_skew':hough_skew\n",
      "\u001b[31m-        })\u001b[39m\n",
      "\u001b[31m-X  = get_features(X_sample)\u001b[39m\n",
      "\u001b[31m-y = names_train\u001b[39m\n",
      "\u001b[32m+        })\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/petfinder-adoption-prediction/praxitelisk/12143077.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-tfv = TfidfVectorizer(min_df=3,  max_features=10000,\u001b[39m\n",
      "\u001b[32m+max_train_len = [len(x) for x in train_desc]\u001b[39m\n",
      "\u001b[32m+max_test_len = [len(x) for x in test_desc]\u001b[39m\n",
      "\u001b[32m+tfv = TfidfVectorizer(min_df=3,  max_features=max([max(max_train_len), max(max_test_len)]),\u001b[39m\n",
      "         strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
      " X_test = tfv.transform(test_desc)\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/ragnar123/22852394.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # best features extracted from run_feature_selection function\n",
      "\u001b[31m-usefull_features = ['num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std', 'game_time_mean', 'game_time_sum', 'Activity', 'Clip', 'Game', 'CRYSTALCAVES', 'NONE', 'TREETOPCITY', ('game_time', 'mean', 'Clip'), \u001b[39m\n",
      "\u001b[31m-                    ('game_time', 'mean', 'Game'), ('game_time', 'std', 'Assessment')]\u001b[39m\n",
      "\u001b[32m+usefull_features = ['num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std', 'game_time_mean', 'game_time_sum', 'Activity', 'Clip', 'Game', 'CRYSTALCAVES', 'NONE', 'TREETOPCITY', 'title']\u001b[39m\n",
      " new_features = ['event_count_mean','event_count_sum', 'event_count_std', ('event_count', 'mean', 'Activity'), ('event_count', 'mean', 'Assessment'), ('event_count', 'mean', 'Clip'), ('event_count', 'mean', 'Game'), ('event_count', 'std', 'Activity'), \n",
      " new_features = ['event_count_mean','event_count_sum', 'event_count_std', ('event\n",
      "                 ('event_count', 'sum', 'NONE'), ('event_count', 'sum', 'TREETOPCITY'), 'hour_mean', 'hour_sum', 'hour_std', 'dayofweek_mean', 'dayofweek_sum', 'dayofweek_std', 'event_id_count_mean', 'event_id_count_sum', 'event_id_count_std', \n",
      "\u001b[31m-                'event_code_count_mean', 'event_code_count_sum', 'event_code_count_std', 'num_correct_mean', 'num_correct_std', 'num_incorrect_\u001b[39m\n",
      "\u001b[32m+                'event_code_count_mean', 'event_code_count_sum', 'event_code_count_std', 'num_correct_mean', 'num_correct_std']\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/madadinga/14474169.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "if test is not None:\n",
      "\u001b[31m-            sub_prediction += pipeline.predict(test) / num_folds\u001b[39m\n",
      "\u001b[32m+            sub_prediction += model.predict(test) / num_folds\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/mercari-price-suggestion-challenge/girlduck/2188701.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+n_words = 20 # top most common words\u001b[39m\n",
      " text = train_raw['item_description'].astype(str).tolist()\n",
      "\u001b[31m-tokenizer = Tokenizer()\u001b[39m\n",
      "\u001b[32m+tokenizer = Tokenizer(num_words=n_words)\u001b[39m\n",
      " tokenizer.fit_on_texts(text)\n",
      "\u001b[34mdata/processed/competitions/march-madness-analytics-2020/kowjan1/33109218.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-hwon = df.where(df['WLoc'].isin(['H', 'N']))\u001b[39m\n",
      "\u001b[31m-hlost = df.where(df['WLoc'].isin(['A']))\u001b[39m\n",
      "\u001b[31m-awon = df.where(df['WLoc'].isin(['A']))\u001b[39m\n",
      "\u001b[31m-alost = df.where(df['WLoc'].isin(['H', 'N']))\u001b[39m\n",
      "\u001b[32m+hwon = df[df['WLoc'].isin(['H', 'N'])]\u001b[39m\n",
      "\u001b[32m+hlost = df[df['WLoc'].isin(['A'])]\u001b[39m\n",
      "\u001b[32m+awon = df[df['WLoc'].isin(['A'])]\u001b[39m\n",
      "\u001b[32m+alost = df[df['WLoc'].isin(['H', 'N'])]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/stocks/18123805.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "sample_submission_blend = sample_submission.copy()\n",
      "\u001b[31m-sample_submission_blend['isFraud'] = (sample_submission1['isFraud'] + sample_submission2['isFraud']*0.5+ sample_submission3['isFraud']*0.5+ sample_submission4['isFraud'])/3\u001b[39m\n",
      "\u001b[32m+sample_submission_blend['isFraud'] = (sample_submission1a['isFraud']+sample_submission1['isFraud'] + sample_submission2['isFraud']*0.5+ sample_submission3['isFraud']*0.5+ sample_submission4['isFraud'])/4\u001b[39m\n",
      " sample_submission_blend.to_csv('xgboost_under_over_blend.csv')\n",
      " sample_submission_blend2 = sample_submission.copy()\n",
      "\u001b[31m-sample_submission_blend2['isFraud'] = (sample_submission1['isFraud']*0.5 + sample_submission2['isFraud']*0.25+ sample_submission3['isFraud']*0.25+ sample_submission4['isFraud'])/2\u001b[39m\n",
      "\u001b[32m+sample_submission_blend2['isFraud'] = (sample_submission1a['isFraud']*0.5+sample_submission1['isFraud']*0.5 + sample_submission2['isFraud']*0.25+ sample_submission3['isFraud']*0.25+ sample_submission4['isFraud'])/2.5\u001b[39m\n",
      " sample_submission_blend2.to_csv('xgboost_under_over_blend2.csv')\n",
      " sample_submission_blend_equal = sample_submission.copy()\n",
      "\u001b[31m-sample_submission_blend_equal['isFraud'] = (sample_submission1['isFraud'] + sample_submission2['isFraud']+ sample_submission3['isFraud']+ sample_submission4['isFraud'])/4\u001b[39m\n",
      "\u001b[32m+sample_submission_blend_equal['isFraud'] = (sample_submission1a['isFraud']+sample_submission1['isFraud'] + sample_submission2['isFraud']+ sample_submission3['isFraud']+ sample_submission4['isFraud'])/5\u001b[39m\n",
      " sample_submission_blend_equal.to_csv('xgboost_under_over_blend_equal.csv')\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/jimmymvp/12504701.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-model_svm = sklearn.svm.SVR(kernel='rbf', C=0.1)\u001b[39m\n",
      "\u001b[32m+model_svm = sklearn.svm.SVR(**best_svm_params)\u001b[39m\n",
      " model_svm = make_pipeline(RobustScaler(), model_svm)\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/tunguz/10874570.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
      "\u001b[31m-aml = H2OAutoML(max_models=200, seed=33, max_runtime_secs=21600)\u001b[39m\n",
      "\u001b[32m+aml = H2OAutoML(max_models=1000, seed=55, max_runtime_secs=31000)\u001b[39m\n",
      " aml.train(x=x, y=y, training_frame=train)\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/allunia/6521330.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "class BernoulliMixture:\n",
      "     def get_loglikelihood(self):\n",
      "\u001b[31m-        self.log_likelihood = np.sum(np.log(np.tensordot(self.pi, np.exp(self.log_bernoullis), (0,1))))\u001b[39m\n",
      "\u001b[32m+        self.log_likelihood = np.sum(np.log(np.tensordot(self.pi, self.bernoullis, (0,1))))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/polarbearzyx/15902171.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "def preprocess(data):\n",
      "     data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
      "\u001b[31m-    data = data.astype(str).apply(lambda x: replace_typical_misspell(x, mispell_dict))\u001b[39m\n",
      "\u001b[32m+    data = data.astype(str).apply(lambda x: replace_typical_misspell(x))\u001b[39m\n",
      "     data = data.astype(str).apply(lambda x: clean_numbers(x))\n",
      "\u001b[34mdata/processed/competitions/PLAsTiCC-2018/tarunpaparaju/7380777.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  features = features.drop('object_id', axis=1)\u001b[39m\n",
      "\u001b[32m+  features = features.drop(['object_id'], axis=1).values\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dog-breed-identification/jcesquiveld/8795786.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-augs_ant = iaa.Sequential([\u001b[39m\n",
      "\u001b[31m-    iaa.Fliplr(0.5),\u001b[39m\n",
      "\u001b[31m-    iaa.Sometimes(0.2, iaa.Affine(rotate=(-15,15), mode='edge')),    \u001b[39m\n",
      "\u001b[31m-    iaa.SomeOf((0,2), [\u001b[39m\n",
      "\u001b[31m-        iaa.AdditiveGaussianNoise(scale=0.01*255),        \u001b[39m\n",
      "\u001b[31m-        iaa.Sharpen(alpha=(0.0,0.3)),\u001b[39m\n",
      "\u001b[31m-        iaa.ContrastNormalization((0.8,1.2)),\u001b[39m\n",
      "\u001b[31m-        iaa.Add(50, per_channel=True),\u001b[39m\n",
      "\u001b[31m-        iaa.AverageBlur(k=(2,11)),\u001b[39m\n",
      "\u001b[31m-        iaa.Multiply((0.8,1.2)),\u001b[39m\n",
      "\u001b[31m-        iaa.Add((-20,20), per_channel=0.5),\u001b[39m\n",
      "\u001b[31m-        iaa.Grayscale(alpha=(0.0,1.0))\u001b[39m\n",
      "\u001b[31m-    ])\u001b[39m\n",
      "\u001b[31m-    ,    iaa.Lambda(rescale_imgs, keypoints)\u001b[39m\n",
      "\u001b[31m-]) \u001b[39m\n",
      " augs = iaa.Sequential([\n",
      " augs = iaa.Sequential([\n",
      "     iaa.Sometimes(0.2, iaa.Affine(rotate=(-15,15), mode='edge')),  \n",
      "\u001b[31m-    iaa.Affine(scale=(0.7,1.3), mode='reflect'),\u001b[39m\n",
      "\u001b[31m-    iaa.SomeOf((0,2), [\u001b[39m\n",
      "\u001b[32m+    iaa.SomeOf((0,3), [\u001b[39m\n",
      "         iaa.AdditiveGaussianNoise(scale=0.01*255),        \n",
      "\u001b[34mdata/processed/competitions/denoising-dirty-documents/zhoulingyan0228/4451547.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-PATCH_WIDTH_HALF = 4\u001b[39m\n",
      "\u001b[32m+PATCH_WIDTH_HALF = 5\u001b[39m\n",
      " PATCH_WIDTH = PATCH_WIDTH_HALF * 2 + 1\n",
      " def train_patch_generator(train_imgs, train_cleaned_imgs, epochs = 5):\n",
      "         for train_file, train_cleaned_file in zip(train_imgs, train_cleaned_imgs):\n",
      "\u001b[31m-            patches = []\u001b[39m\n",
      "\u001b[31m-            labels = []\u001b[39m\n",
      "             train_img = cv2.imread(train_file, cv2.IMREAD_GRAYSCALE)\n",
      " def train_patch_generator(train_imgs, train_cleaned_imgs, epochs = 5):\n",
      "             train_img_ext = cv2.copyMakeBorder(train_img, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, cv2.BORDER_REPLICATE)\n",
      "\u001b[32m+            thresholded_img_ext = cv2.adaptiveThreshold(train_img_ext,255,cv2.ADAPTIVE_THRESH_MEAN_C,\u001b[39m\n",
      "\u001b[32m+                                         cv2.THRESH_BINARY,31,30) \u001b[39m\n",
      "             #eroded_img_ext = cv2.erode(train_img_ext, np.ones((3,3),np.uint8), 1)\n",
      " def train_patch_generator(train_imgs, train_cleaned_imgs, epochs = 5):\n",
      "             for i in range(train_img.shape[0]):\n",
      "\u001b[32m+                patches = []\u001b[39m\n",
      "\u001b[32m+                labels = []\u001b[39m\n",
      "                 for j in range(train_img.shape[1]):\n",
      " def train_patch_generator(train_imgs, train_cleaned_imgs, epochs = 5):\n",
      "                     patch_c1 = train_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255.\n",
      "\u001b[32m+                    patch_c2 = thresholded_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255.\u001b[39m\n",
      "                     #patch_c3 = eroded_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255.\n",
      "                     #patch_c4 = eroded_thresh_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255.\n",
      "\u001b[31m-                    patches.append(np.expand_dims(patch_c1, axis=2))\u001b[39m\n",
      "\u001b[32m+                    patches.append(np.stack((patch_c1, patch_c2), axis=2))\u001b[39m\n",
      "                     labels.append(label / 255.)\n",
      "\u001b[31m-            patches = np.array(patches)# patches.shape\u001b[39m\n",
      "\u001b[31m-            labels = np.array(labels) # labels.shape\u001b[39m\n",
      "\u001b[31m-            yield (patches, labels)\u001b[39m\n",
      "\u001b[32m+                patches = np.array(patches)# patches.shape\u001b[39m\n",
      "\u001b[32m+                labels = np.array(labels) # labels.shape\u001b[39m\n",
      "\u001b[32m+                yield (patches, labels)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/talkingdata-adtracking-fraud-detection/johnluo721/3240242.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-d={}\u001b[39m\n",
      "\u001b[31m-for col in train:\u001b[39m\n",
      "\u001b[31m-    d[col] = train[col].unique()\u001b[39m\n",
      "\u001b[32m+col_dic={}\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/frtgnn/7547671.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-XGB = XGBClassifier(max_depth=5,learning_rate=0.05,n_estimators=100,n_jobs=-1)\u001b[39m\n",
      "\u001b[32m+XGB = XGBClassifier(max_depth=4,learning_rate=0.01,n_estimators=250,n_jobs=-1,min_child_weight=2)\u001b[39m\n",
      " XGB.fit(X_train,y_train)\n",
      "\u001b[34mdata/processed/competitions/m5-forecasting-accuracy/sibmike/32278249.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "def get_w(sale_usd):\n",
      "     # Calculate the total sales in USD for each item id:\n",
      "\u001b[31m-    total_sales_usd = sale_usd.groupby(['id'])['amount'].apply(np.sum).values\u001b[39m\n",
      "\u001b[32m+    total_sales_usd = sale_usd.groupby(['id'])['sale_usd'].apply(np.sum).values\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/willkoehrsen/5152830.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # Create correlation matrix\n",
      "\u001b[31m-corr_matrix = ind_agg.corr()\u001b[39m\n",
      "\u001b[32m+corr_matrix = data.corr()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/anammocanu/7301422.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-fig['layout'].update(title='Data Scientists - Distribution of Top 5 Industries by Highest level of Education in %', margin = dict(b = 110), yaxis = dict(title = \"Percentages\"))\u001b[39m\n",
      "\u001b[32m+fig['layout'].update(title='Data Scientists - Distribution of Top 5 Industries by Highest level of Education in %', margin = dict(b = 110), yaxis = dict(title = \"Percentages\"), paper_bgcolor = \"#F5F6F9\",  plot_bgcolor = \"#F5F6F9\")\u001b[39m\n",
      " iplot(fig) \n",
      "\u001b[34mdata/processed/competitions/tgs-salt-identification-challenge/wenjieluo/7313690.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def residual_block(blockInput, num_filters=16, batch_activate = False):\n",
      "     x = BatchActivate(blockInput)\n",
      "\u001b[31m-    x = convolution_block(x, num_filters, (1,1) )\u001b[39m\n",
      "     x = convolution_block(x, num_filters, (3,3))\n",
      "\u001b[31m-    x = convolution_block(x, num_filters*4, (1,1), activation=False)\u001b[39m\n",
      "\u001b[32m+    x = convolution_block(x, num_filters, (3,3), activation=False)\u001b[39m\n",
      "     x = Add()([x, blockInput])\n",
      "\u001b[34mdata/processed/competitions/siim-isic-melanoma-classification/hubwag/36420659.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "def show_interactive_embedding(train_df, colors = None, sizes = None):\n",
      "                   #symbol = 'is_train',\n",
      "\u001b[31m-                  hover_data = q.columns,\u001b[39m\n",
      "\u001b[32m+                  hover_data = train_df.columns,\u001b[39m\n",
      "                   width = 1200, height = 1200,\n",
      " def show_interactive_embedding(train_df, colors = None, sizes = None):\n",
      "             plot_rgb_hist(axs[i,0], hists[x])        \n",
      "\u001b[31m-            axs[i,0].set_title(f\"{q.at[x, 'target']}, {q.at[x,'ind']} {q.at[x, 'diagnosis']} \")\u001b[39m\n",
      "\u001b[32m+            axs[i,0].set_title(f\"{train_df.at[x, 'target']}, {train_df.at[x,'ind']} {train_df.at[x, 'diagnosis']} \")\u001b[39m\n",
      "             axs[i,0].axes.get_xaxis().set_visible(False)        \n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/erickortegadn/37288386.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "class CNN(nn.Module):\n",
      "         self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
      "\u001b[31m-        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\u001b[39m\n",
      "\u001b[32m+        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)\u001b[39m\n",
      "         self.fc1 = nn.Linear(3*3*64, 256)\n",
      " class CNN(nn.Module):\n",
      "         x = F.relu(self.conv1(x))\n",
      "\u001b[32m+        x = F.dropout(x, p=0.5, training=self.training)\u001b[39m\n",
      "         x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
      " class CNN(nn.Module):\n",
      "         x = F.dropout(x, p=0.5, training=self.training)\n",
      "\u001b[31m-        x = x.view(-1,3*3*64 )\u001b[39m\n",
      "\u001b[32m+        x = x.view(-1,3*3*64)\u001b[39m\n",
      "         x = F.relu(self.fc1(x))\n",
      "\u001b[34mdata/processed/competitions/airbus-ship-detection/manchans/16513412.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def f2(masks_true, masks_pred):\n",
      "     for th in thresholds:\n",
      "\u001b[31m-        tp = sum([iou > 0 for iou in ious])\u001b[39m\n",
      "\u001b[32m+        tp = sum([iou > th for iou in ious])\u001b[39m\n",
      "         fn = len(masks_true) - tp\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/aldrickpaul/23401543.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " countries = ('France','India','Australia','United States of America','Netherlands','Germany','Russia',\n",
      " countries = ('France','India','Australia','United States of America','Netherland\n",
      "              'Norway','Thailand','China','Switzerland','Argentina','Viet Nam','Denmark','Republic of Korea','New Zealand','Iran, Islamic Republic of...','Peru',\n",
      "\u001b[31m-             'Kenya','Malaysia','Belgium','Austria','Algeria','Romania','Philippines','Tunisia','Saudi Arabia')\u001b[39m\n",
      "\u001b[32m+             'Kenya','Malaysia','Belgium','Austria','Algeria','Romania','Philippines','Tunisia','Saudi Arabia') #labels for each bar\u001b[39m\n",
      "\u001b[32m+y_p = np.arange(len(countries)) #coordinates posiitons for each label\u001b[39m\n",
      "\u001b[32m+y_p = [2*i for i in y_p] # this spaces the bars out for better readability\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/humpback-whale-identification/dromosys/8620989.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,4 @@\n",
      "\u001b[31m-learn = create_cnn(data, models.resnet50, metrics=[accuracy, map5], model_dir=MODEL_PATH)\u001b[39m\n",
      "\u001b[32m+%%time\u001b[39m\n",
      "\u001b[32m+learn = create_cnn(data, models.resnet50, lin_ftrs=[2048], model_dir=MODEL_PATH)\u001b[39m\n",
      "\u001b[32m+learn.clip_grad();\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/liverpool-ion-switching/shinogi/34009732.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # 上記のパラメータでモデルを学習する\n",
      "\u001b[31m-model = lgb.train(best_params, lgb_train, valid_sets=lgb_eval)\u001b[39m\n",
      "\u001b[32m+model = lgb.train(best_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100)\u001b[39m\n",
      " # テストデータを予測する\n",
      "\u001b[34mdata/processed/competitions/flower-classification-with-tpus/taohoang/30984409.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " if VALIDATION:\n",
      " else:\n",
      "\u001b[31m-    history2 = model2.fit(get_training_dataset(), steps_per_epoch=STEPS_PER_EPOCH, epochs=30, callbacks = [lr_callback])\u001b[39m\n",
      "\u001b[32m+    history2 = model2.fit(get_training_dataset(), steps_per_epoch=STEPS_PER_EPOCH, epochs=50, callbacks = [lr_callback])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dog-breed-identification/igorslima/5668011.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-base_model = ResNet50(weights=\"../input/keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, input_shape=(im_size, im_size, 3))\u001b[39m\n",
      "\u001b[32m+base_model = VGG16(weights=\"../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\",include_top=False, input_shape=(im_size, im_size, 3))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ashrae-energy-prediction/cereniyim/24649382.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " le = LabelEncoder()\n",
      "\u001b[31m-le_primary_use = le.fit_transform(train.primary_use)\u001b[39m\n",
      "\u001b[32m+le_primary_use = le.fit_transform(test.primary_use)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/plant-pathology-2020-fgvc7/biruk1230/33491916.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-with strategy.scope():\u001b[39m\n",
      "\u001b[31m-    model = get_model()\u001b[39m\n",
      "\u001b[31m-model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['categorical_accuracy'])\u001b[39m\n",
      "\u001b[32m+def get_model4():\u001b[39m\n",
      "\u001b[32m+    model = tf.keras.Sequential([\u001b[39m\n",
      "\u001b[32m+        InceptionResNetV2(input_shape=(img_size, img_size, 3), weights='imagenet', include_top=False),\u001b[39m\n",
      "\u001b[32m+        L.GlobalAveragePooling2D(),\u001b[39m\n",
      "\u001b[32m+        L.Dense(train_labels.shape[1], activation='softmax')\u001b[39m\n",
      "\u001b[32m+    ])\u001b[39m\n",
      "\u001b[32m+    return model\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/web-traffic-time-series-forecasting/shemijacob/23033610.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-model = ARIMA(ts_log.values, order=(1,1,0))\u001b[39m\n",
      "\u001b[32m+model = ARIMA(metallica_log.values, order=(1,1,0))\u001b[39m\n",
      " results_ARIMA = model.fit(disp=-1)\n",
      " plt.plot( results_ARIMA.fittedvalues, color='red' )\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/moghazy/11767196.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-x_train, x_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.25, random_state=42 )\u001b[39m\n",
      "\u001b[32m+x_train, x_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.05, random_state=42 )\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/treybean/11294428.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    def __init__(self, individual_aggregator=None):\u001b[39m\n",
      "\u001b[32m+    def __init__(self, individual_aggregator=None, feature_creator=None):\u001b[39m\n",
      "         self.individual_aggregator = individual_aggregator\n",
      "\u001b[32m+        self.feature_creator = feature_creator\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/jazivxt/6662722.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "#https://www.kaggle.com/ashirahama/blending\n",
      "\u001b[31m-train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\u001b[39m\n",
      "\u001b[32m+train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index) \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/javiersotres/28277182.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-earlystop = EarlyStopping(patience=10)\u001b[39m\n",
      "\u001b[32m+earlystop = EarlyStopping(patience=5)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/sarthakbatra/14037886.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " def xgtrain(X_train, X_valid, y_train, y_valid):\n",
      "\u001b[31m-    regressor = XGBRegressor(n_estimators = 50000, \u001b[39m\n",
      "\u001b[31m-                                 learning_rate = 0.01,\u001b[39m\n",
      "\u001b[31m-                                 max_depth = 4, \u001b[39m\n",
      "\u001b[31m-                                 subsample = 0.3, \u001b[39m\n",
      "\u001b[31m-                                 colsample_bytree = 0.3\u001b[39m\n",
      "\u001b[31m-                                )\u001b[39m\n",
      "\u001b[31m-    regressor_ = regressor.fit(X_train.values, y_train.values, \u001b[39m\n",
      "\u001b[31m-                               eval_metric = 'rmse', \u001b[39m\n",
      "\u001b[31m-                               eval_set = [(X_train.values, y_train.values), \u001b[39m\n",
      "\u001b[31m-                                           (X_valid.values, y_valid.values)\u001b[39m\n",
      "\u001b[31m-                                          ], \u001b[39m\n",
      "\u001b[31m-                               verbose = 250,\u001b[39m\n",
      "\u001b[31m-                               early_stopping_rounds = 200,\u001b[39m\n",
      "\u001b[31m-                              )\u001b[39m\n",
      "\u001b[32m+    regressor = XGBRegressor(\u001b[39m\n",
      "\u001b[32m+        n_estimators = 50000, \u001b[39m\n",
      "\u001b[32m+        learning_rate = 0.01,\u001b[39m\n",
      "\u001b[32m+        max_depth = 4, \u001b[39m\n",
      "\u001b[32m+        subsample = 0.3, \u001b[39m\n",
      "\u001b[32m+        colsample_bytree = 0.1\u001b[39m\n",
      "\u001b[32m+        )\u001b[39m\n",
      "\u001b[32m+    regressor_ = regressor.fit(\u001b[39m\n",
      "\u001b[32m+        X_train.values, y_train.values, \u001b[39m\n",
      "\u001b[32m+        eval_metric = 'rmse', \u001b[39m\n",
      "\u001b[32m+        eval_set = [\u001b[39m\n",
      "\u001b[32m+            (X_train.values, y_train.values), \u001b[39m\n",
      "\u001b[32m+            (X_valid.values, y_valid.values)\u001b[39m\n",
      "\u001b[32m+        ],\u001b[39m\n",
      "\u001b[32m+        verbose = 250,\u001b[39m\n",
      "\u001b[32m+        early_stopping_rounds = 150,\u001b[39m\n",
      "\u001b[32m+        )\u001b[39m\n",
      "     return regressor_\n",
      "\u001b[34mdata/processed/competitions/human-protein-atlas-image-classification/mickeypvx/7192824.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-test_shape = (512, 512, 4)\u001b[39m\n",
      "\u001b[32m+def f1_metric(y_true, y_pred):\u001b[39m\n",
      "\u001b[32m+    y_pred = K.round(y_pred)\u001b[39m\n",
      "\u001b[32m+    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\u001b[39m\n",
      "\u001b[32m+    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\u001b[39m\n",
      "\u001b[32m+    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\u001b[39m\n",
      "\u001b[32m+    p = tp / (tp + fp + K.epsilon())\u001b[39m\n",
      "\u001b[32m+    r = tp / (tp + fn + K.epsilon())\u001b[39m\n",
      "\u001b[32m+    f1 = 2 * p * r / (p + r + K.epsilon())\u001b[39m\n",
      "\u001b[32m+    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\u001b[39m\n",
      "\u001b[32m+    return K.mean(f1)\u001b[39m\n",
      "\u001b[32m+test_shape = (256, 256, 4)\u001b[39m\n",
      " regs = regularizers.l1_l2(l1=0.001, l2=0.001)\n",
      " model.compile(loss='binary_crossentropy',\n",
      "              optimizer=opt,\n",
      "\u001b[31m-             metrics=['acc']) # Accuracy is basically meaningless for this problem, but we'll get to that.\u001b[39m\n",
      "\u001b[32m+             metrics=[f1_metric, 'acc'])\u001b[39m\n",
      " model.summary()\n",
      "\u001b[34mdata/processed/competitions/loan-default-prediction/felicitasbuchner/36468627.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-X = train_data2.drop('Loan_Status', 1)\u001b[39m\n",
      "\u001b[31m-Y = train_data2['Loan_Status']\u001b[39m\n",
      "\u001b[32m+X = train_data3.drop('Loan_Status', 1)\u001b[39m\n",
      "\u001b[32m+Y = train_data3['Loan_Status']\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/m5-forecasting-accuracy/chandrimad31/36965190.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  sales['rolling_sold_mean'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=8).mean()).astype(np.float16)\u001b[39m\n",
      "\u001b[32m+  sales['rolling_sold_mean'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=6).mean()).astype(np.float16)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/gpreda/10158852.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "for windows in [10, 100, 1000]:\n",
      "\u001b[31m-        x_roll_std = x.rolling(windows).std().dropna().values\u001b[39m\n",
      "\u001b[31m-        x_roll_mean = x.rolling(windows).mean().dropna().values\u001b[39m\n",
      "\u001b[32m+        x_roll_std = xc.rolling(windows).std().dropna().values\u001b[39m\n",
      "\u001b[32m+        x_roll_mean = xc.rolling(windows).mean().dropna().values\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/transfer-learning-on-stack-exchange-tags/chenroot/495971.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,2 @@\n",
      "\u001b[32m+'''\u001b[39m\n",
      " threshold = [i*0.1 for i in range(1, 16)]\n",
      " for t in threshold :\n",
      "     vectorizer = CountVectorizer(min_df = 1, stop_words = stop)\n",
      "\u001b[31m-    X = vectorizer.fit_transform(biology_df[title_content])\u001b[39m\n",
      "\u001b[32m+    X = vectorizer.fit_transform(train_df[title_content])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/timetoshow/18028940.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-temp = train.sample(frac=0.1,random_state = 11)\u001b[39m\n",
      "\u001b[32m+temp = train.sample(frac=1.0,random_state = 11)\u001b[39m\n",
      " X_train = temp[features].values\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/roydatascience/27407474.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack_NN,target)):\u001b[39m\n",
      "\u001b[32m+    for fold_, (trn_idx, val_idx) in enumerate(folds_stack_1.split(train_stack,target)):\u001b[39m\n",
      "         print(\"fold {}\".format(fold_))\n",
      "\u001b[31m-        trn_data, trn_y = train_stack_NN[trn_idx], target.iloc[trn_idx].values\u001b[39m\n",
      "\u001b[31m-        val_data, val_y = train_stack_NN[val_idx], target.iloc[val_idx].values\u001b[39m\n",
      "\u001b[32m+        trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\u001b[39m\n",
      "\u001b[32m+        val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/fmarazzi/7581528.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "X = np.stack(df[\"image\"].values)\n",
      " X = (X - X.mean()) / X.std()\n",
      "\u001b[31m-y = to_categorical(df[\"label\"])\u001b[39m\n",
      "\u001b[32m+y = np.stack(df[\"label\"].values)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/allunia/27496244.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " forest = RandomForestRegressor(max_depth=8)\n",
      " kfold = KFold(6)\n",
      "\u001b[31m-rfecv = RFECV(estimator=forest, step=1, scoring=\"rmse\", n_jobs=-1)\u001b[39m\n",
      "\u001b[32m+rfecv = RFECV(estimator=forest, step=1, scoring=\"neg_mean_squared_error\", n_jobs=-1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/generative-dog-images/tenffe/16778997.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        h2 = lrelu(tf.layers.conv2d(h1, kernel_size=5, filters=256, strides=2, padding='same'))\u001b[39m\n",
      "\u001b[32m+        h2 = tf.layers.conv2d(h1, kernel_size=5, filters=256, strides=2, padding='same')\u001b[39m\n",
      "\u001b[32m+        h2 = lrelu(tf.layers.batch_normalization(h2, training=is_training, momentum=momentum))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/instant-gratification/plasticgrammer/15563962.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    pca = PCA(n_components=40)\u001b[39m\n",
      "\u001b[31m-    train2 = pca.fit_transform(train2)\u001b[39m\n",
      "\u001b[31m-    test2 = pca.transform(test2)\u001b[39m\n",
      "\u001b[32m+    sel = VarianceThreshold(threshold=1.5)\u001b[39m\n",
      "\u001b[32m+    train2 = sel.fit_transform(train2)\u001b[39m\n",
      "\u001b[32m+    test2 = sel.transform(test2)    \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/instacart-market-basket-analysis/djuuuu/1198613.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def topk_items(user_ind = 100, k = 10, verbose = False):\n",
      "     i = user.product_id.values[0]\n",
      "\u001b[31m-    return i[np.argsort(l)[::-1][:k]]\u001b[39m\n",
      "\u001b[32m+    top_k_items = i[np.argsort(l)[::-1][:k]]\u001b[39m\n",
      "\u001b[32m+    return ' '.join([str(x) for x in top_k_items])\u001b[39m\n",
      " topk_items(100, 10)\n",
      "\u001b[34mdata/processed/competitions/sentiment-analysis-on-movie-reviews/abhideshmukh1/10916271.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pipe = Pipeline([(\"the cleaner\", predictors()), \u001b[39m\n",
      "\u001b[32m+pipe = Pipeline([ \u001b[39m\n",
      "                 (\"the vectorizer\", vectorizer),\n",
      "\u001b[31m-                (\"the classifier\", classifier)])\u001b[39m\n",
      "\u001b[32m+                (\"the classifier\", rfc),\u001b[39m\n",
      "\u001b[32m+                ])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/favorita-grocery-sales-forecasting/towever/1764406.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-ma_is = train[['item_nbr','store_nbr','unit_sales']].groupby(['item_nbr','store_nbr'])['unit_sales'].mean().to_frame('mais226')\u001b[39m\n",
      "\u001b[32m+tmp = train[['item_nbr','store_nbr','unit_sales']]\u001b[39m\n",
      "\u001b[32m+ma_is = tmp.groupby(['item_nbr', 'store_nbr'])['unit_sales'].mean().to_frame('mais226')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/movie-review-sentiment-analysis-kernels-only/hexadd5/10199495.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "def get_model():\n",
      "     main_input = layers.Input(shape=(max_len,))\n",
      "\u001b[31m-    embedded = layers.Embedding(max_features, 8, input_length=max_len)(main_input)\u001b[39m\n",
      "\u001b[31m-    flattened = layers.Flatten()(embedded)\u001b[39m\n",
      "\u001b[31m-    dense1 = layers.Dense(5, activation='softmax')(flattened)\u001b[39m\n",
      "\u001b[31m-    model = models.Model(inputs=main_input, outputs=dense1)\u001b[39m\n",
      "\u001b[32m+    embedded = layers.Embedding(max_features, 32)(main_input)\u001b[39m\n",
      "\u001b[32m+    rnn_output = layers.GRU(32)(embedded)\u001b[39m\n",
      "\u001b[32m+    dense1 = layers.Dense(32, activation='relu')(rnn_output)\u001b[39m\n",
      "\u001b[32m+    dense1 = layers.Dropout(0.3)(dense1)\u001b[39m\n",
      "\u001b[32m+    dense2 = layers.Dense(5, activation='softmax')(dense1)\u001b[39m\n",
      "\u001b[32m+    model = models.Model(inputs=main_input, outputs=dense2)\u001b[39m\n",
      "     model.compile(optimizer='rmsprop',\n",
      " def get_model():\n",
      " max_features = 10000\n",
      "\u001b[31m-max_len = 100\u001b[39m\n",
      "\u001b[32m+max_len = 200\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "for diff in consolidated_examples[100:200]:\n",
    "    display_kaggle_diff(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*ttest_ind\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/yairhadad1/12395801.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-cls_k_means = KMeans(n_clusters=2)\u001b[39m\n",
      "\u001b[32m+cls_k_means = KMeans(n_clusters=3)\u001b[39m\n",
      " cls_k_means.fit(k_mean_X)\n",
      "\u001b[34mdata/processed/competitions/sf-crime/nguyenbaopc/22348067.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=4)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=14)\u001b[39m\n",
      " kmeans.fit(df_input)\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/priyanka13/1441720.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=2, random_state=0).fit(xkdt)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=3, random_state=0).fit(xkdt)\u001b[39m\n",
      " test['dropoff_id']=kmeans.labels_\n",
      "\u001b[34mdata/processed/competitions/bosch-production-line-performance/egdman/379461.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters = 4)\u001b[39m\n",
      "\u001b[32m+num_clusters = 5\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters = num_clusters)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/finding-elo/keshavshetty/16467540.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  kmeans = KMeans(n_clusters=8, random_state=0).fit(x_train_pca)\u001b[39m\n",
      "\u001b[32m+  kmeans = KMeans(n_clusters=n_boats, random_state=0).fit(x_train_pca)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/bigquery-geotab-intersection-congestion/kabure/20687693.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-km = KMeans(n_clusters=4)\u001b[39m\n",
      "\u001b[32m+km = KMeans(n_clusters=4, random)\u001b[39m\n",
      " km = km.fit(df_train[min_max_cols])\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/plasticgrammer/19742799.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-km = KMeans(n_clusters=5, tol=1e-04, random_state=42)\u001b[39m\n",
      "\u001b[32m+import matplotlib.cm as cm\u001b[39m\n",
      "\u001b[32m+km = KMeans(n_clusters=7, tol=1e-04, random_state=42)\u001b[39m\n",
      " y_km = km.fit_predict(vcol_pca)\n",
      "\u001b[34mdata/processed/competitions/word2vec-nlp-tutorial/nareyko/6986394.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-from sklearn.cluster import KMeans\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=5)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=4)\u001b[39m\n",
      " kmeans.fit(X)\n",
      "\u001b[31m-y_kmeans = kmeans.predict(X)\u001b[39m\n",
      "\u001b[32m+time_news['cluster'] = kmeans.predict(X)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/peterad/7760654.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  clusters = KMeans(n_clusters=20, random_state=0).fit(train_df[['pickup_longitude','pickup_latitude']])\u001b[39m\n",
      "\u001b[32m+  clusters = MiniBatchKMeans(n_clusters=70, random_state=0, batch_size=10000, n_init=100).fit(train_df[:100000][['pickup_longitude','pickup_latitude']])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/bosch-production-line-performance/egdman/378970.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters = 3)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters = 4)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/the-nature-conservancy-fisheries-monitoring/tarang025/29051319.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  kmeans = KMeans(n_clusters=8, random_state=0).fit(x_train_pca)\u001b[39m\n",
      "\u001b[32m+  kmeans = KMeans(n_clusters=n_boats, random_state=0).fit(x_train_pca)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ashrae-energy-prediction/rheajgurung/8073666.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=10, max_iter=600, algorithm = 'auto')\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=3, max_iter=600, algorithm = 'auto')\u001b[39m\n",
      " kmeans.fit(weather_scaled)\n",
      "\u001b[34mdata/processed/competitions/ds4g-environmental-insights-explorer/meenakshiramaswamy/30577779.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans5 = KMeans(n_clusters=5)\u001b[39m\n",
      "\u001b[32m+kmeans5 = KMeans(n_clusters=10)\u001b[39m\n",
      " y_kmeans5 = kmeans5.fit_predict(X)\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/priyanka13/1435777.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=2, random_state=0).fit(xkp)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=3, random_state=0).fit(xkp)\u001b[39m\n",
      " ds['pickup_id']=kmeans.labels_\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/justjun0321/5955836.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = KMeans(n_clusters = 6)\u001b[39m\n",
      "\u001b[32m+model = KMeans(n_clusters = 8)\u001b[39m\n",
      " model.fit(train_test_geo)\n",
      "\u001b[34mdata/processed/competitions/trends-assessment-prediction/mikhaliaw/23400534.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-from sklearn.cluster import KMeans\u001b[39m\n",
      "\u001b[31m-K_Means = KMeans(n_clusters=5, init=\"k-means++\", n_init=10, max_iter=300) \u001b[39m\n",
      "\u001b[32m+K_Means = KMeans(n_clusters=5, init=\"k-means++\", n_init=10, max_iter=250) \u001b[39m\n",
      " pkm[\"cluster\"] = K_Means.fit_predict( pv )\n",
      "\u001b[34mdata/processed/competitions/cat-in-the-dat-ii/erelin6613/30070247.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    kmeans = KMeans(n_clusters=2)\u001b[39m\n",
      "\u001b[31m-    data = data.drop('id', axis=1)\u001b[39m\n",
      "\u001b[31m-    submission_data = submission_data.drop('id', axis=1)\u001b[39m\n",
      "\u001b[31m-    kmeans.fit(data.values)\u001b[39m\n",
      "\u001b[31m-    data_clusters = kmeans.transform(data.values)\u001b[39m\n",
      "\u001b[31m-    sub_data_clusters = kmeans.transform(submission_data.values)\u001b[39m\n",
      "\u001b[31m-    data['cluster_0'] = [x[0] for x in data_clusters]\u001b[39m\n",
      "\u001b[31m-    data['cluster_1'] = [x[1] for x in data_clusters]\u001b[39m\n",
      "\u001b[31m-    submission_data['cluster_0'] = [x[0] for x in sub_data_clusters]\u001b[39m\n",
      "\u001b[31m-    submission_data['cluster_1'] = [x[1] for x in sub_data_clusters]\u001b[39m\n",
      "\u001b[32m+    features = nom_cols+bin_cols+['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\u001b[39m\n",
      "\u001b[32m+    features = [i for i, col in enumerate(data.columns) if col in features]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/online-sales/msondkar/29626925.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clusterer = KMeans(n_clusters=3, random_state=5)\u001b[39m\n",
      "\u001b[32m+clusterer = KMeans(n_clusters=4, random_state=5)\u001b[39m\n",
      " cluster_labels = clusterer.fit_predict(RFM)\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-kernels-only/moghazy/4705628.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=17, random_state=0).fit(x)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=7, random_state=0).fit(x)\u001b[39m\n",
      " x[\"cluster\"] = kmeans.labels_\n",
      "\u001b[34mdata/processed/competitions/forest-cover-type-prediction/moghazy/4705628.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=17, random_state=0).fit(x)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=7, random_state=0).fit(x)\u001b[39m\n",
      " x[\"cluster\"] = kmeans.labels_\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2018/miklgr500/2680888.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=4).fit(X_tr)\u001b[39m\n",
      "\u001b[31m-trainDF['cluster'] = np.argmin(kmeans.transform(X_tr), -1)\u001b[39m\n",
      "\u001b[31m-testDF['clusetr'] = np.argmin(kmeans.transform(X_te), -1)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=2).fit(X_tr)\u001b[39m\n",
      "\u001b[32m+train_cl2 = np.argmin(kmeans.transform(X_tr), -1)\u001b[39m\n",
      "\u001b[32m+test_cl2 = np.argmin(kmeans.transform(X_te), -1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/plant-seedlings-classification/allunia/16723272.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-km = KMeans(n_clusters=3)\u001b[39m\n",
      "\u001b[32m+km = KMeans(n_clusters=5)\u001b[39m\n",
      " plantstate[\"growth_state\"] = km.fit_predict(X)\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/justjun0321/5984009.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = KMeans(n_clusters = 6)\u001b[39m\n",
      "\u001b[32m+model = KMeans(n_clusters = 8)\u001b[39m\n",
      " model.fit(train_test_geo)\n",
      "\u001b[34mdata/processed/competitions/otto-group-product-classification-challenge/ottoschnurr/20623902.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clusterer = KMeans(n_clusters=5)\u001b[39m\n",
      "\u001b[32m+clusterer = KMeans(n_clusters=5, random_state=1)\u001b[39m\n",
      " cluster_indices = pd.Series(\n",
      "\u001b[34mdata/processed/competitions/finding-elo/hadimahihenni/32087609.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clusterer = KMeans(n_clusters=7, random_state = 42)\u001b[39m\n",
      "\u001b[32m+clusterer = KMeans(n_clusters=6, random_state = 42)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/siim-isic-melanoma-classification/jeevats/36920719.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-km = KMeans(n_jobs=-1, n_clusters=10, n_init=20)\u001b[39m\n",
      "\u001b[32m+km = KMeans(n_jobs=-1, n_clusters=2, n_init=20)\u001b[39m\n",
      " km.fit(train_x)\n",
      "\u001b[34mdata/processed/competitions/instacart-market-basket-analysis/asindico/1208603.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clusterer = KMeans(n_clusters=4,random_state=0).fit(tocluster)\u001b[39m\n",
      "\u001b[32m+clusterer = KMeans(n_clusters=10,random_state=42).fit(tocluster)\u001b[39m\n",
      " centers = clusterer.cluster_centers_\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/priyanka13/1429944.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=5, random_state=0).fit(xkt)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=2, random_state=0).fit(xkt)\u001b[39m\n",
      " test['pickup_id']=kmeans.labels_\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/justjun0321/5969379.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = KMeans(n_clusters = 8)\u001b[39m\n",
      "\u001b[32m+model = KMeans(n_clusters = 6)\u001b[39m\n",
      " model.fit(train_test_geo)\n",
      "\u001b[34mdata/processed/competitions/bigquery-geotab-intersection-congestion/kabure/20739641.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-km = KMeans(n_clusters=4, random)\u001b[39m\n",
      "\u001b[32m+km = KMeans(n_clusters=4, random_state=4)\u001b[39m\n",
      " km = km.fit(df_train[min_max_cols])\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/peterad/7760654.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  clusters = KMeans(n_clusters=20, random_state=0).fit(train_df[['pickup_longitude','pickup_latitude']])\u001b[39m\n",
      "\u001b[32m+  clusters = MiniBatchKMeans(n_clusters=70, random_state=0, batch_size=10000, n_init=100).fit(train_df[:100000][['pickup_longitude','pickup_latitude']])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/bosch-production-line-performance/egdman/378970.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters = 5)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters = 3)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/justjun0321/5955836.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = KMeans(n_clusters = 6)\u001b[39m\n",
      "\u001b[32m+model = KMeans(n_clusters = 8)\u001b[39m\n",
      " model.fit(train_test_geo)\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/justjun0321/5969379.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = KMeans(n_clusters = 8)\u001b[39m\n",
      "\u001b[32m+model = KMeans(n_clusters = 6)\u001b[39m\n",
      " model.fit(train_test_geo)\n",
      "\u001b[34mdata/processed/competitions/sentiment-analysis-on-movie-reviews/kitakoj18/2361201.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    kmeans = KMeans(n_clusters = K, n_jobs = -1)\u001b[39m\n",
      "\u001b[32m+    kmeans = KMeans(n_clusters = K)\u001b[39m\n",
      "     kmeans.fit(X)\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/justjun0321/5997642.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = KMeans(n_clusters = 8)\u001b[39m\n",
      "\u001b[32m+model = KMeans(n_clusters = 6)\u001b[39m\n",
      " model.fit(train_test_geo)\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/priyanka13/1435777.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-kmeans = KMeans(n_clusters=2, random_state=0).fit(xkd)\u001b[39m\n",
      "\u001b[32m+kmeans = KMeans(n_clusters=3, random_state=0).fit(xkd)\u001b[39m\n",
      " ds['dropoff_id']=kmeans.labels_\n",
      "\u001b[34mdata/processed/competitions/cat-in-the-dat/erelin6613/30070247.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    kmeans = KMeans(n_clusters=2)\u001b[39m\n",
      "\u001b[31m-    data = data.drop('id', axis=1)\u001b[39m\n",
      "\u001b[31m-    submission_data = submission_data.drop('id', axis=1)\u001b[39m\n",
      "\u001b[31m-    kmeans.fit(data.values)\u001b[39m\n",
      "\u001b[31m-    data_clusters = kmeans.transform(data.values)\u001b[39m\n",
      "\u001b[31m-    sub_data_clusters = kmeans.transform(submission_data.values)\u001b[39m\n",
      "\u001b[31m-    data['cluster_0'] = [x[0] for x in data_clusters]\u001b[39m\n",
      "\u001b[31m-    data['cluster_1'] = [x[1] for x in data_clusters]\u001b[39m\n",
      "\u001b[31m-    submission_data['cluster_0'] = [x[0] for x in sub_data_clusters]\u001b[39m\n",
      "\u001b[31m-    submission_data['cluster_1'] = [x[1] for x in sub_data_clusters]\u001b[39m\n",
      "\u001b[32m+    features = nom_cols+bin_cols+['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\u001b[39m\n",
      "\u001b[32m+    features = [i for i, col in enumerate(data.columns) if col in features]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*KMeans\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/plasticgrammer/4889718.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-skewed = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\u001b[39m\n",
      "\u001b[32m+skewed = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\u001b[39m\n",
      " skewed_feats = skewed[skewed > 1].index\n",
      " print(skewed_feats)\n",
      "\u001b[34mdata/processed/competitions/titanic/headsortails/1151431.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-df = train.loc[:,tcols].dropna()\u001b[39m\n",
      "\u001b[32m+df = training.loc[:,tcols].dropna()\u001b[39m\n",
      " X = df.loc[:,cols]\n",
      "\u001b[34mdata/processed/competitions/ga-customer-revenue-prediction/danofer/5722315.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+    df.dropna(how=\"all\",axis=1,inplace=True)\u001b[39m\n",
      "     df.drop([c for c in list_single_value if c in df.columns], axis=1, inplace=True)\n",
      " def df_prep(file):\n",
      " # ### From : https://www.kaggle.com/mlisovyi/flatten-json-fields-smart-dump-data\n",
      "\u001b[31m-    df['trafficSource_isTrueDirect'] = (df['trafficSource_isTrueDirect'].fillna(False)).astype(bool)\u001b[39m\n",
      "\u001b[31m-    df['totals_bounces'] = (df['totals_bounces'].fillna(0)).astype(np.uint8)\u001b[39m\n",
      "\u001b[31m-    df['totals_newVisits'] = (df['totals_newVisits'].fillna(0).astype(np.uint8)) # has NaNs ?\u001b[39m\n",
      "\u001b[31m-    df['totals_pageviews'] = (df['totals_pageviews'].fillna(0).astype(np.uint16))\u001b[39m\n",
      "\u001b[32m+    df['trafficSource.isTrueDirect'] = (df['trafficSource.isTrueDirect'].fillna(False)).astype(bool)\u001b[39m\n",
      "\u001b[32m+    df['totals.bounces'] = df['totals.bounces'].fillna(0).astype(np.uint8\u001b[39m\n",
      "\u001b[32m+    df['totals.newVisits'] = df['totals.newVisits'].fillna(0).astype(np.uint8) # has NaNs ?\u001b[39m\n",
      "\u001b[32m+    df['totals.pageviews'] = df['totals.pageviews'].fillna(0).astype(np.uint16)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/march-madness-analytics-2020/kowjan1/33109896.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train = df.where(df['Season']<=2018).dropna().drop(columns=['HTeamID', 'ATeamID']).copy()\u001b[39m\n",
      "\u001b[32m+train = df[df['Season']<=2018].dropna().drop(columns=['HTeamID', 'ATeamID']).copy()\u001b[39m\n",
      " data_X_tr = train.drop(columns=['Thrill'])\n",
      " data_y_tr = train['Thrill']\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/inspi101/21347139.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  train.dropna(thresh=0.5*len(train),how ='all', axis=1, inplace = True)\u001b[39m\n",
      "\u001b[32m+  train.dropna(thresh = 0.5*len(train),how ='all', axis=1, inplace = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/naimul314/5658085.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-men_over45 = data_over45.where(male == 1).drop(cols_women, axis = 1).dropna()\u001b[39m\n",
      "\u001b[31m-women_over45 = data_over45.where(male == 0).dropna()\u001b[39m\n",
      "\u001b[32m+men_over45 = data_over45.where(dummied_data.male == 1).drop(cols_women, axis = 1).dropna()\u001b[39m\n",
      "\u001b[32m+women_over45 = data_over45.where(dummied_data.male == 0).dropna()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/home-credit-default-risk/datolp10/31592633.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-app_train = app_train.dropna() \u001b[39m\n",
      "\u001b[32m+app_train = app_train.dropna()\u001b[39m\n",
      " app_test = app_test.fillna(app_train.median())\n",
      "\u001b[34mdata/processed/competitions/titanic/hieu1344/15404740.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-X_train = train_df.drop('Survived', axis=1).dropna()\u001b[39m\n",
      "\u001b[31m-y_train = train_df.dropna()['Survived']\u001b[39m\n",
      "\u001b[32m+X_train = train_df.drop('Survived', axis=1)\u001b[39m\n",
      "\u001b[32m+y_train = train_df['Survived']\u001b[39m\n",
      " X_test = test_df\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9359103.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/petfinder-adoption-prediction/guilhermekodama/12103934.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-dataS = df[numericalFeaturesSentiment + categoricalFeaturesSentiment + [targetSentiment]].dropna()\u001b[39m\n",
      "\u001b[32m+dataS = train[numericalFeatures + categorical + [target]].dropna()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/nikkisharma536/8357328.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-test_data = test_data.dropna(thresh=0.70*len(test_data), axis=1)\u001b[39m\n",
      "\u001b[32m+test_data = test_data.dropna(thresh=0.80*len(test_data), axis=1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/jkokatjuhha/1797224.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-ages = np.concatenate((test['Age'].dropna(), train['Age'].dropna()), axis=0)\u001b[39m\n",
      "\u001b[32m+ages = train['Age'].dropna()\u001b[39m\n",
      " std_ages = ages.std()\n",
      "\u001b[34mdata/processed/competitions/titanic/jkokatjuhha/1797224.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+ax = sns.distplot(females[females['Survived'] == 1].Age.dropna(\u001b[39m\n",
      "\u001b[32m+), bins=30, label=sv_lab, ax=axes[0], kde=False)\u001b[39m\n",
      "\u001b[32m+ax = sns.distplot(females[females['Survived'] == 0].Age.dropna(\u001b[39m\n",
      "\u001b[32m+), bins=30, label=nsv_lab, ax=axes[0], kde=False)\u001b[39m\n",
      " ax.legend()\n",
      " ax.set_title('Female')\n",
      "\u001b[32m+                  bins=30, label=sv_lab, ax=axes[1], kde=False)\u001b[39m\n",
      "\u001b[32m+                  bins=30, label=nsv_lab, ax=axes[1], kde=False)\u001b[39m\n",
      " ax.legend()\n",
      "\u001b[31m-_ = ax.set_title('Male')\u001b[39m\n",
      "\u001b[32m+ax.set_title('Male');\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9295209.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/mgerdas/27746397.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-correlated_features2 = correlated_features[correlated_features != target].dropna(axis = 0).drop(columns = target)\u001b[39m\n",
      "\u001b[31m-correlated_features3 = correlated_features2.set_index('index').transpose()\u001b[39m\n",
      "\u001b[32m+corr_test_features = correlated_features[correlated_features != target].dropna(axis = 0).drop(columns = target)\u001b[39m\n",
      "\u001b[32m+corr_test_features = corr_test_features.set_index('index').transpose()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2018/vjbaskii/3597949.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+df_test = df_test.loc[:,tcols].dropna().reindex()\u001b[39m\n",
      " X_test = df_test.loc[:,cols]\n",
      " y_test = np.ravel(df_test.loc[:,['Survived']])\n",
      " print(X_test.shape)\n",
      "\u001b[31m-df_test.isnull().sum()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/bootiu/20124343.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-temp_df = _train.sample(frac=0.1).dropna()\u001b[39m\n",
      "\u001b[32m+temp_df = temp_df.sample(frac=0.1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/mdomarfaruque/32085924.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  test[test.Country_Region == 'US'].pivot('Province_State', 'Date', 'Fatalities').dropna(axis=1).sum() / 1000\u001b[39m\n",
      "\u001b[32m+  test[test.Country_Region == 'US'].pivot('Province_State', 'Date', 'Fatalities').dropna(axis=1).sum()[:29] / 1000\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/talkingdata-mobile-user-demographics/xieyufish/607159.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-d = devicelabels.dropna(subset=['trainrow'])\u001b[39m\n",
      "\u001b[31m-Xtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \u001b[39m\n",
      "\u001b[31m-                      shape=(gatrain.shape[0],nlabels))\u001b[39m\n",
      "\u001b[31m-d = devicelabels.dropna(subset=['testrow'])\u001b[39m\n",
      "\u001b[31m-Xte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \u001b[39m\n",
      "\u001b[31m-                      shape=(gatest.shape[0],nlabels))\u001b[39m\n",
      "\u001b[32m+nlabels = len(label_encoder.classes_) # 下面csr_matrix后面要加一个shape，不然可能由于中间函数筛选的原因使得大小不一致\u001b[39m\n",
      "\u001b[32m+d = device_labels.dropna(subset=['trainrow'])\u001b[39m\n",
      "\u001b[32m+Xtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), shape=(ga_train.shape[0],nlabels))\u001b[39m\n",
      "\u001b[32m+d = device_labels.dropna(subset=['testrow'])\u001b[39m\n",
      "\u001b[32m+Xte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), shape=(ga_test.shape[0],nlabels))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/talkingdata-mobile-user-demographics/anthonypino/1309549.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-means = dataframe[(dataframe[\"Type\"]==\"h\") & (dataframe[\"Distance\"]<13)].dropna().sort_values(\"Date\", ascending=False).groupby(\"Date\").mean()\u001b[39m\n",
      "\u001b[31m-errors = dataframe[(dataframe[\"Type\"]==\"h\") & (dataframe[\"Distance\"]<13)].dropna().sort_values(\"Date\", ascending=False).groupby(\"Date\").std()\u001b[39m\n",
      "\u001b[32m+means = dataframe[(dataframe[\"Type\"]==\"h\") & (dataframe[\"Distance\"]<13)].sort_values(\"Date\", ascending=False).groupby(\"Date\").mean()\u001b[39m\n",
      "\u001b[32m+errors = dataframe[(dataframe[\"Type\"]==\"h\") & (dataframe[\"Distance\"]<13)].sort_values(\"Date\", ascending=False).groupby(\"Date\").std()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/artgor/17399457.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\u001b[39m\n",
      "\u001b[31m-big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\u001b[39m\n",
      "\u001b[32m+big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.98]\u001b[39m\n",
      "\u001b[32m+big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.98]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/stephaniestallworth/1439042.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-test_dataset.dropna(inplace = True)\u001b[39m\n",
      "\u001b[31m-test_dataset.info()\u001b[39m\n",
      "\u001b[32m+test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\u001b[39m\n",
      "\u001b[32m+test_data.info()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/airbus-ship-detection/helibu/6507674.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-id_images_obj = traincsv.dropna().groupby('ImageId').count()\u001b[39m\n",
      "\u001b[32m+id_images_obj = train.dropna().groupby('ImageId').count()\u001b[39m\n",
      " id_images_obj.rename({'EncodedPixels': 'ObjCount'}, axis='columns', inplace=True)\n",
      "\u001b[34mdata/processed/competitions/loan-default-prediction/jacksonisaac/1142659.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clean_data = data.dropna(thresh=len(data)-200,axis=1)\u001b[39m\n",
      "\u001b[32m+clean_data = data.dropna(thresh=len(data),axis=1)\u001b[39m\n",
      " #clean_data.shape\n",
      "\u001b[34mdata/processed/competitions/microsoft-malware-prediction/adityaecdrid/8989514.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-top_10 = train['AvSigVersion_major'].value_counts(dropna=False, normalize=True).cumsum().index[:10]\u001b[39m\n",
      "\u001b[32m+top_10 = train['AvSigVersion_build'].value_counts(dropna=False, normalize=True).cumsum().index[:10]\u001b[39m\n",
      " train['magic_3'] = 0\n",
      " test['magic_3']  = 0\n",
      "\u001b[31m-train.loc[train['AvSigVersion_major'].isin(top_10) == True, 'magic_3'] = 1\u001b[39m\n",
      "\u001b[31m-test.loc[test['AvSigVersion_major'].isin(top_10) == True, 'magic_3']   = 1\u001b[39m\n",
      "\u001b[32m+train.loc[train['AvSigVersion_build'].isin(top_10) == True, 'magic_3'] = 1\u001b[39m\n",
      "\u001b[32m+test.loc[test['AvSigVersion_build'].isin(top_10) == True, 'magic_3']   = 1\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/neumatron11/11886691.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train_sample = train.dropna()\u001b[39m\n",
      "\u001b[31m-del train\u001b[39m\n",
      "\u001b[32m+train_sample['pickup_datetime'] = pd.to_datetime(train_sample.pickup_datetime, utc=True, format='%Y-%m-%d %H:%M') \u001b[39m\n",
      "\u001b[32m+test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime, utc=True,format='%Y-%m-%d %H:%M') \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/prratek/9118767.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-last_day.dropna(inplace=True)\u001b[39m\n",
      "\u001b[32m+last_day = last_day.dropna()\u001b[39m\n",
      " last_day.isna().sum()\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/smutishiktadas/31984085.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train2.dropna(inplace=True)\u001b[39m\n",
      "\u001b[31m-rolmean = train2['New'].rolling(window=5).mean()\u001b[39m\n",
      "\u001b[31m-rolstd = train2['New'].rolling(window =5).std()\u001b[39m\n",
      "\u001b[32m+rolmean = train2['Mean'].rolling(window=2).mean()\u001b[39m\n",
      "\u001b[32m+rolstd = train2['Mean'].rolling(window =2).std()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9359762.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/sberbank-russian-housing-market/dsmathon/1115350.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  df_train_filt = df.dropna(axis=1)\u001b[39m\n",
      "\u001b[32m+  df_train_filt = df_train.dropna(axis=1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/willkoehrsen/5142072.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  corrs.sort_values().dropna().tail()\u001b[39m\n",
      "\u001b[32m+  corrs['Target'].sort_values(ascending = True).dropna().tail(10)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/chimiro/27511253.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train = train.dropna(axis='rows')\u001b[39m\n",
      "\u001b[32m+origin_train = origin_train.dropna(axis='rows')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/headsortails/1080620.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-df = train.loc[:,['Survived','Sex','Pclass','SibSp','Parch','Fare']].dropna()\u001b[39m\n",
      "\u001b[32m+df = train.loc[:,['Survived','Sex','Pclass','SibSp','Parch']].dropna()\u001b[39m\n",
      " df[\"Sex\"] = df[\"Sex\"].astype(\"category\")\n",
      " df.head()\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/artgor/17591012.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.98]\u001b[39m\n",
      "\u001b[31m-big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.98]\u001b[39m\n",
      "\u001b[32m+big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\u001b[39m\n",
      "\u001b[32m+big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/facial-keypoints-detection/incals/21810694.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-drop_df = train_df.dropna(axis=0)\u001b[39m\n",
      "\u001b[32m+drop_df = train_df.dropna(axis=0).reset_index()\u001b[39m\n",
      " # len(drop_df)\n",
      "\u001b[34mdata/processed/competitions/titanic/trenzalore888/1028396.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  dfv = train['Embarked'].value_counts(dropna=True)\u001b[39m\n",
      "\u001b[32m+  Embarked_classes_count= train['Embarked'].value_counts(dropna=False)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/romainnervil/1776513.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-LPcheck = LP.dropna(how='all').copy()\u001b[39m\n",
      "\u001b[32m+LPcheck = LP.copy()\u001b[39m\n",
      " LPcheck['LearningPlatformSelect'] = choices['LearningPlatformSelect'].apply(\n",
      " LPcheck['NumberLP selected'] = LPcheck['LearningPlatformSelect'].apply(lambda x:\n",
      " LPcheck['LPrated'] = LPcheck[LPlist].count(axis=1)\n",
      "\u001b[31m-LPcheck['Rating missing'] = (LPcheck['NumberLP selected'] - LPcheck['LPrated']).replace({0:np.nan})\u001b[39m\n",
      "\u001b[32m+LPcheck['Missing'] = (LPcheck['NumberLP selected'] - LPcheck['LPrated']).replace({0:np.nan})\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/masumrumi/13974808.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-temp = multiple_choice_response.Q5.value_counts(dropna=False)\u001b[39m\n",
      "\u001b[32m+temp = response_2018.Q5.value_counts(dropna=False)\u001b[39m\n",
      " temp = temp[:len(temp)-1]\n",
      "\u001b[34mdata/processed/competitions/overfitting/overload10/7811657.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='jet',background_color='Cyan',mode='RGBA').generate(' '.join(df_trend_hash['TweetBody'].dropna().astype(str)))\u001b[39m\n",
      "\u001b[32m+wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='jet',background_color='Cyan',mode='RGBA').generate(' '.join(df_trend_tweets['TweetBody'].dropna().astype(str)))\u001b[39m\n",
      " plt.figure(figsize = (16,16))\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9279496.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/miura99/5454639.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  index = anl0.where(lambda x : abs(x) > 0.2).dropna().index[1:-1]\u001b[39m\n",
      "\u001b[32m+  index = anl0.where(lambda x : abs(x) > 0.25).dropna().index[1:-1]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2017/masumrumi/13974808.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-temp = multiple_choice_response.Q5.value_counts(dropna=False)\u001b[39m\n",
      "\u001b[32m+temp = response_2018.Q5.value_counts(dropna=False)\u001b[39m\n",
      " temp = temp[:len(temp)-1]\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/argentium/9097949.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-df_injury_moves_nostars = df_injury_moves_nostars.dropna()\u001b[39m\n",
      "\u001b[32m+df_injury_moves_return = df_injury_moves_return.dropna()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/google-ai-open-images-object-detection-track/ericfreeman/25617408.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    chunk = chunk.dropna().replace([np.inf,-np.inf],np.nan).fillna(0)  \u001b[39m\n",
      "\u001b[32m+    chunk = chunk.replace([np.inf,-np.inf],np.nan).fillna(0)  \u001b[39m\n",
      "     floaters = chunk.select_dtypes('float').columns.tolist()\n",
      "@@ -103,2 +103 @@ for chunk in tqdm(chunker, total = int(80_000_000/csize)):\n",
      "     chunk.to_parquet('InjuryRecord' + str(i) + '.parq')\n",
      "\u001b[31m-    i=i+1\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9359762.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/blaskowitz100/6184284.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-feature_skewness = all_data[NUMERIC_FEATURES].apply(lambda x: skew(x.dropna()))\u001b[39m\n",
      "\u001b[32m+feature_skewness = all_data[CONTINUOUS_FEATURES].apply(lambda x: skew(x.dropna()))\u001b[39m\n",
      " feature_skewness = feature_skewness.sort_values(ascending=False)\n",
      "\u001b[34mdata/processed/competitions/recognizing-faces-in-the-wild/caseyworks/14493942.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-final_train_df = final_train_df.dropna()\u001b[39m\n",
      "\u001b[32m+final_train_df = train_df.dropna()\u001b[39m\n",
      " final_train_df = final_train_df.reset_index()\n",
      " final_test_df = test_df\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/bellyn/24837404.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-airbnb_df[\"description\"] = airbnb_df[\"description\"].dropna().apply(filter_sentence)\u001b[39m\n",
      "\u001b[32m+airbnb_df[\"description\"] = airbnb_df[\"description\"].apply(filter_sentence)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/willkoehrsen/5196152.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  corrs.sort_values().dropna().tail()\u001b[39m\n",
      "\u001b[32m+  corrs['Target'].sort_values(ascending = True).dropna().tail(10)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/willkoehrsen/5145148.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  corrs.sort_values().dropna().tail()\u001b[39m\n",
      "\u001b[32m+  corrs['Target'].sort_values(ascending = True).dropna().tail(10)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/mcgovey/9154283.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-puntDF.dropna(subset=['prevX', 'prevY', 'prevSpeed'], inplace=True)\u001b[39m\n",
      "\u001b[32m+puntDF = puntDF.drop(columns=['prevX','prevY','prevSpeed'])\u001b[39m\n",
      " puntDF.head()\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/kowjan1/31277023.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'ConfirmedCases', test_size=0.3, dropna=True)\u001b[39m\n",
      "\u001b[32m+data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'ConfirmedCases', test_size=0.7, dropna=True)\u001b[39m\n",
      " # data_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/cevangelist/12416316.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    windowed_first = sig_first.rolling(100, win_type='slepian').mean(width=3).dropna()\u001b[39m\n",
      "\u001b[31m-    curr_df[\"window_100_mean_first\"] = [windowed_first.mean()]\u001b[39m\n",
      "\u001b[31m-    curr_df[\"window_100_std_first\"] = [windowed_first.std()]\u001b[39m\n",
      "\u001b[32m+    windowed_first = sig_first.rolling(win_width, win_type='slepian').mean(width=slepian_width).dropna()\u001b[39m\n",
      "\u001b[32m+    curr_df[f\"window_{win_width}_mean_first\"] = [windowed_first.mean()]\u001b[39m\n",
      "\u001b[32m+    curr_df[f\"window_{win_width}_std_first\"] = [windowed_first.std()]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/google-ai-open-images-object-detection-track/ericfreeman/25613288.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+    chunk = chunk.dropna().replace([np.inf,-np.inf],np.nan).fillna(0)  \u001b[39m\n",
      "     floaters = chunk.select_dtypes('float').columns.tolist()\n",
      "     chunk = mr.fit_transform(chunk, float_cols=floaters) #float downcast is optional\n",
      "\u001b[31m-    track_list.append(chunk)\u001b[39m\n",
      "\u001b[32m+    chunk = chunk[chunk['event']==1]\u001b[39m\n",
      "\u001b[32m+    chunk= chunk.drop(columns=['event'])\u001b[39m\n",
      "\u001b[32m+    chunk.to_parquet('InjuryRecord' + str(i) + '.parq')\u001b[39m\n",
      "\u001b[32m+    i=i+1\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/microsoft-malware-prediction/karanjakhar/10158029.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+train_data.dropna(inplace = True)\u001b[39m\n",
      " y_train = train_data[\"HasDetections\"]\n",
      "\u001b[31m-train_data.drop([\"HasDetections\"],axis = 1,inplace = True) \u001b[39m\n",
      "\u001b[32m+train_data.drop([\"HasDetections\"],axis = 1,inplace = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/m5-forecasting-accuracy/nictosi/34222218.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    ucdata = data.copy().dropna()\u001b[39m\n",
      "\u001b[32m+    ucdata['date'] = pd.to_datetime(ucdata.index)\u001b[39m\n",
      "\u001b[32m+    ucdata = ucdata.set_index('date')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/mdomarfaruque/32085924.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  test[test.Country_Region == 'US'].pivot('Province_State', 'Date', 'ConfirmedCases').dropna(axis=1).sum() / 1000\u001b[39m\n",
      "\u001b[32m+  test[test.Country_Region == 'US'].pivot('Province_State', 'Date', 'ConfirmedCases').dropna(axis=1).sum()[:29] / 1000\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/talkingdata-mobile-user-demographics/xieyufish/593029.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-d = devicelabels.dropna(subset=['trainrow'])\u001b[39m\n",
      "\u001b[31m-Xtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \u001b[39m\n",
      "\u001b[31m-                      shape=(gatrain.shape[0],nlabels))\u001b[39m\n",
      "\u001b[31m-d = devicelabels.dropna(subset=['testrow'])\u001b[39m\n",
      "\u001b[31m-Xte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \u001b[39m\n",
      "\u001b[31m-                      shape=(gatest.shape[0],nlabels))\u001b[39m\n",
      "\u001b[32m+nlabels = len(label_encoder.classes_) # 下面csr_matrix后面要加一个shape，不然可能由于中间函数筛选的原因使得大小不一致\u001b[39m\n",
      "\u001b[32m+d = device_labels.dropna(subset=['trainrow'])\u001b[39m\n",
      "\u001b[32m+Xtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), shape=(ga_train.shape[0],nlabels))\u001b[39m\n",
      "\u001b[32m+d = device_labels.dropna(subset=['testrow'])\u001b[39m\n",
      "\u001b[32m+Xte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), shape=(ga_test.shape[0],nlabels))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9279496.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/avito-demand-prediction/shibashis/3876484.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train_df1 = train_df1.dropna(how='any',axis=0) \u001b[39m\n",
      "\u001b[32m+train_df1 = train_df1.dropna(how='any',axis=0) \u001b[39m\n",
      "\u001b[32m+test_df1 = test_df1.dropna(how='any',axis=0) \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9295209.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[x.Event == 'punt'].seconds.mean()).dropna().to_frame(name = 'punt_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/xbcccr/8234197.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = df.dropna()\u001b[39m\n",
      "     # get label data\n",
      "     if train:\n",
      "\u001b[31m-        y = np.array(df.groupby(['matchId','groupId'])[LABEL].agg('mean'))\u001b[39m\n",
      "\u001b[32m+        df_y = df.groupby(['matchId','groupId'])[LABEL].agg('mean')\u001b[39m\n",
      "         ## now we can delete label and 'id' column to save GPU\n",
      " def FE(df,train=True):\n",
      "     lst_features = list(df_X.columns)\n",
      "\u001b[31m-    X = np.array(df_X)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ashrae-energy-prediction/claytonmiller/6564648.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  testdata = test_building_data[test_building_data.dropna().index.month.isin([\"4\",\"8\",\"12\"])]\u001b[39m\n",
      "\u001b[32m+  testdata = single_building_data[single_building_data.index.month.isin([\"4\",\"8\",\"12\"])]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/willkoehrsen/5152830.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  corrs.sort_values().dropna().tail()\u001b[39m\n",
      "\u001b[32m+  corrs['Target'].sort_values(ascending = True).dropna().tail(10)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/cevangelist/12416316.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    windowed_last = sig_last.rolling(100, win_type='slepian').mean(width=3).dropna()\u001b[39m\n",
      "\u001b[31m-    curr_df[\"window_100_mean_last\"] = [windowed_last.mean()]\u001b[39m\n",
      "\u001b[31m-    curr_df[\"window_100_std_last\"] = [windowed_last.std()]\u001b[39m\n",
      "\u001b[32m+    windowed_last = sig_last.rolling(win_width, win_type='slepian').mean(width=slepian_width).dropna()\u001b[39m\n",
      "\u001b[32m+    curr_df[f\"window_{win_width}_mean_last\"] = [windowed_last.mean()]\u001b[39m\n",
      "\u001b[32m+    curr_df[f\"window_{win_width}_std_last\"] = [windowed_last.std()]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/kowjan1/31218408.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'Fatalities', test_size=0.7, dropna=True)\u001b[39m\n",
      "\u001b[32m+data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'Fatalities', test_size=0.25, dropna=True)\u001b[39m\n",
      " # data_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/gihanu/1632377.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  all_data = all_data.dropna(how='any')\u001b[39m\n",
      "\u001b[32m+  all_datax = all_data.dropna(axis=1,how='any')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/prasadpagade/1241151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  data = train.select_dtypes(include=[np.number]).interpolate().dropna() \u001b[39m\n",
      "\u001b[32m+  data = train.select_dtypes(include=[np.number]).interpolate().dropna()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/rishabh254/5221159.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-train_df = train_df.dropna(how = 'any', axis = 'rows')\u001b[39m\n",
      "\u001b[32m+data = data.dropna(how = 'any', axis = 'rows')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/pavanraj159/4707159.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  index = anl0.where(lambda x : abs(x) > 0.25).dropna().index[1:-1]\u001b[39m\n",
      "\u001b[32m+  index = anl0.where(lambda x : abs(x) > 0.2).dropna().index[1:-1]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/malware-classification/karanjakhar/10158029.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+train_data.dropna(inplace = True)\u001b[39m\n",
      " y_train = train_data[\"HasDetections\"]\n",
      "\u001b[31m-train_data.drop([\"HasDetections\"],axis = 1,inplace = True) \u001b[39m\n",
      "\u001b[32m+train_data.drop([\"HasDetections\"],axis = 1,inplace = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/cjporteo/20732076.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-X.dropna(axis=0, subset=['SalePrice'], inplace=True)\u001b[39m\n",
      "\u001b[31m-'''missing_val_count_by_column = (X.isnull().sum())\u001b[39m\n",
      "\u001b[32m+X_train.dropna(axis=0, subset=['SalePrice'], inplace=True)\u001b[39m\n",
      "\u001b[32m+X_train = X_train[(X_train['SalePrice'] > 50000) & (X_train['SalePrice'] < 750000) & (X_train['LotArea'] < 110000) & (X_train['GrLivArea'] < 4000)]\u001b[39m\n",
      "\u001b[32m+y = np.log1p(X_train['SalePrice'])\u001b[39m\n",
      "\u001b[32m+X_train.drop(['SalePrice'], axis=1, inplace=True)\u001b[39m\n",
      "\u001b[32m+train_size = len(X_train)\u001b[39m\n",
      "\u001b[32m+'''def investigate_missing(df):\u001b[39m\n",
      "\u001b[32m+    missing_val_count_by_column = (df.isnull().sum())\u001b[39m\n",
      " '''\n",
      " def featureProcessing(df):\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/miura99/5454581.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  index = anl0.where(lambda x : abs(x) > 0.15).dropna().index[1:-1]\u001b[39m\n",
      "\u001b[32m+  index = anl0.where(lambda x : abs(x) > 0.2).dropna().index[1:-1]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/kowjan1/31218408.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'ConfirmedCases', test_size=0.7, dropna=True)\u001b[39m\n",
      "\u001b[32m+data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'ConfirmedCases', test_size=0.3, dropna=True)\u001b[39m\n",
      " # data_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n",
      "\u001b[34mdata/processed/competitions/ashrae-energy-prediction/claytonmiller/6564374.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  trainingdata = test_building_data[test_building_data.dropna().index.month.isin([\"1\",\"2\",\"3\",\"5\",\"6\",\"7\",\"9\",\"10\",\"11\"])]\u001b[39m\n",
      "\u001b[32m+  trainingdata = test_building_data[test_building_data.index.month.isin([\"1\",\"2\",\"3\",\"5\",\"6\",\"7\",\"9\",\"10\",\"11\"])]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/lesmedgutierrez/3393321.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-tenure = mcr.Tenure.dropna().value_counts().to_frame()\u001b[39m\n",
      "\u001b[32m+ParentsEducation = mcr.ParentsEducation.dropna().value_counts(ascending=True).to_frame()\u001b[39m\n",
      " f, ax = plt.subplots(1,1, figsize=(10, 8))\n",
      " plt.show()\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/danofer/17548462.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-df_all = pd.concat([X_train,X_test]).drop([\"TransactionDT\"],axis=1).dropna(axis=1)\u001b[39m\n",
      "\u001b[32m+df_all = pd.concat([X_train.dropna(axis=1),X_test.dropna(axis=1)]).drop([\"TransactionDT\"],axis=1).dropna(axis=1)\u001b[39m\n",
      " TR_ROWS = X_train.shape[0]\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/crained/8520872.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  conPlayer = conPlayer.dropna(subset=['GSISID']).drop_duplicates(['GSISID'])\u001b[39m\n",
      "\u001b[32m+  conPlayer = conPlayer.dropna(subset=['PlayID']).drop_duplicates(['PlayID','GSISID'])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/subinium/23420997.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-fig = px.histogram(data.dropna(), x='Q15', y='Q15', color='Q19')\u001b[39m\n",
      "\u001b[32m+fig = px.histogram(data.dropna(), x='Q15', y='Q15', color='Q19', template='ggplot2')\u001b[39m\n",
      " fig.update_layout()\n",
      "\u001b[34mdata/processed/competitions/talkingdata-adtracking-fraud-detection/atashnezhad/22541569.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  new_df_train = df_train.dropna(axis = 0, how ='any') \u001b[39m\n",
      "\u001b[32m+  '''new_df_train = df_train.dropna(axis = 0, how ='any') '''\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-question-pairs/defeater/1167290.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  df_test = DataFrame.from_csv('../input/test.csv').dropna()\u001b[39m\n",
      "\u001b[32m+  df_train = DataFrame.from_csv('../input/train.csv').dropna()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/rsna-pneumonia-detection-challenge/thomasjpfan/5441733.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-areas = (tr.dropna(subset=['width'])\u001b[39m\n",
      "\u001b[31m-           .assign(area=tr.width*tr.height))\u001b[39m\n",
      "\u001b[32m+areas = tr.dropna(subset=['area'])\u001b[39m\n",
      " g = sns.FacetGrid(hue='gender', data=areas, height=9, palette=dict(F=\"red\", M=\"blue\"), aspect=1.4)\n",
      "\u001b[34mdata/processed/competitions/m5-forecasting-accuracy/harupy/32304987.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-X_train, X_valid = train_test_split(sales.drop(drop_cols, axis=1).dropna(), test_size=0.1, random_state=42)\u001b[39m\n",
      "\u001b[32m+split_params = {\u001b[39m\n",
      "\u001b[32m+    \"test_size\": 0.1,\u001b[39m\n",
      "\u001b[32m+    \"random_state\": 42,\u001b[39m\n",
      "\u001b[32m+}\u001b[39m\n",
      "\u001b[32m+X_train, X_valid = train_test_split(sales.drop(drop_cols, axis=1).dropna(), **split_params)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/march-madness-analytics-2020/kowjan1/33109896.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-test = df.where(df['Season']>=2019).dropna().drop(columns=['HTeamID', 'ATeamID']).copy()\u001b[39m\n",
      "\u001b[32m+test = df[df['Season']>=2019].dropna().drop(columns=['HTeamID', 'ATeamID']).copy()\u001b[39m\n",
      " data_X_test = test.drop(columns=['Thrill'])\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/amneves/23364674.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-X.dropna(axis=0, subset=['SalePrice'], inplace=True)\u001b[39m\n",
      "\u001b[31m-y = X.SalePrice              \u001b[39m\n",
      "\u001b[31m-X.drop(['SalePrice'], axis=1, inplace=True)\u001b[39m\n",
      "\u001b[32m+y = df.SalePrice              \u001b[39m\n",
      "\u001b[32m+X = df.drop(['SalePrice'], axis=1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/kowjan1/31277023.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'Fatalities', test_size=0.25, dropna=True)\u001b[39m\n",
      "\u001b[32m+data_X_tr, data_X_test, data_y_tr, data_y_test = prepare_data(df_pop, 'Fatalities', test_size=0.7, dropna=True)\u001b[39m\n",
      " # data_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/anand0427/6174250.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  X = X.dropna(axis=1)\u001b[39m\n",
      "\u001b[32m+  X = X.dropna(axis=1).dropna()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/bootiu/20124343.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+temp_df = _train.sample(frac=0.5).dropna()\u001b[39m\n",
      " pca = PCA(n_components=2)\n",
      "\u001b[31m-a = pca.fit_transform(_train[col_list])\u001b[39m\n",
      "\u001b[32m+a = pca.fit_transform(temp_df[col_list])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/vhrique/2168059.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-skewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\u001b[39m\n",
      "\u001b[32m+skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\u001b[39m\n",
      " skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
      "\u001b[34mdata/processed/competitions/overfitting/overload10/7811657.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='jet',background_color='Cyan',mode='RGBA').generate(' '.join(df_trend_user['TweetBody'].dropna().astype(str)))\u001b[39m\n",
      "\u001b[32m+wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='jet',background_color='Cyan',mode='RGBA').generate(' '.join(df_trend_hash['TweetBody'].dropna().astype(str)))\u001b[39m\n",
      " plt.figure(figsize = (16,16))\n",
      "\u001b[34mdata/processed/competitions/data-science-london-scikit-learn/deanfoulds/32002591.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  location_nan_clean = df.dropna(axis=0, subset=['LSOA name'])\u001b[39m\n",
      "\u001b[32m+  location_nan_clean = data_full.dropna(axis=0, subset=['LSOA name'])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/willkoehrsen/5122128.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  corrs.sort_values().dropna().tail()\u001b[39m\n",
      "\u001b[32m+  corrs['Target'].sort_values(ascending = True).dropna().tail(10)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/kaggle-survey-2019/subinium/23420997.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-fig = px.histogram(data.dropna(), x='Q23', y='Q23', color='Q19')\u001b[39m\n",
      "\u001b[32m+fig = px.histogram(data.dropna(), x='Q23', y='Q23', color='Q19', template='ggplot2')\u001b[39m\n",
      " fig.update_layout()\n",
      "\u001b[34mdata/processed/competitions/loan-default-prediction/jacksonisaac/1139327.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  clean_data = data.dropna(thresh = len(data) - 200, axis=1)\u001b[39m\n",
      "\u001b[32m+  clean_data = data.dropna(thresh=len(data)-200,axis=1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/chimiro/28410056.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-origin_train = origin_train.dropna(axis='rows')\u001b[39m\n",
      "\u001b[32m+train = train.dropna(axis='rows')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2017/dglvalerio/1345319.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-loc_list = Counter(df['ocorrencia_uf'].dropna()).most_common()\u001b[39m\n",
      "\u001b[32m+loc_list = Counter(df['ocorrencia_classificacao'].dropna()).most_common()\u001b[39m\n",
      " locs = []\n",
      "\u001b[34mdata/processed/competitions/NFL-Punt-Analytics-Competition/tombliss/9359103.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'),  on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[32m+    df = pd.merge(df, df.groupby('GamePlayKey').apply(lambda x: x.loc[(x.Event == 'fair_catch') | (x.Event == 'tackle') | (x.Event == 'safety') | (x.Event == 'touchback') | (x.Event == 'out_of_bounds') | (x.Event == 'punt_downed') | (x.Event == 'touchdown') | (x.Event == 'fumble_defense_recovered') | (x.Event == 'pass_outcome_incomplete')].seconds.min()).dropna().to_frame(name = 'play_end_seconds'), on = ['GamePlayKey'], how ='outer')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/airbus-ship-detection/helibu/6507674.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-obj_pixels = traincsv.dropna().EncodedPixels.map(lambda x: object_pixels(x))\u001b[39m\n",
      "\u001b[32m+obj_pixels = train.dropna().EncodedPixels.map(lambda x: object_pixels(x))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/piyras23/6263959.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-trace=go.Scatter(x = dftrain[\"Age\"].dropna(), y=dftrain[\"Fare\"], mode = \"markers\", \u001b[39m\n",
      "\u001b[31m-                marker = dict(size = 6, color = \"aqua\"))\u001b[39m\n",
      "\u001b[32m+trace=go.Scatter(x = df[\"Age\"].dropna(), y=df[\"Fare\"], mode = \"markers\", \u001b[39m\n",
      "\u001b[32m+                marker = dict(size = 6, color = \"green\"))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/headsortails/1151431.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-df = train.loc[:,tcols].dropna()\u001b[39m\n",
      "\u001b[32m+df = training.loc[:,tcols].dropna()\u001b[39m\n",
      " X = df.loc[:,cols]\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*dropna\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/36358298.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.13)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/cevangelist/12729100.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    drop_0 = L.SpatialDropout1D(0.2)(emb)\u001b[39m\n",
      "\u001b[32m+    drop_0 = L.Dropout(0.5, seed=42)(emb)\u001b[39m\n",
      "\u001b[32m+    bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS, return_sequences=False))(drop_0)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/mak4alex/21546456.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    con_drop_layer1 = Dropout(0.25)(relu_layer2)\u001b[39m\n",
      "\u001b[31m-    max_pool_layer1 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(con_drop_layer1)\u001b[39m\n",
      "\u001b[31m-    conv_layer2 = Conv2D(32, kernel_size=(3,3), strides=1, padding='same')(max_pool_layer1)    \u001b[39m\n",
      "\u001b[31m-    conv_layer2 = Conv2D(32, kernel_size=(3,3), strides=1, padding='same')(conv_layer2)\u001b[39m\n",
      "\u001b[31m-    batch_norm_layer4 = BatchNormalization(momentum=0.5, gamma_initializer='uniform')(conv_layer2)\u001b[39m\n",
      "\u001b[31m-    relu_layer4 = ReLU()(batch_norm_layer4)\u001b[39m\n",
      "\u001b[32m+    max_pool_layer1 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(relu_layer1)\u001b[39m\n",
      "\u001b[32m+    con_drop_layer1 = Dropout(0.5)(max_pool_layer1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/gustafsilva/14844567.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.35)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.25)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/kernelgenerator/32948738.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p = 0.2)   # Regularization\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p = 0.1)   # Regularization\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats/ichrnkv/33219028.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout1 =  torch.nn.Dropout(p=0.5)\u001b[39m\n",
      "\u001b[31m-        self.dropout2 =  torch.nn.Dropout(p=0.2)\u001b[39m\n",
      "\u001b[32m+        self.dropout50 =  torch.nn.Dropout(p=0.5)\u001b[39m\n",
      "\u001b[32m+        self.dropout25 =  torch.nn.Dropout(p=0.25)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/cifar-10/aniruddhkb/32986151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X = keras.layers.Dropout(rate = 0.5)(X)\u001b[39m\n",
      "\u001b[32m+    X = keras.layers.Dropout(rate = dropout_rate)(X)\u001b[39m\n",
      "     X = keras.layers.Conv2D(filters = F3, kernel_size = 1, strides = 1, padding = \"valid\", data_format = \"channels_last\")(X)\n",
      " def convolutional_block(X, filters, stride):\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/kernelgenerator/34106315.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p = 0.01)   # Regularization\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p = 0.1)   # Regularization\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/35241308.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.LeakyReLU()(x1)\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dense(1)(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(1,1)(x1)\u001b[39m\n",
      "     x1 = tf.keras.layers.Flatten()(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/8765373.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    d0 = Dropout(0.25)(maxpool_0)\u001b[39m\n",
      "\u001b[32m+    d0 = Dropout(0.25)(maxpool_1)\u001b[39m\n",
      "     d0 = Dense(8)(d0)\n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/mak4alex/21546456.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    con_drop_layer3 = Dropout(0.25)(relu_layer6)\u001b[39m\n",
      "\u001b[31m-    max_pool_layer3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(con_drop_layer3)\u001b[39m\n",
      "\u001b[32m+    max_pool_layer3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(relu_layer3)\u001b[39m\n",
      "     flatten = Flatten()(max_pool_layer3)  \n",
      "\u001b[31m-    dropout_layer1 = Dropout(0.25)(flatten)\u001b[39m\n",
      "\u001b[32m+    dropout_layer1 = Dropout(0.5)(flatten)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/cifar-10/aniruddhkb/32986151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X = keras.layers.Dropout(rate = 0.5)(X)\u001b[39m\n",
      "\u001b[32m+    X = keras.layers.Dropout(rate = dropout_rate)(X)\u001b[39m\n",
      "     X = keras.layers.Conv2D(filters = F1, kernel_size = 1, strides = stride, padding = \"valid\", data_format = \"channels_last\")(X)\n",
      " def convolutional_block(X, filters, stride):\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/volody/35597979.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.5)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p_drop)\u001b[39m\n",
      "         self.conv = nn.Conv1d(96, config.hidden_size, 2) # padding='same'\n",
      "         self.conv2 = nn.Conv1d(config.hidden_size, 64, kernel_size = 2) # padding='same'\n",
      "\u001b[32m+        self.dropout2 = nn.Dropout(p_drop)\u001b[39m\n",
      "         self.fc = nn.Linear(766, 2)\n",
      " class TweetModel(nn.Module):\n",
      "         _, _, hs = self.roberta(input_ids, attention_mask)\n",
      "         x = torch.stack([hs[-1], hs[-2], hs[-3]]) \n",
      "         x = torch.mean(x, 0)                      \n",
      "\u001b[31m-        x = self.dropout(x)                       \u001b[39m\n",
      "         x = F.leaky_relu(self.conv(x))                  \n",
      "\u001b[32m+        x = self.dropout(x)                       \u001b[39m\n",
      "         x = self.conv2(x)                  \n",
      "\u001b[32m+        x = self.dropout2(x)                       \u001b[39m\n",
      "         x = self.fc(x)\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/cevangelist/13157914.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    drop_0 = L.SpatialDropout1D(rate=0.2)(emb)\u001b[39m\n",
      "\u001b[32m+    drop_0 = L.SpatialDropout1D(rate=0.1)(emb)\u001b[39m\n",
      "     bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS,\n",
      "\u001b[34mdata/processed/competitions/siim-acr-pneumothorax-segmentation/meaninglesslives/17180595.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    uconv0 = Dropout(0.1/2)(uconv0)\u001b[39m\n",
      "\u001b[32m+    uconv0 = Dropout(dropout_rate/2)(uconv0)\u001b[39m\n",
      "     output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/deepakd14/35959271.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.15)(x[0])\u001b[39m\n",
      "     x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/9509452.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    embedding = SpatialDropout2D(0.15)(embedding)\u001b[39m\n",
      "\u001b[32m+    embedding = SpatialDropout1D(0.15)(embedding)\u001b[39m\n",
      "     lstm = Bidirectional(CuDNNGRU(64, return_sequences=True))(embedding)\n",
      "\u001b[31m-    lstm = SpatialDropout2D(0.15)(lstm)\u001b[39m\n",
      "\u001b[32m+    lstm = SpatialDropout1D(0.15)(lstm)\u001b[39m\n",
      "     lstm = Bidirectional(CuDNNGRU(32, return_sequences=True))(lstm)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/mohannksr/32728103.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.3)(x[0]) \u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(256, 2, padding='same')(x2)\u001b[39m\n",
      "     x2 = tf.keras.layers.LeakyReLU()(x2)\n",
      "\u001b[31m-    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\u001b[39m\n",
      "     x2 = tf.keras.layers.Dense(1)(x2)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/gendered-pronoun-resolution/keyit92/11058508.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+    feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\u001b[39m\n",
      "     feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n",
      "\u001b[31m-    xextrs = [feature_map_layer(xextr) for xextr in xextrs]\u001b[39m\n",
      "\u001b[32m+    xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/rsmits/35590815.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.25)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.30)(x[0]) \u001b[39m\n",
      "     x1 = tf.keras.layers.Activation('softmax')(x1)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/leostep/8267116.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p=0.05)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p=0.1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/2345620.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-def Dropout(x, keep_prop):\u001b[39m\n",
      "\u001b[31m-    mask = np.random.binomial([np.ones_like(x)],(1-keep_prop))[0]  / (1-keep_prop)\u001b[39m\n",
      "\u001b[32m+def Dropout(x, dropout_percent):\u001b[39m\n",
      "\u001b[32m+    mask = np.random.binomial([np.ones_like(x)],(1-dropout_percent))[0]  / (1-dropout_percent)\u001b[39m\n",
      "     return x*mask\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/sklasfeld/16318410.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(.2)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(.5)\u001b[39m\n",
      "         self.linear_out = nn.Linear(dense_hidden_units, 1)\n",
      " class NeuralNet(nn.Module):\n",
      "         # global average pooling\n",
      "\u001b[31m-        avg_pool_2 = weighted_avg(h_lstm2,1)\u001b[39m\n",
      "\u001b[32m+        avg_pool_1 = torch.mean(h_lstm2, 1)\u001b[39m\n",
      "         # global max pooling\n",
      "\u001b[34mdata/processed/competitions/vsb-power-line-fault-detection/miklgr500/9062572.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.25)(x)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.45)(x)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/siim-acr-pneumothorax-segmentation/meaninglesslives/17561056.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    uconv0 = Dropout(0.1/2)(uconv0)\u001b[39m\n",
      "\u001b[32m+    uconv0 = Dropout(dropout_rate/2)(uconv0)\u001b[39m\n",
      "     output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/35283965.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.17)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/gustafsilva/14685844.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p=0.2)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p=0.1680)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/artgor/10035804.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.embedding_dropout = nn.Dropout2d(0.05)\u001b[39m\n",
      "\u001b[32m+        self.embedding_dropout = nn.Dropout2d(0.1)\u001b[39m\n",
      "         self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
      " class NeuralNet(nn.Module):\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/deepakd14/36106123.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/8766290.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    d0 = Dropout(0.25)(maxpool_1)\u001b[39m\n",
      "\u001b[31m-    d0 = Dense(8)(d0)\u001b[39m\n",
      "\u001b[32m+    d0 = Dropout(0.1)(maxpool_1)\u001b[39m\n",
      "\u001b[32m+    d0 = Dense(20)(d0)\u001b[39m\n",
      "\u001b[32m+    d0 = Dense(5)(d0)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/sagar7390/36110902.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Conv1D(1,1)(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.ReLU()(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dense(1)(x1)\u001b[39m\n",
      "     x1 = tf.keras.layers.Flatten()(x1)\n",
      " def build_model():\n",
      "     x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
      "\u001b[31m-    x2 = tf.keras.layers.Conv1D(1,1)(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.ReLU()(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dense(1)(x2)\u001b[39m\n",
      "     x2 = tf.keras.layers.Flatten()(x2)\n",
      "     x2 = tf.keras.layers.Activation('softmax')(x2)\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/9744151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    embedding = SpatialDropout1D(0.15)(embedding)\u001b[39m\n",
      "\u001b[32m+    embedding = SpatialDropout1D(0.2)(embedding)\u001b[39m\n",
      "     lstm = Bidirectional(CuDNNGRU(64, return_sequences=True))(embedding)\n",
      "\u001b[31m-    lstm = SpatialDropout1D(0.15)(lstm)\u001b[39m\n",
      "\u001b[32m+    lstm = SpatialDropout1D(0.2)(lstm)\u001b[39m\n",
      "     lstm = Bidirectional(CuDNNGRU(32, return_sequences=True))(lstm)\n",
      "     a = Attention(MAX_SEQUENCE_LENGTH)(lstm)\n",
      "\u001b[32m+    a = BatchNormalization()(a)\u001b[39m\n",
      "     d1 = Dense(32)(a)\n",
      "\u001b[32m+    d1 = BatchNormalization()(d1)\u001b[39m\n",
      "     d1 = Dropout(0.15)(d1)\n",
      "\u001b[31m-    b = BatchNormalization()(d1)\u001b[39m\n",
      "\u001b[31m-    out = Dense(1, activation='sigmoid')(b)\u001b[39m\n",
      "\u001b[32m+    out = Dense(1, activation='sigmoid')(d1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/leostep/8265662.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p=0.1)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p=0.2)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/whale-detection-challenge/artgor/7982514.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.8)\u001b[39m\n",
      "\u001b[31m-        self.conv2_bn = nn.BatchNorm2d(32)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.5)        \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/dimitreoliveira/30970076.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.3)(cls_token)\u001b[39m\n",
      "\u001b[32m+    x = GlobalAveragePooling1D()(sequence_output)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.2)(x)\u001b[39m\n",
      "     output = Dense(1, activation='sigmoid', name='output')(x)\n",
      "\u001b[34mdata/processed/competitions/cifar-10/aniruddhkb/32986151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X_shortcut = keras.layers.Dropout(rate = 0.5)(X_shortcut)\u001b[39m\n",
      "\u001b[32m+    X_shortcut = keras.layers.Dropout(rate = dropout_rate)(X_shortcut)\u001b[39m\n",
      "     X_shortcut = keras.layers.Conv2D(filters = F3, kernel_size = 1, strides = stride, padding = \"valid\", data_format = \"channels_last\")(X_shortcut)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/cwthompson/35534842.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
      "     x2 = tf.keras.layers.LeakyReLU()(x2)\n",
      "\u001b[31m-    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\u001b[39m\n",
      "     x2 = tf.keras.layers.Dense(1)(x2)\n",
      " def build_model():\n",
      "     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
      "\u001b[31m-    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \u001b[39m\n",
      "\u001b[32m+    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) \u001b[39m\n",
      "     model.compile(loss=loss_fn, optimizer=optimizer)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/35283965.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.17)(x[0]) \u001b[39m\n",
      "     x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/nikhilroxtomar/7319923.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.3)(out2)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.3)(out)\u001b[39m\n",
      "     outp = Dense(1, activation=\"sigmoid\")(x)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/35390290.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.17)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.13)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/myh0307/35695260.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Conv1D(16,1, padding='same')(x1)\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Conv1D(16,1, padding='same')(x1)\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Activation('relu')(x1)\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Dropout(0.1)(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Dropout(0.15)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Conv1D(1024,2, padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.LeakyReLU()(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Conv1D(512,2, padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.LeakyReLU()(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Conv1D(64,1)(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.LeakyReLU()(x1)\u001b[39m\n",
      "     x1 =  tf.keras.layers.Conv1D(1,1)(x1)\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Activation('relu')(x1)\u001b[39m\n",
      "     x1 =  tf.keras.layers.Flatten()(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/johnfarrell/5700709.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.5)(final_state)\u001b[39m\n",
      "\u001b[31m-    outputs = Dense(1, activation='sigmoid')(x)\u001b[39m\n",
      "\u001b[32m+    final_state = BatchNormalization()(final_state)\u001b[39m\n",
      "\u001b[32m+    outputs = Dense(1, activation='sigmoid')(final_state)\u001b[39m\n",
      "     model = Model(inputs=input_x, outputs=outputs)\n",
      " def get_model(n_final_state, lr=1e-3, decay=1e-8):\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/ryches/18354887.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(1.0)(cat_emb)) for cat_emb in categorical_embeddings])\u001b[39m\n",
      "\u001b[32m+    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.2)(cat_emb)) for cat_emb in categorical_embeddings])\u001b[39m\n",
      " #     categorical_logits = Dropout(.5)(categorical_logits)\n",
      " def make_model():\n",
      "     numerical_inputs = Input(shape=[tr_df[numerical].shape[1]], name = 'numerical')\n",
      "\u001b[31m-    numerical_logits = Dropout(.5)(numerical_inputs)\u001b[39m\n",
      "\u001b[32m+    numerical_logits = Dropout(.2)(numerical_inputs)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/nikhilroxtomar/7311014.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.3)(conc)\u001b[39m\n",
      "\u001b[31m-    x = Dense(4096, activation='relu')(x)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.3)(out2)\u001b[39m\n",
      "     outp = Dense(1, activation=\"sigmoid\")(x)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/kernelgenerator/33571559.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p = 0.1)   # Regularization\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p = 0.01)   # Regularization\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/parthplc/30872039.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/34841462.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Conv1D(1,1)(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.LeakyReLU()(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dense(1)(x1)\u001b[39m\n",
      "     x1 = tf.keras.layers.Flatten()(x1)\n",
      " def build_model():\n",
      "     x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
      "\u001b[31m-    x2 = tf.keras.layers.Conv1D(1,1)(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.LeakyReLU()(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dense(1)(x2)\u001b[39m\n",
      "     x2 = tf.keras.layers.Flatten()(x2)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/myh0307/36130098.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Dropout(0.15)(x[0])\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Conv1D(768,2, padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Conv1D(768,1, padding='same')(x1)\u001b[39m\n",
      "     x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Conv1D(256,2, padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Conv1D(256,1, padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 =  tf.keras.layers.Conv1D(16,1, padding='same')(x1)\u001b[39m\n",
      "     x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Conv1D(96,2, padding='same')(x1)\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.LeakyReLU()(x1)\u001b[39m\n",
      "\u001b[31m-    x1 =  tf.keras.layers.Dropout(0.15)(x1)\u001b[39m\n",
      "     x1 =  tf.keras.layers.Dense(1)(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/artgor/9976522.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.embedding_dropout = nn.Dropout2d(0.1)\u001b[39m\n",
      "\u001b[32m+        self.embedding_dropout = nn.Dropout2d(0.15)\u001b[39m\n",
      "         self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
      " class NeuralNet(nn.Module):\n",
      "\u001b[34mdata/processed/competitions/recruit-restaurant-visitor-forecasting/aharless/2289233.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+    drop_layer = keras.layers.GaussianDropout(.3)(cat_layer)\u001b[39m\n",
      "     m = Dense(hidden1_neurons, name='hidden1',\n",
      "              kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n",
      "\u001b[31m-                            stddev=0.05, seed=None))(cat_layer)\u001b[39m\n",
      "\u001b[32m+                            stddev=0.05, seed=None),\u001b[39m\n",
      "\u001b[32m+             kernel_regularizer=keras.regularizers.l2(1e-5))(drop_layer)\u001b[39m\n",
      "     m = keras.layers.PReLU()(m)\n",
      "     m = keras.layers.BatchNormalization()(m)\n",
      "\u001b[32m+    m = keras.layers.GaussianDropout(.25)(m)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/8801017.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    d0 = Dropout(0.5)(maxpool_1)\u001b[39m\n",
      "\u001b[31m-    d0 = Dense(20)(d0)\u001b[39m\n",
      "\u001b[31m-    d0 = Dropout(0.35)(d0)\u001b[39m\n",
      "\u001b[32m+    d0 = Dense(4)(maxpool_0)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/vsb-power-line-fault-detection/tarunpaparaju/10821417.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.1)(crf)\u001b[39m\n",
      "\u001b[32m+    attention = Attention(input_shape[1])(bi_gru)\u001b[39m\n",
      "\u001b[32m+    x = concatenate([attention, feat], axis=1)\u001b[39m\n",
      "     x = Dense(64, activation=\"relu\")(x)\n",
      "\u001b[31m-    x = Dropout(0.1)(x)\u001b[39m\n",
      "     x = Dense(1, activation=\"sigmoid\")(x)\n",
      "\u001b[34mdata/processed/competitions/siim-acr-pneumothorax-segmentation/meaninglesslives/17201815.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    uconv0 = Dropout(0.1/2)(uconv0)\u001b[39m\n",
      "\u001b[32m+    uconv0 = Dropout(dropout_rate/2)(uconv0)\u001b[39m\n",
      "     output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
      "\u001b[34mdata/processed/competitions/humpback-whale-identification/artgor/7966256.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.8)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.3)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tgs-salt-identification-challenge/shaojiaxin/5492287.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    uconv1 = Dropout(DropoutRatio/2)(uconv1)\u001b[39m\n",
      "\u001b[31m-    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\u001b[39m\n",
      "\u001b[32m+    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\u001b[39m\n",
      "\u001b[32m+    output_layer =  Activation('sigmoid')(output_layer_noActi)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tgs-salt-identification-challenge/ashishpatel26/5618302.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    uconv1 = Dropout(DropoutRatio/2)(uconv1)\u001b[39m\n",
      "\u001b[31m-    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\u001b[39m\n",
      "\u001b[32m+    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\u001b[39m\n",
      "\u001b[32m+    output_layer =  Activation('sigmoid')(output_layer_noActi)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/statoil-iceberg-classifier-challenge/knowledgegrappler/1655969.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    dense_ayer = Dropout(0.5) (BatchNormalization(momentum=bn_model) ( Dense(128, activation=p_activation)(img_concat) ))\u001b[39m\n",
      "\u001b[31m-    dense_ayer = Dropout(0.5) (BatchNormalization(momentum=bn_model) ( Dense(32, activation=p_activation)(dense_ayer) ))\u001b[39m\n",
      "\u001b[32m+    dense_ayer = Dropout(0.5) (BatchNormalization(momentum=bn_model) ( Dense(256, activation=p_activation)(img_concat) ))\u001b[39m\n",
      "\u001b[32m+    dense_ayer = Dropout(0.5) (BatchNormalization(momentum=bn_model) ( Dense(64, activation=p_activation)(dense_ayer) ))\u001b[39m\n",
      "     output = Dense(1, activation=\"sigmoid\")(dense_ayer)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/36358298.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.13)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "     x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/deepakd14/35959271.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/mak4alex/21536408.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    con_drop_layer3 = Dropout(0.2)(relu_layer6)\u001b[39m\n",
      "\u001b[32m+    con_drop_layer3 = Dropout(0.25)(relu_layer6)\u001b[39m\n",
      "     max_pool_layer3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(con_drop_layer3)\n",
      "     flatten = Flatten()(max_pool_layer3)  \n",
      "\u001b[34mdata/processed/competitions/humpback-whale-identification/artgor/7967914.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.3)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.8)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/mak4alex/21481962.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    con_drop_layer1 = Dropout(0.15)(relu_layer2)\u001b[39m\n",
      "\u001b[32m+    con_drop_layer1 = Dropout(0.2)(relu_layer2)\u001b[39m\n",
      "     max_pool_layer1 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(con_drop_layer1)\n",
      " def build_advanced_cnn(input_shape, classes):\n",
      "     conv_layer3 = Conv2D(64, kernel_size=(3,3), strides=1, padding='same')(max_pool_layer1)\n",
      "\u001b[31m-    batch_norm_layer3 = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(conv_layer3)\u001b[39m\n",
      "\u001b[32m+    batch_norm_layer3 = BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer='uniform')(conv_layer3)\u001b[39m\n",
      "     relu_layer3 = ReLU()(batch_norm_layer3)\n",
      " def build_advanced_cnn(input_shape, classes):\n",
      "     conv_layer4 = Conv2D(64, kernel_size=(3,3), strides=1, padding='same')(relu_layer3)\n",
      "\u001b[31m-    batch_norm_layer4 = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(conv_layer4)\u001b[39m\n",
      "\u001b[32m+    batch_norm_layer4 = BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer='uniform')(conv_layer4)\u001b[39m\n",
      "     relu_layer4 = ReLU()(batch_norm_layer4)\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/8677081.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.2)(maxpool_0)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.4)(maxpool_0)\u001b[39m\n",
      "     x = Dense(4)(x)\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/strifonov/9141947.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    d0 = Dropout(0.1)(embedding)\u001b[39m\n",
      "\u001b[32m+    d0 = SpatialDropout1D(0.1)(embedding)\u001b[39m\n",
      "     lstm = Bidirectional(LSTM(128, return_sequences=True))(d0)\n",
      "     lstm = Bidirectional(LSTM(64, return_sequences=False))(lstm)\n",
      "\u001b[31m-    d1 = Dropout(0.1)(lstm)\u001b[39m\n",
      "\u001b[32m+    d1 = Dropout(0.15)(lstm)\u001b[39m\n",
      "     d1 = Dense(64)(d1)\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/cevangelist/13238685.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    drop_0 = L.SpatialDropout1D(rate=0.1)(emb)\u001b[39m\n",
      "\u001b[32m+    drop_0 = L.SpatialDropout1D(rate=0.15)(emb)\u001b[39m\n",
      "     bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS,\n",
      " def model_fn():\n",
      "     model = Model(inputs=inp, outputs=[out_main, out_aux, out_subtypes_probas])\n",
      "\u001b[31m-    model.compile(Adam(lr=LR), loss=binary_crossentropy, metrics=[binary_accuracy])\u001b[39m\n",
      "\u001b[32m+    model.compile(Adam(lr=LR), loss=binary_crossentropy, metrics=['acc'])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/ryches/18354887.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.3)(cat_emb)) for cat_emb in categorical_embeddings])\u001b[39m\n",
      "\u001b[32m+    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(1.0)(cat_emb)) for cat_emb in categorical_embeddings])\u001b[39m\n",
      " #     categorical_logits = Dropout(.5)(categorical_logits)\n",
      " def make_model():\n",
      "     numerical_inputs = Input(shape=[tr_df[numerical].shape[1]], name = 'numerical')\n",
      "\u001b[31m-    numerical_logits = Dropout(1.0)(numerical_inputs)\u001b[39m\n",
      "\u001b[32m+    numerical_logits = Dropout(.5)(numerical_inputs)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/online-sales/jmourad100/34126088.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    hidden = Dropout(drop_out_rate/2)(hidden)\u001b[39m\n",
      "\u001b[32m+    hidden = Dropout(drop_out_rate)(hidden)\u001b[39m\n",
      "     out = Dense(1, activation='sigmoid')(hidden)\n",
      " def build_cnn_model(embedding_matrix):\n",
      "     model = Model(inputs=[words, numerics], outputs=out)\n",
      "\u001b[31m-    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(lr = 0.001))\u001b[39m\n",
      "\u001b[32m+    model.compile(loss=f1_loss, metrics=['accuracy', f1], optimizer=Adam(lr = 0.01))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/cevangelist/12749880.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    drop_0 = L.Dropout(0.5, seed=42)(emb)\u001b[39m\n",
      "\u001b[31m-    bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS, return_sequences=False))(drop_0)\u001b[39m\n",
      "\u001b[32m+    bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS, return_sequences=False))(emb)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/mohannksr/32728103.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.3)(x[0]) \u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(256, 2,padding='same')(x1)\u001b[39m\n",
      "     x1 = tf.keras.layers.LeakyReLU()(x1)\n",
      "\u001b[31m-    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\u001b[39m\n",
      "     x1 = tf.keras.layers.Dense(1)(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/whale-detection-challenge/artgor/7967914.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.3)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.8)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/deshwalmahesh/34962355.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = torch.nn.Dropout(dropout) # drop the values by random which comes from previous layer\u001b[39m\n",
      "\u001b[32m+        self.dropout = torch.nn.Dropout(drop) # drop the values by random which comes from previous layer\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/chandrimad31/35749925.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Dropout(0.15)(x[0])\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Conv1D(1024,2, padding='same')(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.LeakyReLU()(x2)    \u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Conv1D(256,2, padding='same')(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.LeakyReLU()(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Dropout(0.15)(x[0])\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Conv1D(96,1)(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.LeakyReLU()(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Dense(1)(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Flatten()(x2)\u001b[39m\n",
      "\u001b[31m-    x2 =  tf.keras.layers.Activation('softmax')(x2)\u001b[39m\n",
      "\u001b[32m+        x2 = tf.keras.layers.Dropout(0.30)(x[1]) \u001b[39m\n",
      "\u001b[32m+        x2 = tf.keras.layers.Activation('softmax')(x2)\u001b[39m\n",
      "\u001b[32m+        model = tf.keras.models.Model(inputs = [ids, att, tok], outputs=[x1, x2])\u001b[39m\n",
      "\u001b[32m+        optimizer = tf.keras.optimizers.Adam(learning_rate = LR)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/zsn6034/9870107.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.embedding_dropout = nn.Dropout2d(0.1)\u001b[39m\n",
      "\u001b[32m+        self.embedding_dropout = nn.Dropout2d(0.09)  #0.1best\u001b[39m\n",
      "         self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
      " class NeuralNet(nn.Module):\n",
      "         self.relu = nn.ReLU()\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.06) #0.1 best\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.1) #0.1 best\u001b[39m\n",
      "         self.out = nn.Linear(16, 1)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/vbmokin/35185030.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\u001b[39m\n",
      "     x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/cifar-10/aniruddhkb/32986151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X = keras.layers.Dropout(rate = 0.1)(X)\u001b[39m\n",
      "\u001b[32m+    X = keras.layers.Dropout(rate = dropout_rate)(X)\u001b[39m\n",
      "     X = keras.layers.Conv2D(filters = 64, kernel_size = 7, strides = 2, data_format = \"channels_last\")(X)\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/johnfarrell/5808442.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.5)(final_state)\u001b[39m\n",
      "\u001b[31m-    outputs = Dense(1, activation='sigmoid')(x)\u001b[39m\n",
      "\u001b[32m+    outputs = Dense(1, activation='sigmoid')(final_state)\u001b[39m\n",
      "     model = Model(inputs=input_x, outputs=outputs)\n",
      " def get_model(n_final_state, lr=1e-3, decay=1e-8):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/cevangelist/13125409.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    drop_0 = L.SpatialDropout1D(rate=0.15)(emb)\u001b[39m\n",
      "\u001b[32m+    drop_0 = L.SpatialDropout1D(rate=0.2)(emb)\u001b[39m\n",
      "     bi_lstm_0 = L.Bidirectional(L.CuDNNLSTM(RECURRENT_UNITS,\n",
      "\u001b[34mdata/processed/competitions/vsb-power-line-fault-detection/miklgr500/9030596.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.75)(x)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.35)(x)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/artgor/9985361.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.embedding_dropout = nn.Dropout2d(0.15)\u001b[39m\n",
      "\u001b[32m+        self.embedding_dropout = nn.Dropout2d(0.05)\u001b[39m\n",
      "         self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
      " class NeuralNet(nn.Module):\n",
      "         self.relu = nn.ReLU()\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.15)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.1)\u001b[39m\n",
      "         self.out = nn.Linear(64, 1)\n",
      " class NeuralNet(nn.Module):\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/cchyun/10148626.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.embedding_dropout = nn.Dropout2d(0.11)\u001b[39m\n",
      "\u001b[32m+        self.embedding_dropout = nn.Dropout2d(0.13)\u001b[39m\n",
      "         self.lstm = nn.LSTM(N_EMBED, hidden_size, bidirectional=True, batch_first=True)\n",
      " class NeuralNet(nn.Module):\n",
      "         self.relu = nn.ReLU()\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.11)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.13)\u001b[39m\n",
      "         self.out = nn.Linear(17, 1)\n",
      "\u001b[34mdata/processed/competitions/plant-seedlings-classification/allunia/22535697.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model.fc = nn.Sequential(nn.Dropout(0.5),\u001b[39m\n",
      "\u001b[32m+model.fc = nn.Sequential(nn.Dropout(0.3),\u001b[39m\n",
      "                          nn.Linear(num_features, NUM_CLASSES))\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/cchyun/10206984.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.embedding_dropout = nn.Dropout2d(0.13)\u001b[39m\n",
      "\u001b[32m+        self.embedding_dropout = nn.Dropout2d(0.10)\u001b[39m\n",
      "         self.lstm = nn.LSTM(N_EMBED, hidden_size, bidirectional=True, batch_first=True)\n",
      " class NeuralNet(nn.Module):\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/rsmits/35402887.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.30)(x[1]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.35)(x[1]) \u001b[39m\n",
      "     x2 = tf.keras.layers.Activation('softmax')(x2)\n",
      "\u001b[34mdata/processed/competitions/statoil-iceberg-classifier-challenge/solomonk/1679411.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-dropout = torch.nn.Dropout(p=0.30)\u001b[39m\n",
      "\u001b[32m+dropout = torch.nn.Dropout(p=0.20)\u001b[39m\n",
      " relu=torch.nn.LeakyReLU()\n",
      " class Net(nn.Module):\n",
      "             nn.BatchNorm2d(2),\n",
      "\u001b[31m-            nn.Conv2d(2, 32, kernel_size=3, padding=1),relu, pool,\u001b[39m\n",
      "\u001b[31m-            nn.Conv2d(32, 64, kernel_size=3, padding=1),relu, pool,\u001b[39m\n",
      "\u001b[32m+            nn.Conv2d(2, 32, kernel_size=3, padding=1),\u001b[39m\n",
      "\u001b[32m+            relu, \u001b[39m\n",
      "\u001b[32m+            pool,\u001b[39m\n",
      "\u001b[32m+            nn.Conv2d(32, 64, kernel_size=3, padding=1),\u001b[39m\n",
      "\u001b[32m+            relu, \u001b[39m\n",
      "\u001b[32m+            pool,\u001b[39m\n",
      "         )        \n",
      "\u001b[34mdata/processed/competitions/cifar-10/aniruddhkb/32986151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X = keras.layers.Dropout(rate = 0.5)(X)\u001b[39m\n",
      "\u001b[32m+    X = keras.layers.Dropout(rate = dropout_rate)(X)\u001b[39m\n",
      "     X = keras.layers.Conv2D(filters = F2, kernel_size = 3, strides = 1, padding = 'same', data_format = \"channels_last\")(X)\n",
      " def convolutional_block(X, filters, stride):\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/andrelmfarias/11723168.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p=drop_prob)\u001b[39m\n",
      "\u001b[31m-        self.fc = nn.Linear(self.hidden_dim*2, output_size)\u001b[39m\n",
      "\u001b[31m-        self.sigmoid = nn.Sigmoid()\u001b[39m\n",
      "\u001b[32m+        self.final_dense = nn.Sequential(\u001b[39m\n",
      "\u001b[32m+            nn.Dropout(p=drop_prob),\u001b[39m\n",
      "\u001b[32m+            nn.Linear(self.hidden_dim*2 + out_dim, self.hidden_layer_dim),\u001b[39m\n",
      "\u001b[32m+            nn.ReLU(),\u001b[39m\n",
      "\u001b[32m+            nn.Dropout(p=drop_prob),\u001b[39m\n",
      "\u001b[32m+            nn.Linear(self.hidden_layer_dim, 1),\u001b[39m\n",
      "\u001b[32m+            nn.Sigmoid()\u001b[39m\n",
      "\u001b[32m+        )\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/gustafsilva/14935436.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.45)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.35)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/leostep/8266702.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(p=0.1)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(p=0.05)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/ajax0564/35259933.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "     x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
      "     x1 = tf.keras.layers.LeakyReLU()(x1)\n",
      "\u001b[32m+    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\u001b[39m\n",
      "     x1 = tf.keras.layers.Dense(1)(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/deepakd14/36106123.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.1)(x[0])\u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.15)(x[0])\u001b[39m\n",
      "     x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
      " def build_model():\n",
      "\u001b[34mdata/processed/competitions/siim-acr-pneumothorax-segmentation/meaninglesslives/17180595.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    uconv0 = Dropout(dropout_rate/2)(uconv0)\u001b[39m\n",
      "\u001b[32m+    uconv0 = Dropout(0.1/2)(uconv0)\u001b[39m\n",
      "     output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
      "\u001b[34mdata/processed/competitions/vsb-power-line-fault-detection/miklgr500/9030704.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.35)(x)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.85)(x)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/lomen0857/35241308.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.LeakyReLU()(x2)\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dense(1)(x2)\u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x2 = tf.keras.layers.Conv1D(1,1)(x2)\u001b[39m\n",
      "     x2 = tf.keras.layers.Flatten()(x2)\n",
      "\u001b[34mdata/processed/competitions/whale-detection-challenge/artgor/7966256.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.dropout = nn.Dropout(0.8)\u001b[39m\n",
      "\u001b[32m+        self.dropout = nn.Dropout(0.3)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-5/aerdem4/33772448.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = KL.Conv1D(8, 3, activation=\"relu\")(KL.GaussianDropout(0.1)(seq_inp))\u001b[39m\n",
      "\u001b[32m+    x = KL.Conv1D(8, 3, activation=\"relu\")(seq_inp)\u001b[39m\n",
      "     x = KL.Conv1D(32, 3, activation=\"relu\", strides=3)(x)\n",
      " def get_model():\n",
      "\u001b[34mdata/processed/competitions/cifar-10/aniruddhkb/32986151.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    X = keras.layers.Dropout(rate = 0.5)(X)\u001b[39m\n",
      "\u001b[32m+    X = keras.layers.Dropout(rate = dropout_rate)(X)\u001b[39m\n",
      "     X = keras.layers.Conv2D(F3, kernel_size = 1, strides = 1,padding = 'valid', data_format = \"channels_last\")(X)\n",
      "\u001b[34mdata/processed/competitions/vsb-power-line-fault-detection/miklgr500/9030522.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x = Dropout(0.25)(x)\u001b[39m\n",
      "\u001b[32m+    x = Dropout(0.75)(x)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/mak4alex/21481962.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    con_drop_layer3 = Dropout(0.15)(relu_layer6)\u001b[39m\n",
      "\u001b[32m+    con_drop_layer3 = Dropout(0.2)(relu_layer6)\u001b[39m\n",
      "     max_pool_layer3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(con_drop_layer3)\n",
      " def build_advanced_cnn(input_shape, classes):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/tarunpaparaju/30804426.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    embedding = SpatialDropout1D(0.3)(embedding)\u001b[39m\n",
      "\u001b[31m-    conv_1 = Conv1D(64, 2)(embedding)\u001b[39m\n",
      "\u001b[31m-    conv_2 = Conv1D(64, 3)(embedding)\u001b[39m\n",
      "\u001b[31m-    conv_3 = Conv1D(64, 4)(embedding)\u001b[39m\n",
      "\u001b[31m-    conv_4 = Conv1D(64, 5)(embedding)\u001b[39m\n",
      "\u001b[31m-    maxpool_1 = GlobalAveragePooling1D()(conv_1)\u001b[39m\n",
      "\u001b[31m-    maxpool_2 = GlobalAveragePooling1D()(conv_2)\u001b[39m\n",
      "\u001b[31m-    maxpool_3 = GlobalAveragePooling1D()(conv_3)\u001b[39m\n",
      "\u001b[31m-    maxpool_4 = GlobalAveragePooling1D()(conv_4)\u001b[39m\n",
      "\u001b[31m-    conc = concatenate([maxpool_1, maxpool_2, maxpool_3, maxpool_4], axis=1)\u001b[39m\n",
      "\u001b[31m-    conc = Dense(64, activation='relu')(conc)\u001b[39m\n",
      "\u001b[32m+    conc = K.sum(embedding, axis=2)\u001b[39m\n",
      "\u001b[32m+    conc = Dense(128, activation='relu')(conc)\u001b[39m\n",
      "     conc = Dense(1, activation='sigmoid')(conc)\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/rsmits/35402887.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.30)(x[0]) \u001b[39m\n",
      "\u001b[32m+    x1 = tf.keras.layers.Dropout(0.35)(x[0]) \u001b[39m\n",
      "     x1 = tf.keras.layers.Activation('softmax')(x1)\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*Dropout\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*chisquare\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/y2kshehan/2007135.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lin_reg_pl = LinearRegression()\u001b[39m\n",
      " #Predicting the SalePrice using cross validation (KFold method)\n",
      "\u001b[31m-y_pred_pl = cross_val_predict(lin_reg_pl, X_poly, y, cv=6 )\u001b[39m\n",
      "\u001b[32m+y_pred_pl = cross_val_predict(lin_reg_pl, X_poly, y, cv=10 )\u001b[39m\n",
      " #Polynominal Regression Accuracy with cross validation\n",
      " accuracy_pl = metrics.r2_score(y, y_pred_pl)\n",
      "\u001b[34mdata/processed/competitions/restaurant-revenue-prediction/ani310/574131.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-cls = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+cls = RandomForestRegressor()\u001b[39m\n",
      " cls.fit(xTrain, yTrain)\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-3/letili0417/31396638.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        xmodel1 = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+        xmodel1 = XGBRegressor(n_estimators=100)\u001b[39m\n",
      "         xmodel1.fit(subtrain, yc_subtrain)\n",
      " for country in list(train.Country_Region.unique()):\n",
      "\u001b[34mdata/processed/competitions/walmart-recruiting-trip-type-classification/mustaphaelmliles/32654614.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | RandomForestRegressor \u001b[39m\n",
      "\u001b[31m-            | GradientBoostingRegressor | ExtraTreesRegressor | KNeighborsRegressor | SVR)\u001b[39m\n",
      "\u001b[32m+pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor)\u001b[39m\n",
      " pipeline.visualize()\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/ashutosh619sudo/31858467.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-reg_cc = LinearRegression()\u001b[39m\n",
      "\u001b[31m-reg_ft = LinearRegression()\u001b[39m\n",
      "\u001b[32m+reg_cc = LinearRegression(fit_intercept=True)\u001b[39m\n",
      "\u001b[32m+reg_ft = LinearRegression(fit_intercept=True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/masdeval/1913736.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-classifierLinearRegression = LinearRegression()\u001b[39m\n",
      "\u001b[32m+classifierSVRAll = SVR(kernel='linear', C=1e4) \u001b[39m\n",
      " kf = KFold(5, random_state=7)    \n",
      " oos_pred = []\n",
      " fold = 0\n",
      "\u001b[31m-pred = []\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/abstraction-and-reasoning-challenge/jamesmcguigan/32837004.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    xgb =  LinearRegression()\u001b[39m\n",
      "\u001b[32m+    xgb =  SVR(C=1.0, epsilon=0.2)\u001b[39m\n",
      "     xgb.fit(feat, target, verbose=-1)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/john850512/5254961.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-LassoRegression = LinearRegression().fit(X_train, y_train.ravel())\u001b[39m\n",
      "\u001b[32m+LassoRegression = Lasso(alpha=0.0005).fit(X_train, y_train.ravel())\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pkdd-15-taxi-trip-time-prediction-ii/raymonmina/458111.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lr = LinearRegression(normalize=True)\u001b[39m\n",
      "\u001b[32m+lr = LinearRegression()\u001b[39m\n",
      " lr.fit(train_d, train_tar)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/miguelangelnieto/744598.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-reg = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+reg = RandomForestRegressor()\u001b[39m\n",
      " reg.fit(train,labels)\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/iamabhi1/30922378.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  new_model = LinearRegression()\u001b[39m\n",
      "\u001b[32m+  new_model = RandomForestRegressor()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/ashutosh619sudo/31858848.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-reg_cc = LinearRegression(fit_intercept=True)\u001b[39m\n",
      "\u001b[31m-reg_ft = LinearRegression(fit_intercept=True)\u001b[39m\n",
      "\u001b[32m+reg_cc = LinearRegression()\u001b[39m\n",
      "\u001b[32m+reg_ft = LinearRegression()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/dtl333/4343661.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-final_model = LinearRegression()\u001b[39m\n",
      "\u001b[32m+final_model = RandomForestRegressor()\u001b[39m\n",
      " final_model.fit(housing_prepared, housing_labels)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/deeplake/1637761.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    clf = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[31m-    results[\"Linear\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = linear_model.Ridge()\u001b[39m\n",
      "\u001b[31m-    results[\"Ridge\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = linear_model.BayesianRidge()\u001b[39m\n",
      "\u001b[31m-    results[\"Bayesian Ridge\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = linear_model.HuberRegressor()\u001b[39m\n",
      "\u001b[31m-    results[\"Hubber\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = linear_model.Lasso(alpha=1e-4)\u001b[39m\n",
      "\u001b[31m-    results[\"Lasso\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = BaggingRegressor()\u001b[39m\n",
      "\u001b[31m-    results[\"Bagging\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = RandomForestRegressor()\u001b[39m\n",
      "\u001b[31m-    results[\"RandomForest\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = AdaBoostRegressor()\u001b[39m\n",
      "\u001b[31m-    results[\"AdaBoost\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = svm.SVR()\u001b[39m\n",
      "\u001b[31m-    results[\"SVM RBF\"]=test_model(clf)\u001b[39m\n",
      "\u001b[31m-    clf = svm.SVR(kernel=\"linear\")\u001b[39m\n",
      "\u001b[31m-    results[\"SVM Linear\"]=test_model(clf)\u001b[39m\n",
      "\u001b[32m+    results[\"Linear\"]=train_get_score(linear_model.LinearRegression())\u001b[39m\n",
      "\u001b[32m+    results[\"Ridge\"]=train_get_score(linear_model.Ridge())\u001b[39m\n",
      "\u001b[32m+    results[\"Bayesian Ridge\"]=train_get_score(linear_model.BayesianRidge())\u001b[39m\n",
      "\u001b[32m+    results[\"Hubber\"]=train_get_score(linear_model.HuberRegressor())\u001b[39m\n",
      "\u001b[32m+    results[\"Lasso\"]=train_get_score(linear_model.Lasso(alpha=1e-4))\u001b[39m\n",
      "\u001b[32m+    results[\"RandomForest\"]=train_get_score(RandomForestRegressor())\u001b[39m\n",
      "\u001b[32m+    results[\"SVM RBF\"]=train_get_score(svm.SVR())\u001b[39m\n",
      "\u001b[32m+    results[\"SVM Linear\"]=train_get_score(svm.SVR(kernel=\"linear\"))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/walmart-recruiting-trip-type-classification/mustaphaelmliles/32737515.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor)\u001b[39m\n",
      "\u001b[32m+pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor |\u001b[39m\n",
      "\u001b[32m+               RandomForestRegressor | GradientBoostingRegressor | ExtraTreesRegressor  | SVR )\u001b[39m\n",
      " pipeline.visualize()\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/vedato/35186547.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  bestLinearModel= LinearRegression().fit(X_train, y_train)\u001b[39m\n",
      "\u001b[32m+  bestLinearModel= Lasso().fit(X_train, y_train)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/walmart-recruiting-store-sales-forecasting/mustaphaelmliles/32654614.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | RandomForestRegressor \u001b[39m\n",
      "\u001b[31m-            | GradientBoostingRegressor | ExtraTreesRegressor | KNeighborsRegressor | SVR)\u001b[39m\n",
      "\u001b[32m+pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor)\u001b[39m\n",
      " pipeline.visualize()\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/veroderberg/27376539.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_all_data = LinearRegression()\u001b[39m\n",
      "\u001b[32m+model_all_data = XGBRegressor()\u001b[39m\n",
      " model_all_data.fit(X, y)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/plasticgrammer/2878512.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = linear_model.LinearRegression()\u001b[39m\n",
      " #clf = linear_model.Ridge(alpha=1.0)\n",
      " clf = linear_model.LinearRegression()\n",
      " #clf = svm.SVR(kernel='linear', C=1e3, epsilon=2.0)\n",
      "\u001b[32m+clf = KernelRidge(alpha=0.001,  gamma=0.01, kernel='polynomial', degree=2, coef0=2.5)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/ranjithks/30713129.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-MODELS = {\"Linear_Reg\": LinearRegression(), \"KNN_Reg\": KNeighborsRegressor(), \"LinearSVR_Reg\": LinearSVR(), \"DecisionTree_Reg\": DecisionTreeRegressor(), \"DecisionTree_Class\": DecisionTreeClassifier(), \"ExtraTree_Reg\": ExtraTreeRegressor(), \"RandomForest_Reg\": RandomForestRegressor(), \"GB_Reg\": GradientBoostingRegressor()}\u001b[39m\n",
      "\u001b[32m+MODELS = {\"Linear_Reg\": LinearRegression(), \"KNN_Reg\": KNeighborsRegressor(), \"LinearSVR_Reg\": LinearSVR(), \"DecisionTree_Reg\": DecisionTreeRegressor(), \"DecisionTree_Class\": DecisionTreeClassifier(), \"ExtraTree_Reg\": ExtraTreeRegressor(), \"RandomForest_Reg\": RandomForestRegressor(), \"GB_Reg\": GradientBoostingRegressor(), \"XGB_Reg\": XGBRegressor() }\u001b[39m\n",
      " # \"SDG_Reg\": SGDRegressor(), \"RN_Reg\": RadiusNeighborsRegressor(), \"NuSVR_Reg\": NuSVR(), \"SVR_Reg\": SVR(),\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/ludovicoristori/31686652.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lrm1 = LinearRegression()\u001b[39m\n",
      "\u001b[31m-lrm2 = LinearRegression()\u001b[39m\n",
      "\u001b[32m+lrm1 = LogisticRegression()\u001b[39m\n",
      "\u001b[32m+lrm2 = LogisticRegression()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/mercari-price-suggestion-challenge/jkkphys/1796292.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = LinearRegression(n_jobs=4, fit_intercept=False)\u001b[39m\n",
      "\u001b[32m+model = LinearRegression(n_jobs=4)\u001b[39m\n",
      " start = time.time()\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-3/mehvishashiq/31199927.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = Pipeline([('linear', LinearRegression())])\u001b[39m\n",
      "\u001b[32m+model = Pipeline([('poly', PolynomialFeatures(degree=2, include_bias=False)),('Ridge', Ridge())]) \u001b[39m\n",
      " features = [\"prev_{}\".format(col) for col in TARGETS]\n",
      " test_full = pd.merge(test[\"ForecastId\"],test_full, on=\"ForecastId\")\n",
      " print(test_full.shape)\n",
      " sub_columns = [\"ForecastId\"]+TARGETS\n",
      "\u001b[34mdata/processed/competitions/bike-sharing-demand/jjuanramos/1095507.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = LinearRegression()\u001b[39m\n",
      "\u001b[32m+clf = LinearRegression(normalize= True)\u001b[39m\n",
      " clf.fit(X_train, y_train)\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/jozefc/25100815.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        regressor = LinearRegression()\u001b[39m\n",
      "\u001b[32m+        regressor = RandomForestRegressor(n_estimators = 100, max_depth = 15, random_state = 42, n_jobs = -1)    \u001b[39m\n",
      "         regressor.fit(train_X, train_y)\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/eliasgreen/30531125.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    sub_clf = LinearRegression().fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases', 'Fatalities']])\u001b[39m\n",
      "\u001b[32m+    sub_clf1 = PassiveAggressiveRegressor(random_state=RANDOM_SEED).fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases']])\u001b[39m\n",
      "\u001b[32m+    sub_clf2 = PassiveAggressiveRegressor(random_state=RANDOM_SEED).fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['Fatalities']])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/tejaspatel1094/31949003.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    reg1 = LinearRegression()\u001b[39m\n",
      "\u001b[31m-    reg1.fit(X_train, y_case_train)\u001b[39m\n",
      "\u001b[32m+    gbr_fatal = XGBRegressor(n_estimators=10)\u001b[39m\n",
      "\u001b[32m+    gbr_fatal.fit(X_train, y_fatal_tain)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/vsesham/24237618.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lg = LinearRegression().fit(X, y)\u001b[39m\n",
      "\u001b[32m+lg = SVC().fit(X, y)\u001b[39m\n",
      " y_pred = lg.predict(test_X)\n",
      " test_X['installation_id'] = test_df_subset['installation_id']\n",
      " final_df = test_X[['installation_id','accuracy_group']]\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/ranjithks/30713129.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-MODELS = {\"Linear_Reg\": LinearRegression(), \"KNN_Reg\": KNeighborsRegressor(), \"LinearSVR_Reg\": LinearSVR(), \"DecisionTree_Reg\": DecisionTreeRegressor(), \"DecisionTree_Class\": DecisionTreeClassifier(), \"ExtraTree_Reg\": ExtraTreeRegressor(), \"RandomForest_Reg\": RandomForestRegressor(), \"GB_Reg\": GradientBoostingRegressor()}\u001b[39m\n",
      "\u001b[32m+MODELS = {\"Linear_Reg\": LinearRegression(), \"KNN_Reg\": KNeighborsRegressor(), \"LinearSVR_Reg\": LinearSVR(), \"DecisionTree_Reg\": DecisionTreeRegressor(), \"DecisionTree_Class\": DecisionTreeClassifier(), \"ExtraTree_Reg\": ExtraTreeRegressor(), \"RandomForest_Reg\": RandomForestRegressor(), \"GB_Reg\": GradientBoostingRegressor(), \"XGB_Reg\": XGBRegressor() }\u001b[39m\n",
      " # \"SDG_Reg\": SGDRegressor(), \"RN_Reg\": RadiusNeighborsRegressor(), \"NuSVR_Reg\": NuSVR(), \"SVR_Reg\": SVR(),\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/nikhilbhaskar16/8668545.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-linear = LinearRegression()\u001b[39m\n",
      "\u001b[32m+linear = GradientBoostingRegressor()\u001b[39m\n",
      " linear.fit(Xtrain,Ytrain)\n",
      " pred = linear.predict(Xtest)\n",
      "\u001b[31m-pred[:5]\u001b[39m\n",
      "\u001b[32m+pred[:10]\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-3/aerdem4/30449348.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lr = LinearRegression()\u001b[39m\n",
      "\u001b[32m+model = Pipeline([('poly', PolynomialFeatures(degree=2, include_bias=False)),\u001b[39m\n",
      "\u001b[32m+                  ('linear', LinearRegression())])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/vbmokin/22583645.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_LR2 = LinearRegression2().fit(train2,target.to_numpy().flatten())\u001b[39m\n",
      "\u001b[32m+model_LR2 = LinearRegression().fit(train2,target.to_numpy().flatten())\u001b[39m\n",
      " model_Autofeat2 = LinearRegression().fit(X_train_feature_creation2, target.to_numpy().flatten())\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/eliasgreen/30530797.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    sub_clf = LinearRegression(random_state=RANDOM_SEED).fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases', 'Fatalities']])\u001b[39m\n",
      "\u001b[32m+    sub_clf = LinearRegression().fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases', 'Fatalities']])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/nphantawee/5697179.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  model_lm = LinearRegression()\u001b[39m\n",
      "\u001b[32m+  model_lg = LogisticRegression()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/eliasgreen/30531125.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    sub_clf = LinearRegression().fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases', 'Fatalities']])\u001b[39m\n",
      "\u001b[32m+    sub_clf1 = PassiveAggressiveRegressor(random_state=RANDOM_SEED).fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases']])\u001b[39m\n",
      "\u001b[32m+    sub_clf2 = PassiveAggressiveRegressor(random_state=RANDOM_SEED).fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['Fatalities']])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/eliasgreen/30530797.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    sub_clf = LinearRegression(random_state=RANDOM_SEED).fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases', 'Fatalities']])\u001b[39m\n",
      "\u001b[32m+    sub_clf = LinearRegression().fit(sub_train['Date'].values.reshape(-1, 1), sub_train[['ConfirmedCases', 'Fatalities']])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/thalia18/7001471.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = LinearRegression()\u001b[39m\n",
      "\u001b[31m-model.fit(train_X, train_y)\u001b[39m\n",
      "\u001b[31m-y_pred = model.predict(val_X) \u001b[39m\n",
      "\u001b[32m+alpha_ridge = [0.000001,0.0001,.001,1]\u001b[39m\n",
      "\u001b[32m+for item in alpha_ridge:\u001b[39m\n",
      "\u001b[32m+    modelt = Ridge(alpha=item,copy_X=True, fit_intercept=True, max_iter=None,\u001b[39m\n",
      "\u001b[32m+          normalize=False, random_state=None, solver='auto', tol=0.001)\u001b[39m\n",
      "\u001b[32m+    modelt.fit(train_X, train_y)\u001b[39m\n",
      "\u001b[32m+    y_predt = modelt.predict(val_X) \u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/fannasankh/6008705.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  regr = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+  regr = linear_model.Ridge()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/nikhilbhaskar16/8283887.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-linear = LinearRegression()\u001b[39m\n",
      "\u001b[32m+linear = GradientBoostingRegressor(learning_rate = 1.0, n_estimators = 100, max_depth = 4)\u001b[39m\n",
      " linear.fit(Xtrain,Ytrain)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/tomahim/2980312.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lr = LinearRegression()\u001b[39m\n",
      "\u001b[31m-lr.fit(X_train[predictor_cols], y_train)\u001b[39m\n",
      "\u001b[32m+regr = ElasticNet(random_state=0)\u001b[39m\n",
      "\u001b[32m+regr.fit(X_train[predictor_cols], y_train)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tmdb-box-office-prediction/alexandermelde/12618206.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-test_y  = LinearRegression().fit(df_train_X.values, df_train_y.values).predict(df_test_X.values)\u001b[39m\n",
      "\u001b[32m+test_y  = KNeighborsRegressor().fit(df_train_X.values, df_train_y.values).predict(df_test_X.values)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/iamabhi1/30922378.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  model = LinearRegression()\u001b[39m\n",
      "\u001b[32m+  model = RandomForestRegressor()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/azzion/2759221.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lr = LinearRegression(fit_intercept=False)\u001b[39m\n",
      "\u001b[32m+lr = LinearRegression()\u001b[39m\n",
      " lr.fit(X_train_org,y_train)\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/ozlerhakan/7456789.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lr = LinearRegression()\u001b[39m\n",
      "\u001b[32m+rf = RandomForestRegressor(n_estimators=20)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/rp1611/34392737.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-LRModel = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+LRModel = LinearRegression()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ga-customer-revenue-prediction/ani310/574131.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-cls = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+cls = RandomForestRegressor()\u001b[39m\n",
      " cls.fit(xTrain, yTrain)\n",
      "\u001b[34mdata/processed/competitions/mercedes-benz-greener-manufacturing/budhiraja/1250251.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-regr = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+regr = RandomForestRegressor()\u001b[39m\n",
      " pca = decomposition.PCA()\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/shaygu/16408265.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+lr = LinearRegression()\u001b[39m\n",
      "\u001b[32m+alphas2 = [0.0001, 0.0002, 0.0003, 0.0005, 0.0006, 0.0007]\u001b[39m\n",
      "\u001b[32m+alphas_alt = [14.6, 14.7,15, 15.1, 15.3, 15.4]\u001b[39m\n",
      " # Kernel Ridge Regression : made robust to outliers\n",
      " models = {'Ridge': ridge,\n",
      "           'Lasso': lasso, \n",
      "\u001b[31m-          'ElasticNet': elasticnet}\u001b[39m\n",
      "\u001b[32m+          'ElasticNet': elasticnet,\u001b[39m\n",
      "\u001b[32m+          'Linear regression':lr}\u001b[39m\n",
      " #           'lightgbm': lightgbm,\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-4/nayonika/31896237.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-from sklearn.linear_model import LinearRegression\u001b[39m\n",
      " #train classifier\n",
      "\u001b[31m-reg_CC = LinearRegression()\u001b[39m\n",
      "\u001b[31m-reg_Fat = LinearRegression()\u001b[39m\n",
      "\u001b[32m+reg_CC = GradientBoostingClassifier(n_estimators=100)\u001b[39m\n",
      "\u001b[32m+reg_Fat = GradientBoostingClassifier(n_estimators=100)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/sharmasanthosh/411829.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = LinearRegression(n_jobs=-1)\u001b[39m\n",
      " algo = \"LR\"\n",
      " for s in SIZE:\n",
      "     for name,fe, X_train, X_val,Y_train,Y_val, d, cols, ra, i_cols_list in X_all[s]:\n",
      "\u001b[32m+        model = LinearRegression(n_jobs=-1)\u001b[39m\n",
      "         model.fit(X_train,Y_train)\n",
      "\u001b[31m-        result = mean_squared_error(numpy.log1p(Y_val), numpy.log1p(model.predict(X_val)))\u001b[39m\n",
      "\u001b[32m+        true = numpy.log1p(Y_val)\u001b[39m\n",
      "\u001b[32m+        pred = numpy.log1p(model.predict(X_val))\u001b[39m\n",
      "\u001b[32m+        result = mean_squared_error(true, pred)\u001b[39m\n",
      "         mse.append(result)\n",
      "\u001b[34mdata/processed/competitions/two-sigma-connect-rental-listing-inquiries/aforest/836610.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+model = linear_model.LinearRegression(normalize=True)\u001b[39m\n",
      " model.fit(x,y)\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/iamabhi1/30808183.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  model = LinearRegression()\u001b[39m\n",
      "\u001b[32m+  model = RandomForestRegressor()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/LANL-Earthquake-Prediction/alexsemenov/15125728.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model = LinearRegression()\u001b[39m\n",
      "     params = {\n",
      " for i in range(K):\n",
      "     model = NuSVR(**params)\n",
      "\u001b[31m-    model.fit(feature_train.reshape(-1,1), 1/(0.30 - target_train))\u001b[39m\n",
      "\u001b[32m+    model.fit(feature_train.reshape(-1,1), 1/(threshold + 0.01 - target_train))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-3/mehvishashiq/31199927.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = Pipeline([('linear', LinearRegression())])\u001b[39m\n",
      "\u001b[32m+model = Pipeline([('poly', PolynomialFeatures(degree=2, include_bias=False)),('Ridge', Ridge())]) \u001b[39m\n",
      " features = [\"prev_{}\".format(col) for col in TARGETS]\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/varunsharmaml/6274750.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  model = LinearRegression(normalize=True, n_jobs=5)\u001b[39m\n",
      "\u001b[32m+  model = LinearRegression(normalize=True, n_jobs=8)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/walmart-recruiting-store-sales-forecasting/mustaphaelmliles/32737515.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor)\u001b[39m\n",
      "\u001b[32m+pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor |\u001b[39m\n",
      "\u001b[32m+               RandomForestRegressor | GradientBoostingRegressor | ExtraTreesRegressor  | SVR )\u001b[39m\n",
      " pipeline.visualize()\n",
      "\u001b[34mdata/processed/competitions/allstate-claims-severity/fredtony/467687.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+from sklearn.linear_model import LinearRegression\u001b[39m\n",
      " clfLR = LinearRegression()\n",
      " clfLR.fit(x_train, y)\n",
      "\u001b[31m-y_pred_LR = clf.predict(x_test)\u001b[39m\n",
      "\u001b[32m+y_pred_LR = clfLR.predict(x_test)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/allstate-claims-severity/jiashenliu/401378.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-Regressors = [LinearRegression(),Lasso(),DecisionTreeRegressor(),GradientBoostingRegressor(learning_rate=0.3,criterion='mae')]\u001b[39m\n",
      "\u001b[32m+Regressors = [LinearRegression(),Lasso(),DecisionTreeRegressor()\u001b[39m\n",
      "\u001b[32m+             ]\u001b[39m\n",
      " MAE=[]\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/kaggledroid/29264546.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lr = LinearRegression()\u001b[39m\n",
      "\u001b[32m+clf = Lasso()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/kylingu/3855261.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-models = [LinearRegression(),\u001b[39m\n",
      "\u001b[31m-          Ridge(alpha=17),\u001b[39m\n",
      "\u001b[32m+models = [Ridge(alpha=17),\u001b[39m\n",
      "           Lasso(alpha=0.00101),\n",
      " models = [LinearRegression(),\n",
      "           RandomForestRegressor(max_features=28, n_estimators=101), \n",
      "\u001b[31m-          GradientBoostingRegressor(random_state=0, max_depth=4, max_features=27, n_estimators=187)]\u001b[39m\n",
      "\u001b[31m-names = ['lr', 'ridge', 'lasso', 'knn', 'svr', 'dtr', 'rfr', 'gbr']\u001b[39m\n",
      "\u001b[32m+          GradientBoostingRegressor(random_state=0, max_depth=4, max_features=24, n_estimators=209)]\u001b[39m\n",
      "\u001b[32m+names = ['ridge', 'lasso', 'knn', 'svr', 'dtr', 'rfr', 'gbr']\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-3/letili0417/31396638.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        xmodel2 = linear_model.LinearRegression()\u001b[39m\n",
      "\u001b[32m+        xmodel2 = XGBRegressor(n_estimators=100)\u001b[39m\n",
      "         xmodel2.fit(subtrain, yf_subtrain)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/sowbarani/21268868.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-from sklearn.linear_model import LinearRegression\u001b[39m\n",
      "\u001b[31m-my_model = LinearRegression()\u001b[39m\n",
      "\u001b[32m+\"\"\"from sklearn.linear_model import LinearRegression\u001b[39m\n",
      "\u001b[32m+my_model = LinearRegression()\"\"\"\u001b[39m\n",
      " my_model.fit(valid_x,valid_y)\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/plasticgrammer/2909430.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf_1 = linear_model.LassoCV(alphas = [1, 0.1, 0.001, 0.0005]) #LinearRegression()\u001b[39m\n",
      "\u001b[32m+clf_1 = xgb.XGBRegressor(max_depth=2, n_estimators=200)\u001b[39m\n",
      " clf_2 = svm.SVR(kernel='rbf', C=1e3, gamma=0.1, epsilon=0.1)\n",
      "\u001b[34mdata/processed/competitions/mercari-price-suggestion-challenge/thomastrenner/2260914.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-lin_reg = LinearRegression()\u001b[39m\n",
      "\u001b[32m+lin_reg = Ridge()\u001b[39m\n",
      " lin_reg.fit(X_train, y_train)\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*LinearRegression\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*wilcoxon\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/online-sales/sgrsgrsgr/6969291.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = SGDClassifier(loss='log', penalty='none', max_iter=1000, fit_intercept=True, random_state=1234)\u001b[39m\n",
      "\u001b[32m+clf = SGDClassifier(loss='log', penalty='L2', max_iter=5000, fit_intercept=True, random_state=1234)\u001b[39m\n",
      " clf.fit(X_train, y_train)\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/7509653.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.parameters(), lr=0.1, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\u001b[39m\n",
      " exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
      "\u001b[34mdata/processed/competitions/plant-seedlings-classification/allunia/18786594.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=1e-2, momentum=0.9)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/755230.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr = 5, decay = 0.1, momentum = .85, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd1 = SGD(lr = 5, decay = 0.5, momentum = .85, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd2 = SGD(lr = 5, decay = 0.5, momentum = .85, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd3 = SGD(lr = 5, decay = 0.5, momentum = .85, nesterov = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-1/gowtamsingulur/30723305.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = linear_model.SGDRegressor(loss='huber',max_iter=1000, tol=1e-3)\u001b[39m\n",
      "\u001b[32m+clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\u001b[39m\n",
      " clf.fit(data_train_with_conf.todense(), df_train['Fatalities'])\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/11131950.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.004, momentum=0.99)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.0004, momentum=0.99)\u001b[39m\n",
      " #scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
      "\u001b[34mdata/processed/competitions/global-wheat-detection/sgtbaur/36462392.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=config.lr)\u001b[39m\n",
      "\u001b[32m+        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.lr)\u001b[39m\n",
      "         self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
      " class Fitter:\n",
      "     def save(self, path):\n",
      "\u001b[31m-        self.model.eval()\u001b[39m\n",
      "\u001b[31m-        torch.save({\u001b[39m\n",
      "\u001b[31m-            'model_state_dict': self.model.state_dict(),\u001b[39m\n",
      "\u001b[31m-            'optimizer_state_dict': self.optimizer.state_dict(),\u001b[39m\n",
      "\u001b[31m-            'scheduler_state_dict': self.scheduler.state_dict(),\u001b[39m\n",
      "\u001b[31m-            'best_summary_loss': self.best_summary_loss,\u001b[39m\n",
      "\u001b[31m-            'epoch': self.epoch,\u001b[39m\n",
      "\u001b[31m-        }, path)\u001b[39m\n",
      "\u001b[32m+        torch.save(model.state_dict(), 'fasterrcnn_mobnet_fpn.pth')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tmdb-box-office-prediction/silverfoxdss/14893116.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd1 = SGDRegressor(alpha=0.01, fit_intercept=False, n_iter=200)\u001b[39m\n",
      "\u001b[32m+sgd1 = SGDRegressor(alpha=0.0001, fit_intercept=False, max_iter=100, early_stopping=True, n_iter_no_change=5, epsilon=0.1,\u001b[39m\n",
      "\u001b[32m+                   loss='squared_loss')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/leaf-classification/ernie55ernie/831754.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    sgd = SGD(lr=0.8, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[31m-    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\u001b[39m\n",
      "\u001b[32m+    optimizer = RMSprop(lr=0.8, rho=0.9, epsilon=1e-08, decay=0.0)\u001b[39m\n",
      "\u001b[32m+    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\u001b[39m\n",
      "     print('Finish construction of a cnn model')\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/772677.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd4 = SGD(lr = 0.3, decay = 0.1, momentum = .95, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd4 = SGD(lr = .3, decay = 0.1, momentum = .95, nesterov = True)\u001b[39m\n",
      " classifier.compile(loss='categorical_crossentropy', optimizer = sgd4, metrics=['accuracy'])\n",
      " classifier.compile(loss='categorical_crossentropy', optimizer = sgd4, metrics=['\n",
      " # What will happen to the learnning rates under this decay schedule\n",
      "\u001b[31m-lr = .5\u001b[39m\n",
      "\u001b[32m+lr = 5\u001b[39m\n",
      " for i in range(12):\n",
      "\u001b[34mdata/processed/competitions/freesound-audio-tagging/ashirahama/12868866.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optimizers.SGD(lr=0.003, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      " # optimizer = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/8139113.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.1, momentum=0.99)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.005, momentum=0.99)\u001b[39m\n",
      " #scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
      "\u001b[34mdata/processed/competitions/titanic/nicapotato/20193922.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-grid = GridSearchCV(SGDClassifier(max_iter=5, tol=None),\u001b[39m\n",
      "\u001b[32m+grid = GridSearchCV(SGDClassifier(),\u001b[39m\n",
      "                     param_grid,cv=cv, scoring=scoring,\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/jypucca/467550.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = SGD(lr=1e-3)\u001b[39m\n",
      "\u001b[32m+optimizer = Adam()\u001b[39m\n",
      " #logloss-> binary_crossentropy\n",
      " def catdog():\n",
      "\u001b[34mdata/processed/competitions/freesound-audio-tagging-2019/ashirahama/12861873.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optimizers.SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+optimizer = optimizers.SGD(lr=0.003, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      " # optimizer = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/amoghjrules/31177764.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+sgd = SGDRegressor()\u001b[39m\n",
      " rf = RandomForestRegressor(n_estimators = N_ESTIMATORS)\n",
      " for model in models:\n",
      "             model1 = model\n",
      "\u001b[31m-            cv_result = cross_validate(model1, train_data, t1, scoring = 'r2', cv= CV)\u001b[39m\n",
      "             score_mean_t1.append(np.mean(cv_result['test_score']))\n",
      "\u001b[34mdata/processed/competitions/titanic/kabure/2635495.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr = 0.03, momentum = 0.9)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr = 0.03, momentum = 0.2)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/plasticgrammer/3429949.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = linear_model.SGDRegressor(max_iter=1000)\u001b[39m\n",
      "\u001b[32m+clf = linear_model.SGDRegressor(eta0=0.01, learning_rate=\"constant\", max_iter=1000, shuffle=False)\u001b[39m\n",
      " clf.fit(X_train, Y_train)\n",
      "\u001b[34mdata/processed/competitions/overfitting/filwaitman/8723072.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    inner_clf = SGDRegressor(penalty='l1')\u001b[39m\n",
      "\u001b[32m+    options = [\u001b[39m\n",
      "\u001b[32m+        (SGDRegressor, [{'penalty': ['l1', 'l2', 'elasticnet'], 'alpha': [0, 0.1, 0.01, 0.001]}]),\u001b[39m\n",
      "\u001b[32m+        (linear_model.Lasso, [{'alpha': [0, 0.1, 0.01, 0.001]}]),\u001b[39m\n",
      "\u001b[32m+        (linear_model.LassoLars, [{'alpha': [0, 0.1, 0.01, 0.001]}]),\u001b[39m\n",
      "\u001b[32m+        (linear_model.ElasticNet, [{'alpha': [10, 1, 0, 0.1, 0.01, 0.001]}]),\u001b[39m\n",
      "\u001b[32m+    ]\u001b[39m\n",
      "\u001b[32m+    inner_clf = get_best_estimator(X_train, y_train, options)\u001b[39m\n",
      "     clf = make_pipeline(\n",
      "         MinMaxScaler(),  # Scale our features to be within 0..1\n",
      "\u001b[32m+        PolynomialFeatures(),\u001b[39m\n",
      "         SelectFromModel(inner_clf),  # Remove unnecessary features (mitigating risk of overfitting)\n",
      "\u001b[34mdata/processed/competitions/leaf-classification/ernie55ernie/819581.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = SGDClassifier(loss=\"log\", penalty=\"l2\")\u001b[39m\n",
      "\u001b[32m+model = classifiers[1]\u001b[39m\n",
      " evaluate_model(model.__class__, model, train_x, train_y, test_x, test_y)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/2300814.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[32m+            weights, cache = ADAM(weights, x, y, outputs, alpha, beta1, beta2, eps, inter_adam, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/11562830.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.classifier.parameters(), lr=0.004, momentum=0.99)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.classifier.parameters(), lr=0.005, momentum=0.99)\u001b[39m\n",
      " #scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/1573486.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[32m+            weights, cache = ADAM(weights, x, y, outputs, alpha, beta1, beta2, eps, inter_adam, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/leaf-classification/ernie55ernie/831470.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    optimizer = SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\u001b[39m\n",
      "\u001b[32m+    optimizer = SGD(lr=0.5, momentum=0.0, decay=0.0, nesterov=False)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/dienhoa/4904448.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\u001b[39m\n",
      "\u001b[32m+  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/titanic/tanvibhandarkar/1494945.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = SGDClassifier(max_iter=8, tol=None)\u001b[39m\n",
      "\u001b[31m-clf.fit(X_train, y_train)\u001b[39m\n",
      "\u001b[31m-y_pred = clf.predict(X_test)\u001b[39m\n",
      "\u001b[32m+sgd_clf = SGDClassifier(max_iter=8, tol=None)\u001b[39m\n",
      "\u001b[32m+sgd_clf.fit(X_train, y_train)\u001b[39m\n",
      "\u001b[32m+y_pred = sgd_clf.predict(X_test)\u001b[39m\n",
      " print ('Accuracy: %.2f' % (accuracy_score(y_test, y_pred)))\n",
      "\u001b[34mdata/processed/competitions/leaf-classification/ernie55ernie/829639.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+    optimizer = SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\u001b[39m\n",
      "     # Let's train the model using RMSprop\n",
      " def construct_cnn_model():\n",
      "                   metrics=['accuracy'])\n",
      "     return model\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/11336436.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.classifier.parameters(), lr=0.04, momentum=0.99)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.classifier.parameters(), lr=0.004, momentum=0.99)\u001b[39m\n",
      " #scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats/sasukebansal/33430324.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, momentum=0.7, decay=0.00, nesterov=False)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.01, momentum=0.8, decay=0.00, nesterov=False)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/planet-understanding-the-amazon-from-space/saksham219/3404669.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model.compile(optimizer= SGD(lr = 1e-5, momentum = 0.9) , loss='binary_crossentropy', metrics = [f2_score])\u001b[39m\n",
      "\u001b[32m+model.compile(optimizer= Adam(lr=1e-5) , loss='binary_crossentropy', metrics = [f2_score])\u001b[39m\n",
      " model.fit_generator(train_generator, steps_per_epoch = 506, epochs = 10 , verbose = 1, validation_data = (fixed_val_imgs,fixed_val_labels) , \n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/curiousprogrammer/10298761.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    def SGD(self, X, Y, epochs, alpha, batch_size):\u001b[39m\n",
      "\u001b[32m+    def SGD(self, X, X_val, Y, Y_val, epochs, alpha, lambd, batch_size):\u001b[39m\n",
      "         J_history = []\n",
      "\u001b[32m+        J_val_history = []\u001b[39m\n",
      "         acc_history = []\n",
      "\u001b[32m+        acc_val_history = []\u001b[39m\n",
      "         core = DNN_Core(self.layer_dims)\n",
      " class SGD:\n",
      "         T = util.yEncode(Y)\n",
      "\u001b[32m+        T_val = util.yEncode(Y_val)\u001b[39m\n",
      "         parameters = core.initialise_parameters()\n",
      " class SGD:\n",
      "                 gradients = core.back_propogation(P, t, caches)\n",
      "\u001b[31m-                parameters = core.update_patameters(parameters, gradients, alpha)\u001b[39m\n",
      "\u001b[32m+                parameters = core.update_patameters(parameters, gradients, alpha, lambd, t.shape[0])\u001b[39m\n",
      "                 start = end\n",
      " class SGD:\n",
      "                 P, _ = core.feed_forward(X, parameters)\n",
      "\u001b[31m-                J = util.softmax_cost_function(P, T)\u001b[39m\n",
      "\u001b[32m+                P_val, _ = core.feed_forward(X_val, parameters)\u001b[39m\n",
      "\u001b[32m+                J = util.softmax_cost_function(P, T, parameters, lambd)\u001b[39m\n",
      "\u001b[32m+                J_val = util.softmax_cost_function(P_val, T_val, parameters, lambd)\u001b[39m\n",
      "                 J_history.append(J)\n",
      "\u001b[32m+                J_val_history.append(J_val)\u001b[39m\n",
      "                 acc = util.accuracy(P, Y)\n",
      "\u001b[32m+                acc_val = util.accuracy(P_val, Y_val)\u001b[39m\n",
      "                 acc_history.append(acc)\n",
      "\u001b[32m+                acc_val_history.append(acc_val)\u001b[39m\n",
      "         return P, parameters, J_history, acc_history\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/778461.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd4 = SGD(lr = .3, decay = 0.1, momentum = .95, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd4 = SGD(lr = 2, decay = 0.5, momentum = .95, nesterov = True)\u001b[39m\n",
      " classifier.compile(loss='categorical_crossentropy', optimizer = sgd4, metrics=['accuracy'])\n",
      "\u001b[34mdata/processed/competitions/titanic/nicapotato/2007646.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-grid = GridSearchCV(SGDClassifier(),\u001b[39m\n",
      "\u001b[32m+grid = GridSearchCV(SGDClassifier(max_iter=5, tol=None),\u001b[39m\n",
      "                     param_grid,cv=cv, scoring=scoring,\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/1567650.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[32m+            weights, cache = ADAM(weights, x, y, outputs, alpha, beta1, beta2, eps, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats/jypucca/467550.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = SGD(lr=1e-3)\u001b[39m\n",
      "\u001b[32m+optimizer = Adam()\u001b[39m\n",
      " #logloss-> binary_crossentropy\n",
      " def catdog():\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/ottpeterr/4518433.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.001, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      " model.compile(loss='categorical_crossentropy',\n",
      "\u001b[34mdata/processed/competitions/titanic/nicapotato/4064437.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-grid = GridSearchCV(SGDClassifier(),\u001b[39m\n",
      "\u001b[32m+grid = GridSearchCV(SGDClassifier(max_iter=5, tol=None),\u001b[39m\n",
      "                     param_grid,cv=cv, scoring=scoring,\n",
      "\u001b[34mdata/processed/competitions/state-farm-distracted-driver-detection/bharatsingh213/22624508.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt=SGD(lr=0.001)\u001b[39m\n",
      "\u001b[32m+opt = SGD()#lr=0.0001\u001b[39m\n",
      " # opt=adam()\n",
      "\u001b[31m-model.compile(optimizer=opt, loss=categorical_crossentropy, metrics=['acc'])\u001b[39m\n",
      "\u001b[32m+model.compile(optimizer=opt, loss=categorical_crossentropy, metrics=['acc'])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/12012675.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.classifier.parameters(), lr=0.005, momentum=0.99)\u001b[39m\n",
      "\u001b[31m-scheduler = ReduceLROnPlateau(optimizer, patience=2, factor=0.1, verbose=True)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.005, momentum=0.99)\u001b[39m\n",
      "\u001b[32m+exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/planet-understanding-the-amazon-from-space/saksham219/3404669.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model.compile(optimizer= SGD(lr = 1e-4, momentum = 0.9) , loss='binary_crossentropy', metrics = [f2_score])\u001b[39m\n",
      "\u001b[31m-model.fit_generator(train_generator, steps_per_epoch = 506, epochs = 7 , verbose = 1, validation_data = (fixed_val_imgs,fixed_val_labels) , \u001b[39m\n",
      "\u001b[32m+model.compile(optimizer= Adam(lr=1e-4) , loss='binary_crossentropy', metrics = [f2_score])\u001b[39m\n",
      "\u001b[32m+model.fit_generator(train_generator, steps_per_epoch = 506, epochs = 6 , verbose = 1, validation_data = (fixed_val_imgs,fixed_val_labels) , \u001b[39m\n",
      "                      callbacks = [checkpoint, reduce_lr], workers = 4)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/745270.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr = 0.5, decay = 0.15, momentum = .8, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr = 5, decay = 0.1, momentum = .8, nesterov = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/prostate-cancer-grade-assessment/nobletp/33962540.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = SGD(lr=0.001)#,momentum=0.9,decay=1e-4)\u001b[39m\n",
      "\u001b[31m-vgg_conv.compile(loss='categorical_crossentropy',optimizer=opt,metrics=[kappa_score])\u001b[39m\n",
      "\u001b[32m+opt = SGD(lr=0.001, momentum=0.9,decay=1e-4)\u001b[39m\n",
      "\u001b[32m+vgg_conv.compile(loss='binary_crossentropy',optimizer=opt,metrics=[kappa_score])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/alaska2-image-steganalysis/urayukitaka/33632556.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    opt = tf.keras.optimizers.SGD(learning_rate=0.00001)\u001b[39m\n",
      "\u001b[31m-    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\u001b[39m\n",
      "\u001b[32m+    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\u001b[39m\n",
      "     model.summary()\n",
      "\u001b[34mdata/processed/competitions/siim-isic-melanoma-classification/ibtesama/35150222.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = SGD(lr=0.001)\u001b[39m\n",
      "\u001b[32m+opt = Adam(lr=1e-5)\u001b[39m\n",
      " vgg_conv.compile(loss=focal_loss(), metrics=[tf.keras.metrics.AUC()],optimizer=opt)\n",
      "\u001b[34mdata/processed/competitions/whale-detection-challenge/overload10/10467934.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = optimizers.SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)\u001b[39m\n",
      "\u001b[31m-model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\u001b[39m\n",
      "\u001b[32m+model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/sasukebansal/33430324.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, momentum=0.7, decay=0.00, nesterov=False)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.01, momentum=0.8, decay=0.00, nesterov=False)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ga-customer-revenue-prediction/silverfoxdss/14893116.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd1 = SGDRegressor(alpha=0.01, fit_intercept=False, n_iter=200)\u001b[39m\n",
      "\u001b[32m+sgd1 = SGDRegressor(alpha=0.0001, fit_intercept=False, max_iter=100, early_stopping=True, n_iter_no_change=5, epsilon=0.1,\u001b[39m\n",
      "\u001b[32m+                   loss='squared_loss')\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/two-sigma-financial-news/johnmakgakga/35616316.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGDClassifier()\u001b[39m\n",
      "\u001b[32m+sgd = SGDClassifier(n_jobs=30,random_state=100)\u001b[39m\n",
      " sgd.fit(X_train, Y_train)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/ottpeterr/4504319.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.05, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      " model.compile(loss='categorical_crossentropy',\n",
      "\u001b[34mdata/processed/competitions/tgs-salt-identification-challenge/dingdiego/5340888.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[31m-model.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[\"accuracy\",iouMetric])\u001b[39m\n",
      "\u001b[32m+from keras.optimizers import SGD\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+model.compile(loss=bce_dice_loss, optimizer=sgd, metrics=[\"accuracy\",iouMetric])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/gaborfodor/31255924.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGDRegressor()\u001b[39m\n",
      "\u001b[32m+sgd = DecisionTreeRegressor()\u001b[39m\n",
      " rf = RandomForestRegressor(n_estimators = N_ESTIMATORS)\n",
      "\u001b[31m-models = [rf]\u001b[39m\n",
      "\u001b[32m+models = [sgd]\u001b[39m\n",
      " for model in models:\n",
      " for model in models:\n",
      "             train_data = train.loc[(train.ps==s) & (train.cr==c),['ps','cr','date']] \n",
      "\u001b[32m+            train_data = pd.get_dummies(train_data, columns = train_data.columns[1:4])\u001b[39m\n",
      "             t1 = train.loc[(train.ps==s) & (train.cr==c),'cases']\n",
      " for model in models:\n",
      "             ids = test.loc[(test.ps==s) & (test.cr==c),'ForecastId']\n",
      "\u001b[32m+            test_data = pd.get_dummies(test_data,  columns = test.columns[1:2])\u001b[39m\n",
      "             model1 = model\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/ottpeterr/4501844.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.05, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      " model.compile(loss='categorical_crossentropy',\n",
      "\u001b[34mdata/processed/competitions/getting-started/cassandraheide/31939912.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = tf.keras.optimizers.SGD(lr=0.0001)\u001b[39m\n",
      "\u001b[32m+opt = tf.keras.optimizers.Adam(lr=0.00001)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/santander-customer-transaction-prediction/plasticgrammer/11374387.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGDClassifier(max_iter=3000, loss='log', tol=1e-4)\u001b[39m\n",
      "\u001b[32m+sgd = SGDClassifier(max_iter=10000, loss='log', tol=1e-5)\u001b[39m\n",
      " sgd.fit(X_train, Y_train)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/2306130.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[32m+            weights, cache = ADAM(weights, x, y, outputs, alpha, beta1, beta2, eps, inter_adam, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/recognizing-faces-in-the-wild/imthiyagarajan/17683541.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model.parameters(),lr=0.01,momentum = 0.9)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model.parameters(),lr=0.008,momentum=0.8)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/flagma/3732479.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+optimizer = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      " model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
      "\u001b[34mdata/processed/competitions/inclusive-images-challenge/avni09/10680879.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[31m-model.compile(loss='categorical_crossentropy',optimizer=sgd)\u001b[39m\n",
      "\u001b[32m+model.compile(loss='categorical_crossentropy',optimizer=Adam)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/1421940.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            model = SGD(model, x, y, outputs, alpha, momentum, l2)\u001b[39m\n",
      "\u001b[32m+            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/freesound-audio-tagging-2019/ashirahama/12868866.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optimizers.SGD(lr=0.003, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      " # optimizer = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/746582.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr = 5, decay = 0.1, momentum = .8, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr = 5, decay = 0.1, momentum = .85, nesterov = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/humpback-whale-identification/mohammedsunasra/10307141.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model.fc.parameters(), lr=0.1)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\u001b[39m\n",
      " criterion = nn.CrossEntropyLoss()\n",
      "\u001b[34mdata/processed/competitions/prostate-cancer-grade-assessment/ibtesama/33857550.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = SGD(lr=0.001,momentum=0.9,decay=1e-4)\u001b[39m\n",
      "\u001b[32m+opt = SGD(lr=0.001)\u001b[39m\n",
      " vgg_conv.compile(loss='categorical_crossentropy',optimizer=opt,metrics=[kappa_score])\n",
      "\u001b[34mdata/processed/competitions/new-york-city-taxi-fare-prediction/yairhadad1/12571246.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-SGD_Score_train = np.sqrt(mean_absolute_error(y_train, clf_SGD.predict(X_train)))\u001b[39m\n",
      "\u001b[31m-SGD_Score_test = np.sqrt(mean_absolute_error(y_test, clf_SGD.predict(X_test)))\u001b[39m\n",
      "\u001b[32m+SGD_Score_train = np.sqrt(mean_squared_error(y_train, reg_SGD.predict(X_train)))\u001b[39m\n",
      "\u001b[32m+SGD_Score_test = np.sqrt(mean_squared_error(y_test, reg_SGD.predict(X_test)))\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/1592481.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[32m+            weights, cache = ADAM(weights, x, y, outputs, alpha, beta1, beta2, eps, inter_adam, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/zillow-prize-1/gowtamsingulur/30723305.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = linear_model.SGDRegressor(loss='huber',max_iter=1000, tol=1e-3)\u001b[39m\n",
      "\u001b[32m+clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\u001b[39m\n",
      " clf.fit(data_train_with_conf.todense(), df_train['Fatalities'])\n",
      "\u001b[34mdata/processed/competitions/tpu-getting-started/cassandraheide/31939912.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = tf.keras.optimizers.SGD(lr=0.0001)\u001b[39m\n",
      "\u001b[32m+opt = tf.keras.optimizers.Adam(lr=0.00001)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/7915799.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.1, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.1, momentum=0.99)\u001b[39m\n",
      " #scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/ottpeterr/4488826.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True)\u001b[39m\n",
      " model.compile(loss='categorical_crossentropy',\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats-redux-kernels-edition/suniliitb96/6132871.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[31m-my_new_model.compile(optimizer=sgd, loss=OBJECTIVE_FUNCTION, metrics=LOSS_METRICS)\u001b[39m\n",
      "\u001b[32m+sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\u001b[39m\n",
      "\u001b[32m+my_new_model.compile(optimizer = sgd, loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ghouls-goblins-and-ghosts-boo/iamcylee/9793002.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  sgd_clf = SGDClassifier(max_iter=5, random_state=42)\u001b[39m\n",
      "\u001b[32m+  sgd_clf = SGDClassifier(max_iter=1000, random_state=42)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/mercedes-benz-greener-manufacturing/budhiraja/1255426.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-regr = linear_model.SGDRegressor(n_iter = 10)\u001b[39m\n",
      "\u001b[32m+regr = linear_model.LassoLarsCV()\u001b[39m\n",
      " pca = decomposition.PCA()\n",
      "\u001b[34mdata/processed/competitions/human-protein-atlas-image-classification/nikhilpandey360/8629702.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-from keras.optimizers import SGD\u001b[39m\n",
      " model = create_model(\n",
      "\u001b[31m-    input_shape=(299,299,3), \u001b[39m\n",
      "\u001b[32m+    input_shape, \u001b[39m\n",
      "     n_out=28)\n",
      " model.compile(\n",
      "     loss='binary_crossentropy',  \n",
      "\u001b[31m-    optimizer=SGD(lr = 0.0001, momentum=0.9, nesterov=True),\u001b[39m\n",
      "\u001b[32m+    optimizer=Adam(1e-4),\u001b[39m\n",
      "     metrics=['acc', f1])\n",
      " history = model.fit_generator(\n",
      "     validation_data=next(validation_generator),\n",
      "\u001b[31m-    epochs=15, \u001b[39m\n",
      "\u001b[32m+    epochs=10, \u001b[39m\n",
      "     verbose=1,\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/markdavey/5379679.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model = SGDClassifier()\u001b[39m\n",
      "\u001b[32m+model = RandomForestClassifier()\u001b[39m\n",
      " model.fit(x_train,y_train)\n",
      "\u001b[34mdata/processed/competitions/two-sigma-financial-modeling/fernandocanteruccio/915972.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    optm = SGD(lr=0.003, momentum=0.9, decay=0.0, nesterov=True)\u001b[39m\n",
      "\u001b[32m+    optm = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=True)\u001b[39m\n",
      "     model.compile(loss='mse',\n",
      "\u001b[34mdata/processed/competitions/facebook-v-predicting-check-ins/zrythpzhl/523659.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = linear_model.SGDClassifier()\u001b[39m\n",
      "\u001b[32m+clf = KNeighborsClassifier(n_neighbors = 5)\u001b[39m\n",
      " #clf = GradientBoostingClassifier()\n",
      "\u001b[34mdata/processed/competitions/prostate-cancer-grade-assessment/ibtesama/33850274.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = SGD(lr=0.001)\u001b[39m\n",
      "\u001b[32m+opt = SGD(lr=0.001,momentum=0.9,decay=1e-4)\u001b[39m\n",
      " vgg_conv.compile(loss='categorical_crossentropy',optimizer=opt,metrics=[kappa_score])\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats/myndel/29403959.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      " model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/jcodogno/2837492.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\u001b[39m\n",
      "\u001b[32m+            weights, cache = ADAM(weights, x, y, outputs, alpha, beta1, beta2, eps, inter_adam, l2, cache)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/mohanamurali/11612786.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr=3.5e-4, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr=2e-3, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      " model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['acc'])\n",
      "\u001b[34mdata/processed/competitions/deepfake-detection-challenge/kimyoh/29381539.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd,momentum=0.9,nesterov=True)\u001b[39m\n",
      "\u001b[32m+optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/11316062.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.0004, momentum=0.99)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.classifier.parameters(), lr=0.04, momentum=0.99)\u001b[39m\n",
      " #scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')\n",
      "\u001b[34mdata/processed/competitions/leaf-classification/ernie55ernie/831529.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    optimizer = SGD(lr=0.5, momentum=0.0, decay=0.0, nesterov=False)\u001b[39m\n",
      "\u001b[32m+    optimizer = SGD(lr=0.8, momentum=0.0, decay=0.0, nesterov=False)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/planet-understanding-the-amazon-from-space/prateekjha/15691425.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+  optimizer = optim.Adam(model.parameters(), lr=0.003)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/ghouls-goblins-and-ghosts-boo/alitvin/457435.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    sgd = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True)\u001b[39m\n",
      "\u001b[31m-    model.compile(optimizer = sgd, loss='mae', metrics=['accuracy'])\u001b[39m\n",
      "\u001b[32m+    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/two-sigma-financial-modeling/fernandocanteruccio/916394.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    optm = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=True)\u001b[39m\n",
      "\u001b[32m+    optm = SGD(lr=0.1, momentum=0.3, decay=0.0, nesterov=True)\u001b[39m\n",
      "     model.compile(loss='mse',\n",
      "\u001b[34mdata/processed/competitions/dog-breed-identification/igorslima/5819075.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = SGD()\u001b[39m\n",
      "\u001b[31m-model_sgd.compile(optimizer=opt,\u001b[39m\n",
      "\u001b[32m+model.compile(optimizer='sgd',\u001b[39m\n",
      "               loss='categorical_crossentropy', \n",
      " model_sgd.compile(optimizer=opt,\n",
      " callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
      "\u001b[31m-model_sgd.summary()\u001b[39m\n",
      "\u001b[32m+model.summary()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/Kannada-MNIST/antoninavertinskaya/23480926.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model.compile(optimizer=tf.keras.optimizers.SGD(),\u001b[39m\n",
      "\u001b[32m+model = Model(inputs=[x], output=[prediction])\u001b[39m\n",
      "\u001b[32m+model.compile(optimizer=keras.optimizers.SGD(),\u001b[39m\n",
      "               loss='sparse_categorical_crossentropy',\n",
      "\u001b[34mdata/processed/competitions/dogs-vs-cats/suniliitb96/6132871.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[31m-my_new_model.compile(optimizer=sgd, loss=OBJECTIVE_FUNCTION, metrics=LOSS_METRICS)\u001b[39m\n",
      "\u001b[32m+sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\u001b[39m\n",
      "\u001b[32m+my_new_model.compile(optimizer = sgd, loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/flower-classification-with-tpus/cassandraheide/31939912.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = tf.keras.optimizers.SGD(lr=0.0001)\u001b[39m\n",
      "\u001b[32m+opt = tf.keras.optimizers.Adam(lr=0.00001)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/artgor/7493525.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model_conv.parameters(), lr=0.01, momentum=0.9)\u001b[39m\n",
      "\u001b[31m-exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.SGD(model_conv.parameters(), lr=0.1, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/dog-breed-identification/igorslima/5607600.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = SGD(lr=10e-5)\u001b[39m\n",
      "\u001b[32m+opt = RMSprop()\u001b[39m\n",
      " model.compile(optimizer=opt,\n",
      "\u001b[34mdata/processed/competitions/facebook-ii/zrythpzhl/523659.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-clf = linear_model.SGDClassifier()\u001b[39m\n",
      "\u001b[32m+clf = KNeighborsClassifier(n_neighbors = 5)\u001b[39m\n",
      " #clf = GradientBoostingClassifier()\n",
      "\u001b[34mdata/processed/competitions/imet-2019-fgvc6/itwice/14298313.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        optimizer = optim.SGD(params=model.parameters(), \u001b[39m\n",
      "\u001b[31m-                                lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=True)\u001b[39m\n",
      "\u001b[31m-        scheduler = CosineAnnealingLR(optimizer, eta_min=1e-4, T_max=n_epochs)\u001b[39m\n",
      "\u001b[32m+        optimizer = optim.Adam(params=model.parameters(), \u001b[39m\n",
      "\u001b[32m+                                lr=0.0001)\u001b[39m\n",
      "\u001b[32m+        scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs)\u001b[39m\n",
      "         best_score = np.inf\n",
      " class Trainer:\n",
      "                 y_pred = model(i_batch)\n",
      "\u001b[32m+                y_batch, y_pred = data_generator.mix_up(y_batch, y_pred)\u001b[39m\n",
      "                 loss = self.loss_fn(y_pred, y_batch)\n",
      " class Trainer:\n",
      "             with torch.no_grad():\n",
      "\u001b[31m-                y_pred = model(i_batch)#.detach()\u001b[39m\n",
      "\u001b[32m+                y_pred = model(i_batch).detach()\u001b[39m\n",
      "                 avg_val_loss += self.loss_fn(y_pred, y_batch).item() / len(loader)\n",
      " class Trainer:\n",
      "                 model.load_state_dict(torch.load(path))\n",
      "\u001b[32m+                model.to(self.device)\u001b[39m\n",
      "                 model.eval()\n",
      " class Trainer:\n",
      "                     with torch.no_grad():\n",
      "\u001b[31m-                        y_pred = model(i_batch)#.detach()\u001b[39m\n",
      "\u001b[32m+                        y_pred = model(i_batch).detach()\u001b[39m\n",
      "                         temp[i * self.valid_batch:(i + 1) * self.valid_batch] = \\\n",
      "\u001b[34mdata/processed/competitions/flower-classification-with-tpus/volody/29135572.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-opt = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[32m+opt = tf.keras.optimizers.SGD(lr=0.01, \u001b[39m\n",
      "\u001b[32m+                              decay=1e-6, \u001b[39m\n",
      "\u001b[32m+                              momentum=0.9, \u001b[39m\n",
      "\u001b[32m+                              nesterov=True)\u001b[39m\n",
      " model.compile(\n",
      " model.compile(\n",
      " )\n",
      "\u001b[31m-model.summary()()\u001b[39m\n",
      "\u001b[32m+model.summary()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/746582.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd2 = SGD(lr = 0.001, decay = 0, momentum = .95, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd2 = SGD(lr = 0.3, decay = 0.1, momentum = .95, nesterov = True)\u001b[39m\n",
      " classifier.compile(loss='categorical_crossentropy', optimizer = sgd2, metrics=['accuracy'])\n",
      "\u001b[34mdata/processed/competitions/denoising-dirty-documents/phylake1337/30077738.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\u001b[39m\n",
      "\u001b[31m-rms = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\u001b[39m\n",
      "\u001b[31m-ada = keras.optimizers.Adagrad(learning_rate=0.01)\u001b[39m\n",
      "\u001b[31m-model.compile(optimizer = 'adam' , loss = \"mean_squared_error\")\u001b[39m\n",
      "\u001b[32m+autoencoder = create_model()\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/745270.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-sgd = SGD(lr = 0.2, decay = 0.15, momentum = .9, nesterov = False)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr = 0.5, decay = 0.15, momentum = .8, nesterov = True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/mercari-price-suggestion-challenge/solomonk/1784606.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+optimizer = optim.Adam(model.parameters(), lr=learning_rate)\u001b[39m\n",
      " print (optimizer)\n",
      "\u001b[34mdata/processed/competitions/humpback-whale-identification/dumavit1/14475636.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-optimizer = torch.optim.SGD(params=model.parameters(), lr=0.005, momentum=0.9)\u001b[39m\n",
      "\u001b[32m+optimizer = torch.optim.SGD(params=model.parameters(), lr=0.007, momentum=0.9)\u001b[39m\n",
      " lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.4)\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*SGD\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/33634205.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_config = transformers.AlbertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      " model_config.output_hidden_states = True\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/viswajithkn/32086267.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+    model_config = transformers.RobertaConfig.from_pretrained(config.roberta_path)\u001b[39m\n",
      "     model = BertBaseQA(768, 2,model_config).to(device)\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/bamps53/30818481.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTest:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTest:\n",
      "         self.max_length = max_length\n",
      "\u001b[32m+        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30754299.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTest:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTest:\n",
      "         self.max_length = max_length\n",
      "\u001b[32m+        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30801337.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32137015.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      " model_config.output_hidden_states = True\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/rsmits/35611667.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    roberta_model = TFRobertaForQuestionAnswering.from_pretrained(ROBERTA_BASE_PATH + 'pretrained-roberta-base.h5', config = config)\u001b[39m\n",
      "\u001b[31m-    x = roberta_model(ids, attention_mask = att, token_type_ids = tok)\u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Dropout(0.30)(x[0]) \u001b[39m\n",
      "\u001b[31m-    x1 = tf.keras.layers.Activation('softmax')(x1)\u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Dropout(0.30)(x[1]) \u001b[39m\n",
      "\u001b[31m-    x2 = tf.keras.layers.Activation('softmax')(x2)\u001b[39m\n",
      "\u001b[32m+        roberta_model = TFRobertaForQuestionAnswering.from_pretrained(ROBERTA_BASE_PATH + 'pretrained-roberta-base.h5', config = config)\u001b[39m\n",
      "\u001b[32m+        x = roberta_model(ids, attention_mask = att, token_type_ids = tok)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32114189.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "     model_config.output_hidden_states = True\n",
      " def run_fold(k):\n",
      "             {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lrr},\n",
      "\u001b[31m-            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\":lrr * 500},\u001b[39m\n",
      "\u001b[32m+            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\":lrr * 100},\u001b[39m\n",
      "         ]\n",
      " def run_fold(k):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30795231.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/iezepov/16118905.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-UNCASED_BERT_TOKENIZER = BertTokenizer.from_pretrained(UNCASED_BERT_MODEL_PATH, do_lower_case=True)\u001b[39m\n",
      "\u001b[31m-CASED_BERT_TOKENIZER = BertTokenizer.from_pretrained(CASED_BERT_MODEL_PATH, do_lower_case=False)\u001b[39m\n",
      " def clip_to_max_len(batch):\n",
      " def prepare_bert(bin_file, is_cased):\n",
      "     model = model.cuda()\n",
      "\u001b[31m-    return model\u001b[39m\n",
      "\u001b[32m+    return model\u001b[39m\n",
      "\u001b[32m+def prepare_tokenizer(is_cased):\u001b[39m\n",
      "\u001b[32m+    if is_cased:\u001b[39m\n",
      "\u001b[32m+        return BertTokenizer.from_pretrained(CASED_BERT_MODEL_PATH, do_lower_case=False)\u001b[39m\n",
      "\u001b[32m+    else:\u001b[39m\n",
      "\u001b[32m+        return BertTokenizer.from_pretrained(UNCASED_BERT_MODEL_PATH, do_lower_case=True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32048085.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+    model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "     model_config.output_hidden_states = True\n",
      " def run_fold(k):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/kaushikvit/32942829.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        ids = ids + ([0] * padding_length)#We padd it on the right for BERT\u001b[39m\n",
      "\u001b[32m+        ids = ids + ([0] * padding_length)#We pad it on the right for BERT as its a model with absolute position embeddings\u001b[39m\n",
      "         mask = mask + ([0] * padding_length)\n",
      " class BERTDatasetTraining:\n",
      "             'mask': torch.tensor(mask, dtype=torch.long),\n",
      "\u001b[31m-            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\u001b[39m\n",
      "\u001b[31m-            'targets': torch.tensor(self.targets[item], dtype=torch.float)\u001b[39m\n",
      "\u001b[32m+            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\u001b[39m\n",
      "         }\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/mgornergoogle/31280757.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    bert_layer = tf.saved_model.load(BERT_GCS_PATH_SAVEDMODEL)\u001b[39m\n",
      "\u001b[31m-    bert_layer = hub.KerasLayer(bert_layer, trainable=trainable_bert)\u001b[39m\n",
      "\u001b[32m+    bert_layer = tf.saved_model.load(BERT_GCS_PATH)  # copy of TF Hub model 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'\u001b[39m\n",
      "\u001b[32m+    bert_layer = hub.KerasLayer(bert_layer, trainable=True)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/35231388.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[31m-    model_config.output_hidden_states = True\u001b[39m\n",
      "\u001b[31m-    model = BERTBaseUncased(conf=model_config)\u001b[39m\n",
      "\u001b[32m+    model = TweetModel()\u001b[39m\n",
      "     params = list(model.named_parameters())\n",
      "\u001b[31m-    def is_backbone(n):\u001b[39m\n",
      "\u001b[31m-        return \"bert\" in n\u001b[39m\n",
      "\u001b[31m-    lrr=2e-5\u001b[39m\n",
      "\u001b[31m-    optimizer_grouped_parameters = [\u001b[39m\n",
      "\u001b[31m-            {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lrr},\u001b[39m\n",
      "\u001b[31m-            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\":lrr * 500},\u001b[39m\n",
      "\u001b[31m-        ]\u001b[39m\n",
      "\u001b[31m-    optimizer = torch.optim.AdamW(\u001b[39m\n",
      "\u001b[31m-            optimizer_grouped_parameters, lr=lrr, weight_decay=0\u001b[39m\n",
      "\u001b[31m-        )\u001b[39m\n",
      "\u001b[32m+    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\u001b[39m\n",
      "     #cbfs = [Recorder,partial(AvgStatsCallback,metric),CudaCallback,ProgressCallback,partial(EarlyStopingCallback,1)]\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30795972.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTest:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTest:\n",
      "         self.max_length = max_length\n",
      "\u001b[32m+        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/kaushikvit/32960106.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        ids = ids + ([0] * padding_length)#We padd it on the right for BERT\u001b[39m\n",
      "\u001b[32m+        ids = ids + ([0] * padding_length)#We pad it on the right for BERT as its a model with absolute position embeddings\u001b[39m\n",
      "         mask = mask + ([0] * padding_length)\n",
      " class BERTDatasetTraining:\n",
      "             'mask': torch.tensor(mask, dtype=torch.long),\n",
      "\u001b[31m-            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\u001b[39m\n",
      "\u001b[31m-            'targets': torch.tensor(self.targets[item], dtype=torch.float)\u001b[39m\n",
      "\u001b[32m+            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\u001b[39m\n",
      "         }\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/vaibhavbhandari2999/32849787.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30800109.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTest:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTest:\n",
      "         self.max_length = max_length\n",
      "\u001b[32m+        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/divyamdj/31658986.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32114189.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      " model_config.output_hidden_states = True\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30754936.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32713139.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.bert = transformers.RobertaModel.from_pretrained(config.BERT_PATH,config=conf)#transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\u001b[39m\n",
      "\u001b[31m-        self.big_dropout = nn.Dropout(0.1)\u001b[39m\n",
      "\u001b[31m-        self.small_dropout = nn.Dropout(0.1)\u001b[39m\n",
      "\u001b[32m+        self.bert = transformers.AlbertModel.from_pretrained(config.BERT_PATH,config=model_config)#transformers.RobertaModel.from_pretrained(config.BERT_PATH,config=conf)#transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\u001b[39m\n",
      "\u001b[32m+        self.big_dropout = nn.Dropout(0.2)\u001b[39m\n",
      "         #ATTENTION ------------------------\n",
      "\u001b[31m-        n_weights = 13\u001b[39m\n",
      "\u001b[31m-        weights_init = torch.zeros(n_weights).float()\u001b[39m\n",
      "\u001b[31m-        weights_init.data[:-1] = -3\u001b[39m\n",
      "\u001b[31m-        self.layer_weights = torch.nn.Parameter(weights_init)\u001b[39m\n",
      "         self.l0 = nn.Linear(768 , 2)\n",
      " class BERTBaseUncased(transformers.BertPreTrainedModel):\n",
      "     def forward(self, x):\n",
      "\u001b[31m-        _,out1,out= self.bert(\u001b[39m\n",
      "\u001b[32m+        rrr,out1,out= self.bert(\u001b[39m\n",
      "             x['ids'],\n",
      " class BERTBaseUncased(transformers.BertPreTrainedModel):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30798902.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32713139.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+model_config = transformers.AlbertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      " model_config.output_hidden_states = True\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/33364846.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.bert = transformers.AlbertModel.from_pretrained(config.BERT_PATH,config=model_config)#transformers.RobertaModel.from_pretrained(config.BERT_PATH,config=conf)#transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\u001b[39m\n",
      "\u001b[32m+        self.bert = transformers.RobertaModel.from_pretrained(config.BERT_PATH,config=model_config)#transformers.RobertaModel.from_pretrained(config.BERT_PATH,config=conf)#transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\u001b[39m\n",
      "         self.big_dropout = nn.Dropout(0.2)\n",
      " class BERTBaseUncased(transformers.BertPreTrainedModel):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/iezepov/16116174.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[32m+UNCASED_BERT_TOKENIZER = BertTokenizer.from_pretrained(UNCASED_BERT_MODEL_PATH, do_lower_case=True)\u001b[39m\n",
      "\u001b[32m+CASED_BERT_TOKENIZER = BertTokenizer.from_pretrained(CASED_BERT_MODEL_PATH, do_lower_case=False)\u001b[39m\n",
      " def clip_to_max_len(batch):\n",
      " def prepare_bert(bin_file, is_cased):\n",
      "     model = model.cuda()\n",
      "\u001b[31m-    return model\u001b[39m\n",
      "\u001b[31m-def prepare_tokenizer(is_cased):\u001b[39m\n",
      "\u001b[31m-    if is_cased:\u001b[39m\n",
      "\u001b[31m-        return BertTokenizer.from_pretrained(CASED_BERT_MODEL_PATH, do_lower_case=False)\u001b[39m\n",
      "\u001b[31m-    else:\u001b[39m\n",
      "\u001b[31m-        return BertTokenizer.from_pretrained(UNCASED_BERT_MODEL_PATH, do_lower_case=True)\u001b[39m\n",
      "\u001b[32m+    return model\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/divyamdj/31651909.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTest:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTest:\n",
      "         self.max_length = max_length\n",
      "\u001b[32m+        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/vaibhavbhandari2999/32868133.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/viswajithkn/31567725.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model = BertBaseQA(BERT_TYPE, 768, 2).to(device)\u001b[39m\n",
      "\u001b[32m+    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+    model = BertBaseQA(768, 2,model_config).to(device)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30750066.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/hiromoon166/36518835.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-            out['ids'][i] = (tokenized_text + [1]    *(max_pad - text_len))[:max_pad] # RoBERTaはpaddingのトークン番号が1\u001b[39m\n",
      "\u001b[32m+            out['ids'][i]            = (tokenized_text + [1]    *(max_pad - text_len))[:max_pad]\u001b[39m\n",
      "             out['token_type_ids'][i] = (token_type_ids + [0]    *(max_pad - text_len))[:max_pad]\n",
      " class RerankingCollate:\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32006628.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      " model_config.output_hidden_states = True\n",
      "\u001b[34mdata/processed/competitions/jigsaw-unintended-bias-in-toxicity-classification/takumiito/19061923.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        self.linear_out = nn.Linear(BERT_HIDDEN_SIZE, 1)\u001b[39m\n",
      "\u001b[31m-        nn.init.xavier_uniform_(self.linear_out.weight)\u001b[39m\n",
      "\u001b[31m-        self.linear_aux_out = nn.Linear(BERT_HIDDEN_SIZE, num_aux_targets)\u001b[39m\n",
      "\u001b[31m-        nn.init.xavier_uniform_(self.linear_aux_out.weight)\u001b[39m\n",
      "\u001b[32m+        self.before_linear = nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE)\u001b[39m\n",
      "\u001b[32m+        self.before_linear2 = nn.Linear(BERT_HIDDEN_SIZE, 50)\u001b[39m\n",
      "\u001b[32m+        self.linear_out = nn.Linear(50, 1)\u001b[39m\n",
      "\u001b[32m+        self.linear_aux_out = nn.Linear(50, num_aux_targets)\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/divyamdj/31651361.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTest:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTraining:\n",
      "         self.max_length = max_length\n",
      "\u001b[31m-        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/kaushikvit/32956916.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        ids = ids + ([0] * padding_length)#We pad it on the right for BERT as its a model with absolute position embeddings\u001b[39m\n",
      "\u001b[32m+        ids = ids + ([0] * padding_length)#We padd it on the right for BERT\u001b[39m\n",
      "         mask = mask + ([0] * padding_length)\n",
      " class BERTDatasetTest:\n",
      "             'mask': torch.tensor(mask, dtype=torch.long),\n",
      "\u001b[31m-            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\u001b[39m\n",
      "\u001b[32m+            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\u001b[39m\n",
      "\u001b[32m+            'targets': torch.tensor(self.targets[item], dtype=torch.float)\u001b[39m\n",
      "         }\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/abhishek/30758595.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-class BERTDatasetTest:\u001b[39m\n",
      "\u001b[31m-    def __init__(self, comment_text, tokenizer, max_length):\u001b[39m\n",
      "\u001b[32m+class BERTDatasetTraining:\u001b[39m\n",
      "\u001b[32m+    def __init__(self, comment_text, targets, tokenizer, max_length):\u001b[39m\n",
      "         self.comment_text = comment_text\n",
      " class BERTDatasetTest:\n",
      "         self.max_length = max_length\n",
      "\u001b[32m+        self.targets = targets\u001b[39m\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/drhouse3/32137015.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "\u001b[32m+    model_config = transformers.RobertaConfig.from_pretrained(config.BERT_PATH)\u001b[39m\n",
      "     model_config.output_hidden_states = True\n",
      " def run_fold(k):\n",
      "             {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lrr},\n",
      "\u001b[31m-            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\":lrr * 100},\u001b[39m\n",
      "\u001b[32m+            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\":lrr * 500},\u001b[39m\n",
      "         ]\n",
      " def run_fold(k):\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/vaibhavbhandari2999/32868133.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-        ids = ids + ([0] * padding_length)#We padd it on the right for BERT\u001b[39m\n",
      "\u001b[32m+        ids = ids + ([0] * padding_length)#We pad it on the right for BERT as its a model with absolute position embeddings\u001b[39m\n",
      "         mask = mask + ([0] * padding_length)\n",
      " class BERTDatasetTraining:\n",
      "             'mask': torch.tensor(mask, dtype=torch.long),\n",
      "\u001b[31m-            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\u001b[39m\n",
      "\u001b[31m-            'targets': torch.tensor(self.targets[item], dtype=torch.float)\u001b[39m\n",
      "\u001b[32m+            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\u001b[39m\n",
      "         }\n"
     ]
    }
   ],
   "source": [
    "show_diffs_that_match_regex(consolidated_examples, \"^[\\+-].*BERT\", limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, do we want to keep positional args afterall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIONAL_ARGS_PATH=\"~/RobustDataScience/data/processed/filtered_diffs_with_args.jsonl\"\n",
    "positional_args_examples = !shuf $POSITIONAL_ARGS_PATH\n",
    "positional_args_examples = [json.loads(x) for x in positional_args_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuf: ./data/processed/filtered_diffs_with_args.jsonl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!shuf $POSITIONAL_ARGS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diffs_that_match_regex(positional_args_examples, \"^[\\+-].*ttest_ind\", limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using keywords from libraries\n",
    "The following examples use libraries mined with `/homes/gws/mikeam/RobustDataScience/src/data/scrape_library_structures.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING_LIBRARY_ARGS = \"/homes/gws/mikeam/RobustDataScience/data/processed/filtered_with_lib_structure.txt\"\n",
    "using_lib_examples = !shuf $USING_LIBRARY_ARGS\n",
    "using_lib_examples = [json.loads(x) for x in using_lib_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata/processed/competitions/microsoft-malware-prediction/adityaecdrid/8717145.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-train['first_4'] = train['MachineIdentifier'].apply(lambda x: x[:4])\u001b[39m\n",
      "\u001b[32m+train['first_4'] = train['MachineIdentifier'].apply(lambda x: x[:4]).astype('category')\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[31m-test['first_4'] = test['MachineIdentifier'].apply(lambda x: x[:4])\u001b[39m\n",
      "\u001b[32m+test['first_4'] = test['MachineIdentifier'].apply(lambda x: x[:4]).astype('category')\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/pubg-finish-placement-prediction/praneethvarmaalluri/8130448.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " config = tf.contrib.learn.RunConfig(tf_random_seed=42)\n",
      " feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(x_train_squad_fpp_scaled)\n",
      "\u001b[31m-dnn_reg_squad_fpp = tf.contrib.learn.DNNRegressor(hidden_units=[2500,2500,2500,1500,1000],optimizer = tf.train.AdamOptimizer(), activation_fn = tf.nn.relu, feature_columns=feature_columns, config=config)\u001b[39m\n",
      "\u001b[32m+dnn_reg_squad_fpp = tf.contrib.learn.DNNRegressor(hidden_units=[500, 500], activation_fn = tf.nn.relu, feature_columns=feature_columns, config=config)\u001b[39m\n",
      " dnn_reg_squad_fpp = tf.contrib.learn.SKCompat(dnn_reg_squad_fpp) # to be compatible with sklearn\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/titanic/dejavu23/8597099.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " scaler = StandardScaler()\n",
      " # for df_train_ml\n",
      "\u001b[31m-scaler.fit(df_train_ml.drop('Survived',axis=1))\u001b[39m\n",
      "\u001b[31m-scaled_features = scaler.transform(df_train_ml.drop('Survived',axis=1))\u001b[39m\n",
      "\u001b[32m+scaler.fit(df_train_ml.drop(['Survived'],axis=1))\u001b[39m\n",
      "\u001b[32m+scaled_features = scaler.transform(df_train_ml.drop(['Survived'],axis=1))\u001b[39m\n",
      " \n",
      " df_train_ml_sc = pd.DataFrame(scaled_features, columns=df_train_ml.columns[:-1])\n",
      " df_test_ml.fillna(df_test_ml.mean(), inplace=True)\n",
      " scaled_features = scaler.transform(df_test_ml)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/titanic/longyin2/1423623.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31m-  test['Fare'].fillna(train['Fare'].mean(), inplace = True) \u001b[39m\n",
      "\u001b[32m+  test['Fare'].fillna(train['Fare'].mean(), inplace = True)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/bgeier/9549515.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def capsule():\n",
      " \n",
      "\u001b[31m-    x = Capsule(num_capsule=15, dim_capsule=5, routings=3, share_weights=True)(x)\u001b[39m\n",
      "\u001b[32m+    x = Capsule(num_capsule=15, dim_capsule=5, routings=4, share_weights=True)(x)\u001b[39m\n",
      "     x = Flatten()(x)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/muhammetcakmak/35633998.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # Confusion matrix and Classification report\n",
      "\u001b[31m-pred1 =  clf.predict(X_test_df)\u001b[39m\n",
      "\u001b[31m-fpr, tpr, thresholds = metrics.roc_curve(y_test_df, pred1, pos_label=2)\u001b[39m\n",
      "\u001b[32m+pred1 = model.predict(X_test1)\u001b[39m\n",
      "\u001b[32m+fpr, tpr, thresholds = metrics.roc_curve(y_test1, pred1, pos_label=2)\u001b[39m\n",
      " metrics.auc(fpr, tpr)\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/santander-customer-transaction-prediction/frtgnn/11014445.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-folds = StratifiedKFold(n_splits=9, shuffle=True, random_state=31415)\u001b[39m\n",
      "\u001b[32m+folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=31415)\u001b[39m\n",
      " oof = np.zeros(len(train))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/flower-classification-with-tpus/akshat4112/32223428.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " with strategy.scope():\n",
      "\u001b[31m-    pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\u001b[39m\n",
      "\u001b[32m+    pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\u001b[39m\n",
      "     pretrained_model.trainable = False # False = transfer learning, True = fine-tuning\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/trackml-particle-identification/asalzburger/3535453.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,2 @@\n",
      " delta_r_global_hit = np.sqrt(direction_global_hit[0][0]*direction_global_hit[0][0]+direction_global_hit[1][0]*direction_global_hit[1][0])\n",
      "\n",
      "\n",
      "\u001b[31m-cluster_length_v_hit = 2.*module.module_t.data[0]/np.tan(theta_global_hit)\u001b[39m\n",
      "\u001b[32m+cluster_length_v_hit = np.abs(2.*module.module_t.data[0]/np.tan(theta_global_hit))\u001b[39m\n",
      " cluster_size_v_hit = cluster_length_v_hit/module.pitch_v.data[0]\n",
      "\n",
      " def extract_rotation_matrix(module) :\n",
      "                              [ module.rot_zu.data[0], module.rot_zv.data[0], module.rot_zw.data[0]]])\n",
      "     return rot_matrix, np.linalg.inv(rot_matrix)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/sumitai/30573676.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # combine Country region and Province state: and avoide NAN values \n",
      "\u001b[31m-train_csv['combine state'] = train_csv['Country/Region'].fillna(':') + str(' ') +train_csv['Province/State'].fillna('')\u001b[39m\n",
      "\u001b[32m+train_csv['combine state'] = train_csv['Country/Region'].fillna('') + str(': ') +train_csv['Province/State'].fillna('')\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/digit-recognizer/baogorek/742830.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-sgd = SGD(lr = 0.1, decay = 0, momentum = .9, nesterov = True)\u001b[39m\n",
      "\u001b[32m+sgd = SGD(lr = 0.01, decay = 0, momentum = .9, nesterov = True)\u001b[39m\n",
      " \n",
      "@@ -14,2 +14 @@ sgd2 = SGD(lr = 0.001, decay = 0, momentum = .95, nesterov = True)\n",
      " classifier.compile(loss='categorical_crossentropy', optimizer = sgd2, metrics=['accuracy'])\n",
      "\n",
      "\n",
      "\u001b[31m-classifier.fit(training_inputs, training_targets[0:20000],\u001b[39m\n",
      "\u001b[32m+classifier.fit(training_inputs, training_targets,\u001b[39m\n",
      "                 nb_epoch = 3, batch_size = 512,\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/bosch-production-line-performance/joconnor/365665.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-clf = XGBClassifier(max_depth=5, base_score=0.005)\u001b[39m\n",
      "\u001b[32m+clf = XGBClassifier(max_depth=4, base_score=0.005)\u001b[39m\n",
      " cv = StratifiedKFold(y, n_folds=4)\n",
      "\n",
      "\n",
      " # pick the best threshold out-of-fold\n",
      "\u001b[31m-thresholds = np.linspace(0.01, 0.99, 50)\u001b[39m\n",
      "\u001b[32m+thresholds = np.linspace(0.01, 0.99, 100)\u001b[39m\n",
      " mcc = np.array([matthews_corrcoef(y, preds>thr) for thr in thresholds])\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/liverpool-ion-switching/vbmokin/29565557.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # Thanks to https://www.kaggle.com/jazivxt/physically-possible with tuning from https://www.kaggle.com/siavrez/simple-eda-model and my tuning\n",
      "\u001b[31m-x1, x2, y1, y2 = train_test_split(train[col], train['open_channels'], test_size=0.3, random_state=seed_random)\u001b[39m\n",
      "\u001b[31m-params = {'learning_rate': lr, 'max_depth': -1, 'num_leaves': num_leaves, 'metric': 'rmse', 'random_state': seed_random, 'n_jobs':-1, 'sample_fraction':0.33} \u001b[39m\n",
      "\u001b[32m+x1, x2, y1, y2 = train_test_split(train[col], y, test_size=0.3, random_state=seed_random)\u001b[39m\n",
      "\u001b[32m+params = {'learning_rate': lr_lgb, 'max_depth': -1, 'num_leaves': num_leaves, 'metric': 'rmse', 'random_state': seed_random, 'n_jobs':-1, 'sample_fraction':0.33} \u001b[39m\n",
      " model = lgb.train(params, lgb.Dataset(x1, y1), num_iterations,  lgb.Dataset(x2, y2), verbose_eval=0, early_stopping_rounds=100, feval=MacroF1Metric)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/movie-review-sentiment-analysis-kernels-only/redwankarimsony/34088411.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accur\n",
      " # fitting the model\n",
      "\u001b[31m-model.fit(data_array_train, y_train, validation_data = (data_array_val, y_val), epochs=1, batch_size=256, verbose = 1)\u001b[39m\n",
      "\u001b[32m+model.fit(data_array_train, y_train, validation_data = (data_array_val, y_val), epochs=4, batch_size=256, verbose = 1)\u001b[39m\n",
      " model.save('trained_model')\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/marcocarnini/8245448.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "@@ -1 +1,3 @@\n",
      "\u001b[32m+import numpy as np\u001b[39m\n",
      " # loading embedding: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n",
      " embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
      " \n",
      "\u001b[31m-'Loaded %s word vectors' % len(embeddings_index)\u001b[39m\n",
      "\u001b[32m+words_embedding = set(embedding_index.keys())\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/word2vec-nlp-tutorial/twistedtensor/2359433.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-train_padded = pad_sequences(train_ind)[:,-100:]\u001b[39m\n",
      "\u001b[31m-test_padded = pad_sequences(test_ind)[:,-100:]\u001b[39m\n",
      "\u001b[32m+train_padded = pad_sequences(train_ind,maxlen=100,truncating='pre')\u001b[39m\n",
      "\u001b[32m+test_padded = pad_sequences(test_ind,maxlen=100,truncating='pre')\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/career-con-2019/indranilkhedkar/12441418.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " from sklearn.model_selection import train_test_split\n",
      "\u001b[31m-x_train_model, x_test_model, y_train_model, y_test_model = train_test_split(processed_train_data, y_train['labeled_surface'], test_size=0.3) \u001b[39m\n",
      "\u001b[32m+x_train_model, x_test_model, y_train_model, y_test_model = train_test_split(processed_train_data, y_train['surface'], test_size=0.30) \u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/recursion-cellular-image-classification/tanlikesmath/17484462.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-submission_df.sirna = preds.numpy().astype(int)\u001b[39m\n",
      "\u001b[32m+submission_df.sirna = preds_.numpy().astype(int)\u001b[39m\n",
      " submission_df.head(10)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/quora-insincere-questions-classification/ufoo68/7598334.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " embedding_dim = embedding_matrix.shape[1]\n",
      " inp_01 = layers.Input(shape = (MAX_LENGTH,))\n",
      "\u001b[31m-embedding_layer = layers.Embedding(NB_WORDS,embedding_dim, initialization = [embedding_matrix],input_length = MAX_LENGTH,trainable = False)(inp_01)\u001b[39m\n",
      "\u001b[32m+embedding_layer = layers.Embedding(NB_WORDS,embedding_dim, weights = [embedding_matrix],input_length = MAX_LENGTH,trainable = False)(inp_01)\u001b[39m\n",
      " \n",
      "\n",
      "\u001b[31m-  history_CNN_01 = CNN_model_01.fit(X_train, Y_train, batch_size = 512, epochs = 30, validation_data = (X_val,Y_val))\u001b[39m\n",
      "\u001b[32m+  history_CNN_01 = CNN_model_01.fit(X_train, Y_train, batch_size = 512, epochs = 100, validation_data = (X_val,Y_val))\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/costa-rican-household-poverty-prediction/youngdaniel/5827148.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def split_model(model, train_set, train_labels, test_set, test_ids):\n",
      "     \n",
      "     # Filter out the non-vulnerable households\n",
      "     test1_set = test_set[test0_labels]\n",
      "\u001b[31m-    test1_ids = test_ids[test0_labels]\u001b[39m\n",
      "\u001b[32m+    test1_ids = [test_ids[i] for i in range(len(test_ids)) if test0_labels[i]]\u001b[39m\n",
      "     train1_set = train_set[train0_labels]\n",
      "\n",
      "\n",
      "\u001b[31m-features = list(train_set.columns)\u001b[39m\n",
      " pipeline = Pipeline([('imputer', Imputer(strategy = 'median')), ('scaler', MinMaxScaler())])\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/severstal-steel-defect-detection/ekhtiar/18320089.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " # split the training data into train and validation set (stratified)\n",
      "\u001b[31m-X_train, X_val = train_test_split(repeated_train_image_ids, test_size=val_size, random_state=42)\u001b[39m\n",
      "\u001b[32m+X_train, X_val = train_test_split(train_df[train_df['EncodedPixels']!=-1]['ImageId'].unique(), test_size=val_size, random_state=42)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/covid19-global-forecasting-week-2/yustasalex/31179236.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " train = train_X[features]\n",
      "\u001b[31m-skf = StratifiedKFold(n_splits=3021, random_state=42)\u001b[39m\n",
      "\u001b[32m+skf = StratifiedKFold(n_splits=4021, random_state=42)\u001b[39m\n",
      " score_c = []\n",
      "\n",
      "\n",
      "\u001b[31m-skf = StratifiedKFold(n_splits=3021, random_state=42)\u001b[39m\n",
      "\u001b[32m+skf = StratifiedKFold(n_splits=4021, random_state=42)\u001b[39m\n",
      " score_f = []\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/talkingdata-adtracking-fraud-detection/atashnezhad/28748348.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-df_train = read_train_test_data_balanced(address_train)\u001b[39m\n",
      " df_train.head(3)\n",
      "\n",
      "\n",
      " # See the output paramters distribution \n",
      "\u001b[31m-xlist = ['is_attributed']\u001b[39m\n",
      "\u001b[32m+xlist = ['Real density (gr/cc)']\u001b[39m\n",
      " Plot_Hist_columns(df_train, xlist)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/generative-dog-images/francoisdubois/16494192.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " class BIGAN():\n",
      "         self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
      "\u001b[31m-        self.latent_dim = 100\u001b[39m\n",
      "\u001b[32m+        self.latent_dim = 3\u001b[39m\n",
      " \n",
      "\u001b[31m-        optimizer = Adam(0.0001, 0.1)\u001b[39m\n",
      "\u001b[32m+        optimizer = Adam(0.0001, 0.5)\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/data-science-bowl-2019/newbielch/27530679.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-sample_submission['accuracy_group'] = pred.astype(int)\u001b[39m\n",
      "\u001b[32m+sample_submission['accuracy_group'] = pred_lgb.astype(int)\u001b[39m\n",
      " sample_submission.to_csv('./submission.csv', index=False)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/tweet-sentiment-extraction/shawon10/30807454.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " submission = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\n",
      "\u001b[31m-select_random=test[\"text\"].apply(lambda x: \" \".join(x.split()[-31:]))\u001b[39m\n",
      "\u001b[31m-submission[\"selected_text\"]= select_random\u001b[39m\n",
      "\u001b[32m+submission[\"selected_text\"]= word_list\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/bike-sharing-demand/drcapa/30129291.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " for year in year_list:\n",
      "         model_rfr = RandomForestRegressor(n_estimators=1000)\n",
      "\u001b[31m-        model_gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.2)\u001b[39m\n",
      "\u001b[32m+        model_gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01)\u001b[39m\n",
      "         model_rfr.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/jigsaw-multilingual-toxic-comment-classification/theoviel/30757247.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " def predict(model, dataset, batch_size=64):\n",
      "     \"\"\"\n",
      "     device = xm.xla_device()\n",
      "\u001b[31m-    model = model.to(device)\u001b[39m\n",
      "\u001b[31m-    model.eval()\u001b[39m\n",
      "\u001b[32m+    model.eval().to(device)\u001b[39m\n",
      "     preds = np.empty((0, 1))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sentiment-analysis-on-movie-reviews/dylanlisk/31966957.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
      " sid = SentimentIntensityAnalyzer()\n",
      "\u001b[31m-df[\"sentiments\"] = df['clean_review'].apply(lambda x: sid.polarity_scores(x))\u001b[39m\n",
      "\u001b[31m-reviews_df = pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1)\u001b[39m\n",
      "\u001b[32m+reviews[\"sentiments\"] = reviews['clean_review'].apply(lambda x: sid.polarity_scores(x))\u001b[39m\n",
      "\u001b[32m+reviews_df = pd.concat([reviews.drop(['sentiments'], axis=1), reviews['sentiments'].apply(pd.Series)], axis=1)\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/nyc-taxi-trip-duration/usaf091847/8687071.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[32m+r_legend = ['Men', 'Women']\u001b[39m\n",
      " plt.figure(figsize=(10,8))\n",
      " ax = plt.subplot()\n",
      " ax.set_xticks(range(len(men_race)))\n",
      "\u001b[31m-ax.set_xticklabels(r_labels)\u001b[39m\n",
      "\u001b[32m+ax.set_xticklabels(list_of_races)\u001b[39m\n",
      " plt.xlabel('Race')\n",
      "\n",
      "\n",
      " age_sort = df.groupby('AGE').INMATEID.count().reset_index()\n",
      " age_x = age_sort.iloc[:,0]\n",
      "\n",
      " med_race = np.array(c_and_c_pivot.iloc[1,1:])\n",
      " min_race = np.array(c_and_c_pivot.iloc[2,1:])\n",
      " trace1 = go.Bar(\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/histopathologic-cancer-detection/hanjoonchoe/25141211.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " class Learner(object):\n",
      "                 outputs = model(inputs.float().cuda())\n",
      "\u001b[31m-                loss = criterion(outputs,targets)\u001b[39m\n",
      "\u001b[32m+                loss = criterion(outputs.cpu().squeeze(),targets.float())\u001b[39m\n",
      "                 running_loss += loss\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/chess/arjanso/16897236.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " board = Board()\n",
      " \n",
      "\u001b[31m-agent = Agent(network='conv_pg',lr=0.001)\u001b[39m\n",
      "\u001b[32m+agent = Agent(network='conv_pg',lr=0.07)\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/ieee-fraud-detection/tuttifrutti/21520429.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " groupC2 = groupsCid192031Device.copy()\n",
      " \n",
      "\u001b[31m-total1 = total1.merge(groupC1, how='left',left_on='TransactioID',right_on='TransactionID')\u001b[39m\n",
      "\u001b[31m-total1 = total1.merge(groupC2, how='left',left_on='TransactioID',right_on='TransactionID')\u001b[39m\n",
      "\u001b[32m+total1 = total1.merge(groupC1, how='left',left_on='TransactionID',right_on='TransactionID')\u001b[39m\n",
      "\u001b[32m+total1 = total1.merge(groupC2, how='left',left_on='TransactionID',right_on='TransactionID')\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/instacart-market-basket-analysis/snapmickey/1319040.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " rnew = pd.merge(products[['product_id','product_name']],result,how=\"inner\",on=['product_id'])\n",
      "\n",
      "\n",
      "\u001b[31m-highest_ord = rnew['product_name'].value_counts()[:20]\u001b[39m\n",
      "\u001b[32m+highest_ord = rnew['product_name'].value_counts()[:5]\u001b[39m\n",
      " highest_ord\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/adakibet/33579162.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[32m+train_dependent = np.log1p(train_dependent)\u001b[39m\n",
      " f, ax = plt.subplots(figsize=(9, 8))\n",
      " ax.set(ylabel=\"Frequency\")\n",
      " ax.set(xlabel=\"SalePrice\")\n",
      "\u001b[31m-ax.set(title=\"SalePrice distribution\")\u001b[39m\n",
      "\u001b[32m+ax.set(title=\"SalePrice distribution\")\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/nomad2018-predict-transparent-conductors/alyuev/2146305.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " sns.distplot(data.iloc[:, 12:13])\n",
      " sns.distplot(data.iloc[:, 13:])\n",
      "\n",
      " model.add(Dense(units=y_train_std.shape[1], kernel_initializer=k_init, activatio\n",
      " #model.compile(loss='mean_squared_logarithmic_error', optimizer='rmsprop', metrics=['mean_squared_error'])\n",
      "\u001b[31m-model.compile(loss ='mean_squared_logarithmic_error', optimizer='rmsprop',metrics=['mean_squared_logarithmic_error']) #accuracy mean_squared_logarithmic_error categorical_crossentropy mean_squared_error mean_absolute_error\u001b[39m\n",
      " # СОБСТВЕННО ТРЕНИРОВКА\n",
      "\n",
      " import sklearn.metrics as metrics\n",
      " \n",
      "\u001b[32m+model.compile(loss ='mean_squared_logarithmic_error', optimizer='rmsprop',metrics=['mean_squared_logarithmic_error']) #accuracy mean_squared_logarithmic_error categorical_crossentropy mean_squared_error mean_absolute_error\u001b[39m\n",
      " # СОБСТВЕННО ТРЕНИРОВКА\n",
      "\u001b[31m-history1 = model.fit(X_train_std,y_train_std,epochs=1000,batch_size=30,verbose=0,validation_split=0.1) #,show_accuracy=True\u001b[39m\n",
      "\u001b[32m+history1 = model.fit(X_train_std,y_train_std,epochs=3000,batch_size=30,verbose=0,validation_split=0.1) #,show_accuracy=True\u001b[39m\n",
      " # ПРОВЕРКА\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/house-prices-advanced-regression-techniques/lucasgiutavares/20504761.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " fsel = RFECV(est, step=1, cv=5, n_jobs=-1, scoring=scorer)\n",
      " fsel = fsel.fit(X, y)\n",
      "\u001b[31m-important_feat = list(X.loc[:, fsel.ranking_<=96].columns)\u001b[39m\n",
      "\u001b[32m+important_feat = list(X.loc[:, fsel.ranking_<=100].columns)\u001b[39m\n",
      " X = train.loc[:, important_feat].values\n",
      "\u001b[31m-fsel.n_features_\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/allstate-claims-severity/aaboyles/392825.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      " params = {\n",
      " # Grid Search CV optimized settings\n",
      "\u001b[31m-num_rounds = 5000\u001b[39m\n",
      "\u001b[31m-bst = xgb.train(params, xgdmat, num_boost_round = num_rounds)\u001b[39m\n",
      "\u001b[32m+bst = xgb.train(params, xgdmat, num_boost_round = 5000)\u001b[39m\n",
      "\n",
      " submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
      " submission.iloc[:, 1] = np.exp(bst.predict(test_xgb))\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/elo-merchant-category-recommendation/tandonarpit6/8119374.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\u001b[31m-model= xgb.XGBRegressor(learning_rate=0.1, gamma= 0)\u001b[39m\n",
      "\u001b[31m-model.fit(xtrain,ytrain)\u001b[39m\n",
      "\u001b[32m+model_1= xgb.XGBRegressor(learning_rate=0.05, gamma= 0)\u001b[39m\n",
      "\u001b[32m+model_1.fit(xtrain,ytrain)\u001b[39m\n",
      " \n",
      "\n",
      "\n",
      " plt.show()\n",
      "\n",
      "\n",
      "\u001b[34mdata/processed/competitions/sberbank-russian-housing-market/aharless/1189529.json\n",
      "----------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      " num_boost_rounds = len(cv_output)\n",
      "\u001b[31m-model = xgb.train(xgb_params, dtrain, num_boost_round= num_boost_rounds)\u001b[39m\n",
      "\u001b[32m+model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)\u001b[39m\n",
      "\n",
      "\n",
      " fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for diff in using_lib_examples[:40]:\n",
    "    display_kaggle_diff(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('RobustDataScience': conda)",
   "language": "python",
   "name": "python38264bitrobustdatasciencecondaff3daa7a14f54e6fb30e1fe30261bdb5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
