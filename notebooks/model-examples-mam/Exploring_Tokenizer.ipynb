{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many tokens is the typical function split into? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BartTokenizerFast\n",
    "from src.data.filter_pairs import build_function_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/processed/filtered_less_than_5_lines.jsonl\"\n",
    "diffs = pd.read_json(DATA_PATH, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_path</th>\n",
       "      <th>metadata</th>\n",
       "      <th>cell_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/processed/competitions/bluebook-for-bulld...</td>\n",
       "      <td>{'version_id': '16033940', 'slug_id': 'gstvolv...</td>\n",
       "      <td>n_valid = 12000\\n-n_trn = len(df) - n_valid\\n+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/processed/competitions/liberty-mutual-gro...</td>\n",
       "      <td>{'version_id': '443427', 'slug_id': 'aarotang'...</td>\n",
       "      <td># Uncomment the two lines below to roughly che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/processed/competitions/cdiscount-image-cl...</td>\n",
       "      <td>{'version_id': '1530168', 'slug_id': 'jpizarro...</td>\n",
       "      <td>-input_tensor = Input(shape=(180, 180, 3))\\n+i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/processed/competitions/cdiscount-image-cl...</td>\n",
       "      <td>{'version_id': '1530168', 'slug_id': 'jpizarro...</td>\n",
       "      <td># create the base pre-trained model\\n #base_mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/processed/competitions/cdiscount-image-cl...</td>\n",
       "      <td>{'version_id': '1530168', 'slug_id': 'jpizarro...</td>\n",
       "      <td>-num_images_test = 10\\n+num_images_test = 1000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_path  \\\n",
       "0  data/processed/competitions/bluebook-for-bulld...   \n",
       "1  data/processed/competitions/liberty-mutual-gro...   \n",
       "2  data/processed/competitions/cdiscount-image-cl...   \n",
       "3  data/processed/competitions/cdiscount-image-cl...   \n",
       "4  data/processed/competitions/cdiscount-image-cl...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'version_id': '16033940', 'slug_id': 'gstvolv...   \n",
       "1  {'version_id': '443427', 'slug_id': 'aarotang'...   \n",
       "2  {'version_id': '1530168', 'slug_id': 'jpizarro...   \n",
       "3  {'version_id': '1530168', 'slug_id': 'jpizarro...   \n",
       "4  {'version_id': '1530168', 'slug_id': 'jpizarro...   \n",
       "\n",
       "                                           cell_diff  \n",
       "0  n_valid = 12000\\n-n_trn = len(df) - n_valid\\n+...  \n",
       "1  # Uncomment the two lines below to roughly che...  \n",
       "2  -input_tensor = Input(shape=(180, 180, 3))\\n+i...  \n",
       "3  # create the base pre-trained model\\n #base_mo...  \n",
       "4  -num_images_test = 10\\n+num_images_test = 1000...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = \"../../models/CORAL_BART/tokenizer/\"\n",
    "merges = os.path.join(TOKENIZER_PATH,\"merges.txt\")\n",
    "vocab = os.path.join(TOKENIZER_PATH,\"vocab.json\")\n",
    "tokenizer = BartTokenizerFast(vocab, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = pd.DataFrame(build_function_vocabulary([row for i, row in diffs.iterrows()]).most_common(1000), columns = [\"Function\",\"Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>predict</td>\n",
       "      <td>7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>5054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_test_split</td>\n",
       "      <td>3378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drop</td>\n",
       "      <td>2840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Function  Count\n",
       "0           predict   7447\n",
       "1                     5054\n",
       "2               fit   3607\n",
       "3  train_test_split   3378\n",
       "4              drop   2840"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions[\"num_tokens\"] = functions[\"Function\"].map(lambda x: len(tokenizer.encode(x, add_special_tokens=False))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean        2.356000\n",
       "std         1.563877\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max        10.000000\n",
       "Name: num_tokens, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[\"num_tokens\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_example(string,tokenizer):\n",
    "    tokens = tokenizer.encode(string,add_special_tokens=False)\n",
    "    results = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LogisticRegression']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example(\"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', '_', 'test']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example(\"t_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf',\n",
       " 'Ġ=',\n",
       " 'ĠLogisticRegression',\n",
       " '(',\n",
       " 'random',\n",
       " '_',\n",
       " 'state',\n",
       " '=',\n",
       " '0',\n",
       " ').',\n",
       " 'fit',\n",
       " '(',\n",
       " 'X',\n",
       " ',',\n",
       " 'Ġy',\n",
       " ')']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_example(\"clf = LogisticRegression(random_state=0).fit(X, y)\",tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the old tokenizer we used in the last paper? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.CORAL_BART.dataset import CoralDiffsReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Diffs: 69898it [00:01, 40806.09it/s]\n",
      "100%|██████████| 69898/69898 [00:02<00:00, 33771.13it/s]\n",
      "  0%|          | 70/47272 [00:00<01:08, 694.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 22626 unparsable diffs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47272/47272 [01:27<00:00, 543.35it/s]\n",
      "Building Vocab...: 100%|██████████| 47272/47272 [00:00<00:00, 123436.61it/s]\n"
     ]
    }
   ],
   "source": [
    "coral_diffs = CoralDiffsReader(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x in coral_diffs.word2idx for x in functions[\"Function\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Capsule',\n",
       " 'show_wordcloud',\n",
       " 'UpConvBlock',\n",
       " 'getElementById',\n",
       " 'ConvTranspose2d',\n",
       " 'KernelSettings',\n",
       " 'fit_set',\n",
       " 'geom_bar',\n",
       " 'BatchNorm2d',\n",
       " 'SN',\n",
       " 'Model_train',\n",
       " 'return_error',\n",
       " 'build_model2',\n",
       " 'most_frequent_category',\n",
       " 'createDataPartition',\n",
       " 'conv2d_block',\n",
       " 'Dropout2d',\n",
       " 'function',\n",
       " 'deconv_block',\n",
       " 'load_game_and_ngs',\n",
       " 'geom_tile',\n",
       " 'missForest',\n",
       " 'left_join',\n",
       " 'theme_bw',\n",
       " 'build_model1',\n",
       " 'rgba',\n",
       " 'alt_session_wrapper',\n",
       " 'get_data_generators',\n",
       " 'merge_train_test',\n",
       " 'load_patient_data',\n",
       " 'ADAM',\n",
       " 'group_by',\n",
       " 'session_wrapper',\n",
       " 'get_aggregate_metrics',\n",
       " 'blocks',\n",
       " 'ridge_regression_model',\n",
       " 'transform_raw',\n",
       " '_DownSamplingBlock',\n",
       " 'lrelu',\n",
       " 'scaleSequential',\n",
       " 'theme',\n",
       " 'ConvBlock',\n",
       " 'xgb_fit',\n",
       " 'varImpPlot',\n",
       " 'spectral_norm',\n",
       " 'glm',\n",
       " 'SaveFeatures',\n",
       " 'ifelse',\n",
       " 'decide_test_file',\n",
       " 'CycleScheduler',\n",
       " 'makeLearner',\n",
       " 'forward_propagation',\n",
       " 'build_dataset',\n",
       " 'create_embedding_weights',\n",
       " 'create_prediction',\n",
       " 'Wave_Block',\n",
       " 'pytorch_model_run_cv',\n",
       " 'summarise_all',\n",
       " 'SVD',\n",
       " 'ReLu',\n",
       " 'add_feats',\n",
       " 'getDenseLayer',\n",
       " 'get_ensemble',\n",
       " 'upsample_concat_block',\n",
       " 'SingularValueDecomposition',\n",
       " 'layer_dense',\n",
       " 'applyPCA',\n",
       " 'augment_images',\n",
       " 'wine_attribute',\n",
       " 'str_extract',\n",
       " 'SpatialDropout1D',\n",
       " 'toxicwordcloud',\n",
       " 'HallOfFame',\n",
       " 'next_batch',\n",
       " 'geom_histogram',\n",
       " 'confusionMatrix',\n",
       " 'csv2',\n",
       " 'string_extract',\n",
       " 'theme_classic',\n",
       " 'fc2',\n",
       " 'addWeighted',\n",
       " 'create_test_data',\n",
       " 'Cal_Series_Avg',\n",
       " 'drop_sparse',\n",
       " 'createFolds',\n",
       " 'SVDpp',\n",
       " 'BatchNorm1d',\n",
       " 'layer_dropout',\n",
       " 'Darknet',\n",
       " 'transform_ridge',\n",
       " 'character',\n",
       " 'train_func',\n",
       " 'eval_jacc',\n",
       " 'conv2d_transpose',\n",
       " 'get_imgs_labels',\n",
       " 'optimizer_adam',\n",
       " 'create_cnn_lstm',\n",
       " 'ets',\n",
       " 'clip_upper']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in functions[\"Function\"] if not x in coral_diffs.word2idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
