{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples from Finetuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from transformers import BartTokenizerFast\n",
    "\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, RobertaTokenizerFast, Trainer, TrainingArguments, DataCollator\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def load_model(path):\n",
    "    model = BartForConditionalGeneration.from_pretrained(path)\n",
    "    return model\n",
    "\n",
    "def load_model_wandb(wandb_id, results_path = \"results/\"):\n",
    "    model_path = os.path.join(results_path,wandb_id,wandb_id,\"*\",\"\")\n",
    "    model_checkpoints = glob(model_path)\n",
    "    print(model_checkpoints)\n",
    "    most_recent_checkpoint = sorted(model_checkpoints)[-1]\n",
    "    return load_model(most_recent_checkpoint)\n",
    "\n",
    "def get_alternative_interface(model, tokenizer):\n",
    "    def interface(text, gen_kwargs = {\"max_length\":50,\n",
    "                                      \"num_beams\":5,\n",
    "                                      \"early_stopping\":True}):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        # outputs = model.generate(inputs[\"input_ids\"], max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "        outputs = model.generate(inputs[\"input_ids\"], **gen_kwargs)\n",
    "        print(outputs)\n",
    "        return ([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs.tolist()])\n",
    "    return interface\n",
    "\n",
    "def process_input(model,tokenizer,inputs):\n",
    "    outputs = model.generate(\n",
    "                        inputs[\"input_ids\"], \n",
    "                        max_length=50, \n",
    "                        num_beams=5, \n",
    "                        early_stopping=True)\n",
    "\n",
    "    return ([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs.tolist()])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../../results\"\n",
    "\n",
    "TOKENIZER_PATH = \"../../models/CORAL_BART/tokenizer\"\n",
    "vocab_path = os.path.join(TOKENIZER_PATH,\"vocab.json\")\n",
    "merges_path = os.path.join(TOKENIZER_PATH, \"merges.txt\")\n",
    "tokenizer = BartTokenizerFast(vocab_path,merges_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [fancy-sponge-24](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/21hscc2l)\n",
    "Initialized with `facebook/bart-large` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"\"\"clf=LogisticeRegression(x,y)\"\"\"\n",
    "\n",
    "fancy_sponge = load_model(\"/homes/gws/mikeam/RobustDataScience/results/fancy-sponge-24/checkpoint-120000\")\n",
    "fancy_sponge_alternatives = get_alternative_interface(fancy_sponge,tokenizer)\n",
    "fancy_sponge_alternatives(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sleek-vortex-25](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/1gnq0z54?workspace=user-mikeamerrill) \n",
    "Initialized with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleek_vortex = load_model(\"/homes/gws/mikeam/RobustDataScience/results/sleek-vortex-25/checkpoint-40000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleek_vortex_alternatives = get_alternative_interface(sleek_vortex,tokenizer)\n",
    "sleek_vortex_alternatives(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_STRS = [\n",
    "    \"clf=xgb(x,y)\",\n",
    "    \"LogisticRegression.fit(x,y)\",\n",
    "    \"SVC(kernel='rbf')\"\n",
    "]\n",
    "TEST_PARAMS = [\n",
    "    (\"Greedy\",{\"max_length\":25}),\n",
    "    (\"3-Beams\",{\"num_beams\":3,\"max_length\":25,\"early_stopping\":True}),\n",
    "    (\"5-Beams\",{\"num_beams\":5,\"max_length\":25,\"early_stopping\":True}),\n",
    "    (\"5-Beams-No-2-Grams\",{\"num_beams\":5,\"max_length\":25,\"early_stopping\":True,\"no_repeat_ngram_size\":2}),\n",
    "    (\"Pure-Sampling\",{\"max_length\":25, \"top_k\":0, \"early_stopping\":True, \"do_sample\":True}),\n",
    "    (\"Top-3-Sampling\",{\"max_length\":25, \"top_k\":3, \"early_stopping\":True, \"do_sample\":True}),\n",
    "    (\"Top-5-Sampling\",{\"max_length\":25, \"top_k\":5, \"early_stopping\":True, \"do_sample\":True}),\n",
    "    (\"Top-5-Sampling-Higher-Temp\",{\"max_length\":25, \"top_k\":5, \"early_stopping\":True, \"do_sample\":True, \"temperature\":0.7}),\n",
    "    (\"Nucleus-Sampling-0.9\",{\"max_length\":25, \"top_k\":5, \"early_stopping\":True, \"do_sample\":True, \"top_p\":0.9}),\n",
    "    (\"Nucleus-Sampling-0.5\",{\"max_length\":25, \"top_k\":5, \"early_stopping\":True, \"do_sample\":True, \"top_p\":0.5})\n",
    "]\n",
    "\n",
    "def sampling_strats(text,interface,num_return_sequences=1):\n",
    "    print(text)\n",
    "    results = []\n",
    "    for strat_name, params in TEST_PARAMS:\n",
    "        if not (len(params) == 1 and \"max_length\" in params):\n",
    "            params[\"num_return_sequences\"] = num_return_sequences\n",
    "        result = \"\\n--------------\\n\".join(interface(text,gen_kwargs = params))\n",
    "        results.append([strat_name,result])\n",
    "    print(tabulate(results,[\"Strategy\",\"Result\"],tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len({\"a\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [still-shadow-44](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/1sy5io22?workspace=)\n",
    "Was trained with Ge's method for encouraging differences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_shadow = load_model_wandb(\"still-shadow-44\",results_path = RESULTS_PATH)\n",
    "still_shadow_alternatives = get_alternative_interface(still_shadow, tokenizer)\n",
    "sampling_strats(\"LogisticRegression.fit(x,y)\", still_shadow_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n 10 /homes/gws/mikeam/RobustDataScience/results/still-shadow-44/still-shadow-44/eval-preds-10000.jsonl | jq -r ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [vivid-fire-41](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/13de72q6?workspace=user-mikeamerrill)\n",
    "This model doesn't have the method for discouraging differences, but was intialized with facebook weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vivid_fire = load_model(\"/homes/gws/mikeam/RobustDataScience/results/vivid-fire-41/vivid-fire-41/checkpoint-200000\")\n",
    "vivid_fire_alternatives = get_alternative_interface(vivid_fire,tokenizer)\n",
    "sampling_strats(\"LogisticRegression.fit(x,y)\", vivid_fire_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", vivid_fire_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [rural-spaceship-48](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/1xnx0ges?workspace=user-mikeamerrill)\n",
    "This model was trained using FACE - Frequence Aware Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rural_spaceship = load_model(\"/homes/gws/mikeam/RobustDataScience/results/rural-spaceship-48/checkpoint-80000\")\n",
    "rural_spaceship_alternatives = get_alternative_interface(rural_spaceship,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", rural_spaceship_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think theres evidence here that BPE tokenization is screwing us over. I don't understand why \"nPlateaun\" is showing up. We do have the following in our training set:\n",
    "```\n",
    "ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, verbose=1, min_lr=1e-7)\n",
    "```\n",
    "I think it's definitely going to be productive to somehow use more information from the library structure. For example, building a knowledge graph with the library stucture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [glorious-snowflake-54](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/3wwtknpy?workspace=user-mikeamerrill)\n",
    "Trained on smaller diffs, FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glorious_snowflake = load_model(\"/homes/gws/mikeam/RobustDataScience/results/glorious-snowflake-54/checkpoint-120000/\")\n",
    "glorious_snowflake_alternatives = get_alternative_interface(glorious_snowflake,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : 1000,'learning_rate': 0.1,\\n             'max_depth': [5]}\", glorious_snowflake_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly.... this looks better to me. With Pure-Sampling  we got `LGBMRegressor` in there, and even if the learning rate was changed to zero, that's still the token that we'd want to see modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"LogisticRegression.fit(x,y)\", glorious_snowflake_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"\"\"clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)\"\"\", glorious_snowflake_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [zany-music-55](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/3aq45qf7/overview?workspace=user-mikeamerrill)\n",
    "Same as above, but with lower learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zany_music = load_model(\"/homes/gws/mikeam/RobustDataScience/results/glorious-snowflake-54/checkpoint-120000/\")\n",
    "zany_music_alternatives = get_alternative_interface(zany_music,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", zany_music_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n 10 /homes/gws/mikeam/RobustDataScience/results/zany-music-55/eval-preds-128700.jsonl | jq -r ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [polar-puddle-58](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/1rk236ew/overview?workspace=user-mikeamerrill)\n",
    "Trained using the first of the \"predict_spans\" task where the model only calculated loss on tokens that changed. Looks like it seriously overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_puddle = load_model(\"/homes/gws/mikeam/RobustDataScience/results/polar-puddle-58/checkpoint-90000/\")\n",
    "polar_puddle_alternatives = get_alternative_interface(polar_puddle,tokenizer)\n",
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", polar_puddle_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of this makes sense to me? We either still need to generate the other tokens, so we still need to include them in the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[worldly-microwave-59](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/37mjeivo?workspace=user-mikeamerrill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldly_microwave = load_model(\"/homes/gws/mikeam/RobustDataScience/results/worldly-microwave-59/checkpoint-90000/\")\n",
    "worldly_microwave_alternatives = get_alternative_interface(worldly_microwave,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", worldly_microwave_alternatives, num_return_sequences = 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [earnest-river-64](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/2whutfrj?workspace=user-mikeamerrill)\n",
    "First model trained with the masked loss on mixed dataset for 30 epochs. Loss got very low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnest_river = load_model(\"/homes/gws/mikeam/RobustDataScience/results/earnest-river-64/checkpoint-220000/\")\n",
    "earnest_river_alternatives = get_alternative_interface(earnest_river,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", earnest_river_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n 10 /homes/gws/mikeam/RobustDataScience/results/earnest-river-64/eval-preds-175000.jsonl | jq -r ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\", earnest_river_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\", earnest_river_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\",  earnest_river_alternatives,  num_return_sequences = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [winter-planet-66](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/1iumhxpg?workspace=user-mikeamerrill)\n",
    "The first model trained with the multi-task loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_planet = load_model(\"/homes/gws/mikeam/RobustDataScience/results/winter-planet-66/checkpoint-670000/\")\n",
    "winter_planet_alternatives = get_alternative_interface(winter_planet,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"xgboost = XGBClassifier()\\nparameters_rf = {'n_estimators' : [1000],'learning_rate': [0.1],\\n             'max_depth': [5]}\", winter_planet_alternatives, num_return_sequences = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n 10 /homes/gws/mikeam/RobustDataScience/results/winter-planet-66/eval-preds-95000.jsonl | jq -r ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(df):\n",
    "    html_str = df.to_html().replace(\"\\\\n\",\"<br>\")\n",
    "    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )\n",
    "\n",
    "def sample_predictions(path,limit=10):\n",
    "    results = pd.read_json(path,lines=True)\n",
    "    did_change = results[results[\"input\"]!=results[\"label\"]]\n",
    "    is_small = did_change[did_change[\"input\"].map(lambda x: len(x.split(\"\\n\"))) < 4]\n",
    "    return is_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sample_predictions(\"/homes/gws/mikeam/RobustDataScience/results/winter-planet-66/eval-preds-95000.jsonl\", limit=100)\n",
    "results.to_csv(\"./examples.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"./examples.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_json(\"/homes/gws/mikeam/RobustDataScience/results/winter-planet-66/eval-preds-95000.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"input\"].map(lambda x: len(x.split(\"\\n\"))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /homes/gws/mikeam/RobustDataScience/results/winter-planet-66/eval-preds-95000.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"clf = LogisticRegression(x,y)\",  winter_planet_alternatives,  num_return_sequences = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_json(\"/homes/gws/mikeam/RobustDataScience/data/processed/mixed.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[\"cell_diff\"].map(lambda x: len(x.split(\"\\n\"))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [autumn-salad-68](https://app.wandb.ai/mikeamerrill/robustdatascience/runs/lg7ush09/overview?workspace=user-mikeamerrill)\n",
    "Same as above, but with random masking of tokens that don't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autumn_salad = load_model(\"/homes/gws/mikeam/RobustDataScience/results/winter-planet-66/checkpoint-210000/\")\n",
    "autumn_salad_alternatives = get_alternative_interface(autumn_salad,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strats(\"clf = LogisticRegression(x,y)\",  autumn_salad_alternatives,  num_return_sequences = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response, we'll try training with the FACE loss, which should discourage results like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hopeful-brook-108\n",
    "Using span-aware generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopeful_brook_results = pd.read_json(\"/homes/gws/mikeam/RobustDataScience/results/hopeful-brook-108/eval-preds-55000.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopeful_brook_results.head()[[\"input\",\"label\",\"prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hopeful_brook_results.head().iloc[3][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hopeful_brook_results.head().iloc[3][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions(\"/homes/gws/mikeam/RobustDataScience/results/hopeful-brook-108/eval-preds-55000.jsonl\")\\\n",
    "                [[\"input\",\"label\",\"prediction\"]].sample(100).to_csv(\"example_csvs/hopeful-brook-108.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('RobustDataScience': conda)",
   "language": "python",
   "name": "python38264bitrobustdatasciencecondaff3daa7a14f54e6fb30e1fe30261bdb5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
