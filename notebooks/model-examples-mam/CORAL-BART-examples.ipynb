{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORAL-BART Examples\n",
    "The following examples are for the CORAL-BART model trained on the first version of the Kaggle diffs dataset.\n",
    "The pretraining scheme asked the model to predict the full contents of cells that had been edited between sequential kaggle submissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mmegavolt.cs.washington.edu\u001b[m  Thu Jul 16 09:25:40 2020  \u001b[1m\u001b[30m440.82\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mGeForce RTX 2070\u001b[m |\u001b[31m 45'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   11\u001b[m / \u001b[33m 7982\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mTITAN RTX       \u001b[m |\u001b[1m\u001b[31m 85'C\u001b[m, \u001b[1m\u001b[32m 93 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m17162\u001b[m / \u001b[33m24220\u001b[m MB | \u001b[1m\u001b[30mmikeam\u001b[m(\u001b[33m17151M\u001b[m)\n",
      "\u001b[36m[2]\u001b[m \u001b[34mGeForce RTX 2070\u001b[m |\u001b[1m\u001b[31m 76'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   11\u001b[m / \u001b[33m 7982\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mGeForce RTX 2070\u001b[m |\u001b[1m\u001b[31m 67'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   11\u001b[m / \u001b[33m 7981\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this later:\n",
    "`export CUDA_DEVICE_ORDER=PCI_BUS_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "from transformers import RobertaTokenizerFast, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = BartForConditionalGeneration.from_pretrained(path)\n",
    "    return model\n",
    "\n",
    "def get_alternative_interface(model, tokenizer):\n",
    "    def interface(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        # outputs = model.generate(inputs[\"input_ids\"], max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "        outputs = model.generate(\n",
    "                        inputs[\"input_ids\"], \n",
    "                        max_length=50, \n",
    "                        num_beams=5, \n",
    "                        early_stopping=True)\n",
    "\n",
    "        return ([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs.tolist()])[0]\n",
    "    return interface\n",
    "\n",
    "def process_input(model,tokenizer,inputs):\n",
    "    outputs = model.generate(\n",
    "                        inputs[\"input_ids\"], \n",
    "                        max_length=50, \n",
    "                        num_beams=5, \n",
    "                        early_stopping=True)\n",
    "\n",
    "    return ([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs.tolist()])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('facebook/bart-large-cnn')\n",
    "model =  load_model('../../results/big_better/checkpoint-70000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_interface = get_alternative_interface(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
      "X__kpcaca = k k kpca.fit_transform(X))) the\n"
     ]
    }
   ],
   "source": [
    "input_example = \"\"\"kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\"\"\"\n",
    "print(example_interface(input_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch24__2019=delhi_data[((delhihi__data['Date'] >= '2019-03-03') & ( & ( ( (hi-04-15')]\n",
      "m the\n"
     ]
    }
   ],
   "source": [
    "input_example = \"\"\"march24_2019=delhi_data[(delhi_data['Date'] >= '2019-03-23') & (delhi_data['Date'] <= '2019-04-15')]\n",
    "march24_2020=delhi_data[(delhi_data['Date'] >= '2020-03-23') & (delhi_data['Date'] <= '2020-04-15')]\"\"\"\n",
    "print(example_interface(input_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('RobustDataScience': conda)",
   "language": "python",
   "name": "python38264bitrobustdatasciencecondaff3daa7a14f54e6fb30e1fe30261bdb5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
